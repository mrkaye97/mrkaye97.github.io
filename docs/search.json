[
  {
    "objectID": "my-three-favorite.html",
    "href": "my-three-favorite.html",
    "title": "My Three Favorite, Alphabetically",
    "section": "",
    "text": "ALGORITHMS: Bogo Sort, Logistic Regression, Metropolis-Hastings\nARTISTS: Dal√≠, Monet, Picasso\nARTISTS (MUSICAL): Ozuna, Quinn XCII, Taylor Swift\nAUTHORS: Fredrik Backman, Khaled Hosseini, Sally Rooney\nBEERS: Heady Topper ‚Äì The Alchemist , King Sue ‚Äì Toppling Goliath, Very Green ‚Äì Tree House\nBOOKS (FICTION): The Kite Runner, Neverwhere, A Storm of Swords\nBOOKS (NON-FICTION): The Signal and the Noise, Factfulness, Weapons of Math Destruction\nCITIES (AMERICAN): Boston, New York City, San Diego\nCITIES (EUROPEAN): Barcelona, Budapest, Stockholm\nCITIES (OTHER): Medell√≠n, Oaxaca, Vancouver\nCOLLEGE COURSES: Advanced Algorithms, Mathematical Structures, Price Theory\nCONDIMENTS: Calabrian Chiles, Preserved Lemons, Secret Aardvark Hot Sauce\nCUISINES (Couldn‚Äôt pick three): Italian, Japanese, Lebanese, Mexican\nDATA STRUCTURES: Graph, Skip List, Tibble\nECONOMIC SUBFIELDS: Labor Economics, Monetary Theory, Urban Economics\nFICTIONAL CHARACTERS: Elaine Benes, Oberyn Martell, Samwise Gamgee\nFOODS: Bananas, Maitake Mushrooms, Peanut Butter\nKITCHEN UTENSILS: 7‚Äù Santoku Knife, Tasting Spoon, Tongs\nMOVIES: Birdman, It‚Äôs a Wonderful Life, Ferris Bueller‚Äôs Day Off\nNON-CITY PLACES: Acadia National Park, Baseball Hall of Fame, Little Cottonwood Canyon\nPARADIGMS: Bayesian, Behavioral, Functional\nPIECES OF COOKWARE: 3-Quart Saute Pan, Cast Iron Skillet, Dutch Oven\nR PACKAGES: brms, dplyr, ggplot2 + ggthemes + patchwork + viridisLite\nSERIOUSEATS RECIPES: Halal Cart Chicken, Pozole Verde with Chicken, Red Wine-Braised Short Ribs\nSKI MOUNTAINS: Alta, Big Sky, Jackson Hole\nSPIRITS: Gin, Islay Scotch, Mezcal\nSPORTS TO WATCH: Baseball, College Football, Soccer\nSUBREDDITS: AdvancedRunning, MaleLivingSpace, UnexpectedFactorial\nTV SHOWS: Atlanta, Sex Education, Seinfeld\nWEBSITES: Fangraphs, FiveThirtyEight, Serious Eats"
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-01-library-code/library-code.html",
    "href": "posts/series/doing-data-science/2023-04-01-library-code/library-code.html",
    "title": "Writing Internal Libraries for Analytics Work",
    "section": "",
    "text": "This post is part of a series called The Missing Semester of Your DS Education."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-01-library-code/library-code.html#introduction",
    "href": "posts/series/doing-data-science/2023-04-01-library-code/library-code.html#introduction",
    "title": "Writing Internal Libraries for Analytics Work",
    "section": "Introduction",
    "text": "Introduction\nBy definition, library code is code that‚Äôs written to being reused by programs other than itself that are unrelated to each other. For instance, dplyr (R) and pandas (Python) are common examples of library code: Instead of writing code from scratch to work with tabular data, you might use one of those two fantastic libraries. And you get some additional benefits from using them:\n\nThose libraries are well-documented, so it‚Äôs easy to figure out how to use them.\nThey‚Äôre well-tested, so you (presumably) know that bugs are less likely than if you were to try to write the same functionality from scratch.\nThey‚Äôre (relatively) performant.\nThey‚Äôre widely used, so it‚Äôs easy to find answers to questions and get help from the communities using them."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-01-library-code/library-code.html#a-common-library",
    "href": "posts/series/doing-data-science/2023-04-01-library-code/library-code.html#a-common-library",
    "title": "Writing Internal Libraries for Analytics Work",
    "section": "A Common Library",
    "text": "A Common Library\nAt CollegeVine, we have collegeviner: An R package containing a lot of code that we use very often for all kinds of analytics projects. Some things that live in collegeviner include:\n\nPlot theming code, so that we can consistently theme graphics across all of our work.\nA custom implementation of item-based collaborative filtering, which is core to our school recommendation system.\nDBI and aws.s3 wrappers for connecting to and querying our databases and working with data in S3.\nHelper methods for common math we do, such as converting between odds, log-odds, and probabilities.\nMiscellaneous helper code for things that don‚Äôt exist natively in R, such as yeet for removing an item from a list and %notin%, the inverse of the %in% operator.\nAn implementation of the Brier Skill Score, which is a metric we often use for evaluating classification models.\nA lot more!\n\nYou might think of collegeviner as being a common library of things that our team does often, so we don‚Äôt need to repeat ourselves or reinvent the wheel.\n\nA Toy Example\nLet‚Äôs imagine that you‚Äôre setting up a common library (in this example, an R package) for your team. The first thing you might want to do is have some logic to help your team connect to your data warehouse. For this example, let‚Äôs just imagine that ‚Äúwarehouse‚Äù is your local Postgres instance. Then, you might write a method for your library called connect_to_dwh like this:\n\n\n\n\nlibrary(DBI)\nlibrary(rlang)\nlibrary(httr)\nlibrary(RPostgres)\n\nconnect_to_dwh <- function(url = Sys.getenv(\"DATA_WAREHOUSE_URL\")) {\n  if (url == \"\") abort(\"You must specify a dwh URL.\")\n\n  check_installed(pkg = \"httr\")\n  check_installed(pkg = \"RPostgres\")\n\n  db_params <- parse_url(url)\n\n  db_drv <- Postgres()\n  db_user <- db_params$username\n  db_password <- db_params$password\n  db_host <- db_params$hostname\n  db_port <- db_params$port %||% 5432\n  db_name <- db_params$path\n\n  dbConnect(\n    db_drv,\n    dbname = db_name,\n    host = db_host,\n    port = db_port,\n    user = db_user,\n    password = db_password\n  )\n}\n\nNow you have a single function that your whole team can share to connect to your data warehouse, assuming that they can provide the connection string. Let‚Äôs test out how the workflow might look for querying data now.\n\nConnecting\n\nSys.setenv(DATA_WAREHOUSE_URL = \"postgresql://localhost:5432/postgres\")\n\nconn <- connect_to_dwh()\n\nAnd that‚Äôs it! You‚Äôre connected. You can now query away using dbGetQuery(), a custom wrapper, dbplyr, or any other method of choice.\n\n\nQuerying\n\n## Put some data into the warehouse for example purposes\ndbWriteTable(conn, Id(schema = \"blog\", table = \"iris\"), janitor::clean_names(iris))\n\nresult <- dbGetQuery(\n  conn = conn,\n  \"\n  SELECT species, MAX(sepal_length) AS max_sepal_length\n  FROM blog.iris\n  GROUP BY species\n  ORDER BY 2 DESC\n  \"\n)\n\npretty_print(result)\n\n\n\n \n  \n    species \n    max_sepal_length \n  \n \n\n  \n    virginica \n    7.9 \n  \n  \n    versicolor \n    7.0 \n  \n  \n    setosa \n    5.8 \n  \n\n\n\n\n\n\n\nTesting\nIt‚Äôs also important to test your code. testthat makes writing unit tests for your new connect_to_dwh function very simple.\n\nlibrary(testthat)\n\ntest_that(\"Connecting works as expected\", {\n  ## This should error because the URL is empty\n  expect_error(\n    connect_to_dwh(\"\"),\n    \"You must specify a dwh URL\"\n  )\n  \n  conn <- connect_to_dwh()\n\n  ## Should return a PqConnection object\n  expect_s4_class(conn, \"PqConnection\")\n  \n  ## Should be able to query an example table\n  expect_equal(\n    dbGetQuery(conn, \"SELECT COUNT(*) FROM blog.iris\")$count,\n    150\n  )\n})\n\nTest passed üéâ\n\n\n\n\nVersioning\nLastly, it‚Äôs important to version your code. Semantic Versioning (SemVer) is a very common standard for versioning library code. In R specifically, you can read about versioning in Chapter 22 of R Packages.\nIn our toy example, this means that if you change how the logic of your connect_to_dwh function works, you should change the version of your package so that your users (your teammates) don‚Äôt get blindsided by your change. Incrementing your package‚Äôs version shows your teammates that something has changed in your library, and they can update their code to rely on the latest version (if they wish), or continue using the current version they‚Äôre on, or anything else.\n\n\n\n\n\n\nNote that being able to control which version of a library your code is using requires some manner of managing dependencies. In R, I would highly recommend renv. In Python, I like Poetry."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-01-library-code/library-code.html#one-library-per-model",
    "href": "posts/series/doing-data-science/2023-04-01-library-code/library-code.html#one-library-per-model",
    "title": "Writing Internal Libraries for Analytics Work",
    "section": "One Library Per Model",
    "text": "One Library Per Model\nIn addition to a common library for sharing code that‚Äôs very often used across the data org, our team has also gotten into the habit of having a library per ML model in production. This definition can be a bit flexible (both in terms of what ‚ÄúML model‚Äù means, and also what ‚Äúproduction‚Äù means), but the basic principle should be the same: ML in production requires at least some training logic and some monitoring logic. It‚Äôs a good idea to share code between those two things. Let‚Äôs consider another simple example.\n\nIris Species Prediction\nLet‚Äôs imagine that we work for a florist. On our website, we have a service where someone can provide some measurements about an iris (either a setosa or a virginica), as we‚Äôll tell them which of the two we think it is. We know we‚Äôll want to retrain the model periodically as we get more data, and we‚Äôll also want to monitor how our model performs out-of-sample. Both training the model and monitoring will require some shared logic: loading raw data, doing feature engineering, and making predictions. So it would make sense to have those two jobs rely on a single library, as opposed to needing to repeat the logic. Let‚Äôs write that library here.\n\nFetching the Data\nFirst, let‚Äôs write a method to fetch the raw data from our data warehouse. In practice, it probably makes sense to factor out the SQL here into individual SQL scripts, but for this example I‚Äôll just include the SQL directly as a string.\n\nfetch_raw_data <- function(conn) {\n  dbGetQuery(\n    conn = conn,\n    \"\n    SELECT *\n    FROM blog.iris\n    WHERE species IN ('setosa', 'virginica')\n    \"\n  )\n}\n\n\n\nFeature Engineering\nNext, let‚Äôs write some methods to create features.\n\ncreate_sepal_length_feature <- function(sepal_length) {\n  sepal_length + rnorm(n = length(sepal_length))\n}\n\ncreate_petal_width_feature <- function(petal_width) {\n  petal_width + rgamma(n = length(petal_width), shape = 2)\n}\n\nAnd then we can write a function to take our raw data, run the feature engineering steps, and return the features.\n\ncreate_features <- function(raw_data) {\n  sepal_length <- create_sepal_length_feature(raw_data$sepal_length)\n  petal_width <- create_petal_width_feature(raw_data$petal_width)\n  is_setosa <- raw_data$species == \"setosa\"\n  \n  data.frame(\n    sepal_length,\n    petal_width,\n    is_setosa\n  )\n}\n\n\n\nModel Fitting and Prediction\nNext, let‚Äôs write methods to fit the model and make predictions.\n\nfit_model <- function(features) {\n  formula <- is_setosa ~ sepal_length + petal_width\n\n  ## Dynamically extract the variables in the formula\n  ## so we don't need to repeat ourselves\n  predictors <- as.character(labels(terms(formula)))\n  target <- as.character(formula[[2]])\n  \n  ## Very basic error handling\n  if (!all(c(predictors, target) %in% colnames(features))) {\n    abort(\"Some required columns were missing from `features`\")\n  }\n  \n  model <- glm(\n    formula,\n    data = features,\n    family = binomial\n  )\n  \n  class(model) <- c(\"irises\", class(model))\n  \n  model\n}\n\npredict.irises <- function(object, newdata = NULL, ...) {\n  probs <- predict.glm(\n    object,\n    newdata,\n    type = \"response\"\n  )\n  \n  ## If the predicted probability is > 50%,\n  ## return `true`, else return `false`\n  probs > 0.50\n}\n\n\n\nModel Evaluation\nAnd finally, let‚Äôs add a function to compute the model‚Äôs accuracy on some evaluation data.\n\ncompute_accuracy <- function(prediction, is_setosa) {\n  100 * sum(prediction == is_setosa) / length(prediction)\n}\n\n\n\nTesting\nIt‚Äôs important to note that all of the methods above can and should be unit tested in the same way we tested our helper for connecting to the database. Testing is a great way to ensure the correctness of your code and make it more maintainable by making it easier to refactor in the future, and putting all of your modeling logic into a library like this makes it very easy to test. For instance, here‚Äôs how you might write a couple of unit tests for the petal width feature.\n\ntest_that(\"Petal width feature is created correctly\", {\n  ## The feature should be positive even when the\n  ## petal width is zero, since we're adding gamma\n  ## random noise.\n  expect_gt(create_petal_width_feature(0), 0)\n  \n  ## It should be extremely unlikely that a single \n  ## draw from a gamma(2) is >10, which means this\n  ## feature should be < 10 when the input is 0 in \n  ## the vast majority of cases.\n  ##\n  ## NOTE: This is by definition a brittle test, and\n  ## I wouldn't recommend writing tests that are\n  ## probabilistic like this in practice unless\n  ## you really need to. If you do, this will\n  ## fail _some_ of the time, at random, even\n  ## if \"some\" is a very small percentage.\n  purrr::walk(\n    rep(0, 100), \n    function(x) {\n      expect_lt(\n        create_petal_width_feature(x),\n        10\n      )\n    } \n  )\n})\n\nTest passed üéâ\n\n\n\n\n\nA Retraining Job\nGreat! Now that we have all of that library code written, we can package it up into a retraining job. A very simple training job might look like this:\nFirst, connect to the data warehouse\n\nconn <- connect_to_dwh()\n\nNext, fetch the raw data from the warehouse that we need to train the model.\n\nraw <- fetch_raw_data(conn)\n\npretty_print(head(raw))\n\n\n\n \n  \n    sepal_length \n    sepal_width \n    petal_length \n    petal_width \n    species \n  \n \n\n  \n    5.1 \n    3.5 \n    1.4 \n    0.2 \n    setosa \n  \n  \n    4.9 \n    3.0 \n    1.4 \n    0.2 \n    setosa \n  \n  \n    4.7 \n    3.2 \n    1.3 \n    0.2 \n    setosa \n  \n  \n    4.6 \n    3.1 \n    1.5 \n    0.2 \n    setosa \n  \n  \n    5.0 \n    3.6 \n    1.4 \n    0.2 \n    setosa \n  \n  \n    5.4 \n    3.9 \n    1.7 \n    0.4 \n    setosa \n  \n\n\n\n\n\nNext, create the features from the raw data by running the feature engineering pipeline.\n\nfeatures <- create_features(raw)\n\npretty_print(head(features))\n\n\n\n \n  \n    sepal_length \n    petal_width \n    is_setosa \n  \n \n\n  \n    3.149 \n    1.038 \n    TRUE \n  \n  \n    4.081 \n    0.917 \n    TRUE \n  \n  \n    5.785 \n    0.362 \n    TRUE \n  \n  \n    4.277 \n    2.962 \n    TRUE \n  \n  \n    4.839 \n    2.027 \n    TRUE \n  \n  \n    5.498 \n    6.044 \n    TRUE \n  \n\n\n\n\n\nThen fit a model using the features.\n\nmodel <- fit_model(features)\n\npretty_print(coef(model))\n\n\n\n \n  \n      \n    x \n  \n \n\n  \n    (Intercept) \n    11.106 \n  \n  \n    sepal_length \n    -1.503 \n  \n  \n    petal_width \n    -0.769 \n  \n\n\n\n\n\nFinally, evaluate the performance of the model by making predictions and computing the accuracy of those predictions.\n\npreds <- predict(model)\naccuracy <- compute_accuracy(preds, features$is_setosa)\n\ncat(paste0(\"Model accuracy is \", accuracy, \"%\"))\n\nModel accuracy is 85%\n\n\nAnd that‚Äôs it ‚Äì you have a simple retraining job. This is a very minimal example, but this general framework is very flexible and modular, and it makes up the foundation for how we write our retraining jobs at CollegeVine. You can plug and play all different kinds of feature engineering logic, logic to fetch raw data, metrics, etc. We also use MLFlow for versioning models and tracking experiments, so our retraining jobs have lots of logging of artifacts, parameters, and metrics to our MLFlow instance.\n\n\nA Monitoring Job\nNext, let‚Äôs imagine we want to monitor out-of-sample performance of the model. Let‚Äôs modify the table with our raw data in the database for this, just for the sake of an example.\n\ntransaction_result <- dbWithTransaction(\n  conn = conn,\n  {\n    dbExecute(\n      conn = conn,\n      \"\n      ALTER TABLE blog.iris\n      ADD COLUMN created_at TIMESTAMP\n      \"\n    )\n    \n    dbExecute(\n      conn = conn,\n      \"\n      UPDATE blog.iris\n      SET created_at = NOW() - random() * INTERVAL '2 weeks'\n      \"\n    )\n  }\n)\n\nGreat, and now let‚Äôs make one or two small modifications to our code from above that pulled the raw data from the data warehouse.\n\nfetch_raw_data <- function(conn, created_after = '1970-01-01 00:00:00') {\n  dbGetQuery(\n    conn = conn,\n    sprintf(\n      \"\n      SELECT *\n      FROM blog.iris\n      WHERE \n        species IN ('setosa', 'virginica')\n        AND created_at > '%s'\n      \",\n      created_after\n    )\n  )\n}\n\nAll we‚Äôve done here is added the ability to specify a created_at date to use as the cutoff point, where we‚Äôd only include records in our raw data that were created after said point. In practice, this lets us filter our raw data down to only records that were created after the model was trained (the out-of-sample data).\n\n## In practice, this would be set at training time and \"frozen\"\n## possibly by logging the value as a parameter in the MLFlow run\nmodel_trained_at <- median(fetch_raw_data(conn)$created_at)\n\nAnd now that we‚Äôve artificially created a trained_at date for the model, we can run our monitoring job. It‚Äôs quite simple, and very similar to the retraining job. All we do here is pull raw data that has been created since the model was trained, run the feature engineering pipeline, make predictions, and compute the accuracy of the model out-of-sample.\n\nraw <- fetch_raw_data(conn, created_after = model_trained_at)\n\nfeatures <- create_features(raw)\npredictions <- predict(model, features)\n\naccuracy <- compute_accuracy(predictions, features$is_setosa)\n\ncat(paste0(\"Out-of-sample accuracy is \", accuracy, \"%\"))\n\nOut-of-sample accuracy is 82%"
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-01-library-code/library-code.html#tying-it-together",
    "href": "posts/series/doing-data-science/2023-04-01-library-code/library-code.html#tying-it-together",
    "title": "Writing Internal Libraries for Analytics Work",
    "section": "Tying it Together",
    "text": "Tying it Together\nThe key piece to notice is how much we‚Äôre leveraging our library code in both the retraining and monitoring job. In both cases, we‚Äôre doing some very similar things ‚Äì pulling raw data, creating features, making predictions, computing accuracy ‚Äì so it makes a lot of sense that we‚Äôd want to reuse the code for those two jobs.\nThere might also be more use cases for the code: More retraining or monitoring jobs, REST APIs, ETL jobs, etc. The more times you need to rely on the same logic, the more benefit you‚Äôll derive from having a single source of truth for all of the logic for your modeling process.\nIt also might be useful to separate this library from the common library proposed at the start. There are important tradeoffs to consider: On one hand, a single library might be convenient for having all of your logic in a single place. But on the other hand, as your library grows in scope, it‚Äôll necessarily have a bigger footprint, rely on more dependencies, etc. which will make its use and maintenance more difficult. A happy middle ground for us has been having a single library per ‚Äúmodel‚Äù or use case. For instance, at CollegeVine we have a package called mlchancing for our chancing model and a separate package called schoolrecommendr for school recommendations and affinity scoring. Keeping these separate has made it easier to iterate on each model individually while also not being a maintenance or ramp-up headache.\nIt‚Äôs my view that models and other analytics work that is in production is software, and should be treated as such. If a model is going to be shipped to production, it at the very least needs to be tested, documented, versioned, and put through some kind of CI/CD process. It‚Äôd be even better if it‚Äôs monitored automatically so that the data scientists working on it can be notified quickly if things start going wrong. Ultimately, writing library code for your modeling work is very well-suited to meeting all of these expectations. And it also just makes everyone‚Äôs lives easier by not needing to reinvent the wheel all the time."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#introduction",
    "href": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#introduction",
    "title": "Unit Testing Analytics Code",
    "section": "Introduction",
    "text": "Introduction\nUnit testing was a concept I had never even heard of before I started my second data science job. It never came up in any of my college statistics or computer science courses. It never came up in any of my data science internships. It never came up in my first data science job.\nIn conversations I have with friends ‚Äì and, broadly, conversations with basically anyone doing data science or analytics ‚Äì I face lots of pushback when it comes to unit testing. Usually the objections come in the form of either not knowing why you might test, since the code is just so simple and straightforward that nothing could go wrong, or not understanding the value added. In my opinion, both of these objections come from the same place. At first glance, it seems like some combination of blissful ignorance about what could go wrong and overconfidence in one‚Äôs own ability or in their code‚Äôs correctness, but I think that the objections actually come from something deeper. In my opinion, it‚Äôs the unfamiliarity of testing. It‚Äôs not something that‚Äôs commonly taught to people involved in analytics, and so it feels new. That can be scary."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#whats-a-unit-test",
    "href": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#whats-a-unit-test",
    "title": "Unit Testing Analytics Code",
    "section": "What‚Äôs A Unit Test?",
    "text": "What‚Äôs A Unit Test?\nFirst thing‚Äôs first: What‚Äôs a unit test? It‚Äôs actually really simple! Unit testing tests the smallest possible components of your code for correctness and what I would define as ‚Äúgood behavior.‚Äù In analytics, you might test things like feature engineering steps, metric definitions, or data wrangling code. The basic idea of a unit test is that you take a function you‚Äôve written, and you‚Äôd make up some inputs to the function and then check if your code produces the outputs you‚Äôd expect. In the simplest case, you might test the identity function as follows:\n\nidentity <- function(x) {\n  x\n}\n\ntest_that(\"The identity returns the input\", {\n  expect_identical(1, identity(1))\n  expect_identical(\"foo\", identity(\"foo\"))\n  expect_identical(1:100, identity(1:100))\n  expect_identical(\n    lm(iris$Sepal.Length ~ iris$Sepal.Width), \n    identity(lm(iris$Sepal.Length ~ iris$Sepal.Width))\n  )\n})\n\nTest passed üò∏\n\n\nAll we‚Äôre doing here is checking that for some given input, our function returns the expected (correct) output. You can also test that your function returns an error or warning, returns nothing, returns some output, and much more. I‚Äôd highly recommend looking at the documentation for testthat or pytest to get a sense for what and how to test."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#case-study-feature-engineering",
    "href": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#case-study-feature-engineering",
    "title": "Unit Testing Analytics Code",
    "section": "Case Study: Feature Engineering",
    "text": "Case Study: Feature Engineering\nData scientists very often write feature engineering code. And as it turns out, feature engineering is a very common place where bugs can pop up unbeknownst to the code‚Äôs author.\n\n\n\n\n\n\nIt‚Äôs important to note that data scientists are usually writing Python or R, which are both dynamically typed, interpreted languages. Unit testing is doubly valuable in these types of languages, since you don‚Äôt get the benefits of a compiler and static type checks to catch issues in your code. In languages like Python and R, anything can go wrong in your code. And you often won‚Äôt find out about issues until runtime, or, depending on the nature of the bug, even later (if ever).\n\n\n\n\nAn Example\nLet‚Äôs write some example code to create a simple feature to use in a hypothetical machine learning model downstream.\n\ncreate_feature <- function(x) {\n  (x - mean(x)) / sd(x)\n}\n\nThis might look familiar: It‚Äôs a function that takes a variable x and standardizes it. Now I‚Äôll generate some data to show how it works.\n\n## 1000 draws from a Normal(25, 5)\nraw <- rnorm(1000, mean = 25, sd = 5)\n\nWe can plot a histogram of our data to show what it looks like:\n\nhist(raw, breaks = 20)\n\n\n\n\n\n\n\n\nNow, let‚Äôs run raw through our feature engineering function.\n\nstd <- create_feature(raw)\n\nhist(std, breaks = 20)\n\n\n\n\n\n\n\n\nGreat! Now std, the new variable that we created from standardizing raw, looks like it follows a standard normal distribution.\n\n\nTrivial Examples, Unexpected Results\nNow that we‚Äôve written a function to create a feature, let‚Äôs use that feature engineering step on a couple of real-world examples. Keep in mind that as data scientists, we‚Äôre often working with messy data ‚Äì it could include missing values, outliers, data of an incorrect type, etc. and there are often very few guarantees about what our data will look like in practice. These next few examples show how things could go wrong (often very quietly) in our seemingly correct feature engineering step we wrote above.\n\nMissings\nLet‚Äôs do the simplest thing first: What happens when we have missing values in our data?\n\ncreate_feature(c(1, 2, 3, NA_real_))\n\n[1] NA NA NA NA\n\n\nIf you‚Äôre familiar with R, this should be expected. And you‚Äôre probably thinking to yourself that we just need to set na.rm = TRUE, and you‚Äôd be right! But this brings me to the first major point I‚Äôd like to make on how things can go wrong.\n\n\n\n\n\n\nWhen you‚Äôre writing your code, it‚Äôs easy to forget things like adding na.rm = TRUE when the data you‚Äôre working with doesn‚Äôt appear to need it. It‚Äôs probably not your default behavior to remember to always set the flag to tell R to remove NA values, since if you‚Äôre not working with any of them, why would you remember to do that? Expecting yourself to remember to do something like this is a recipe for very brittle, error-prone code.\n\n\n\nLet‚Äôs fix this bug the na√Øve way.\n\ncreate_feature <- function(x) {\n  (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)\n}\n\nGreat, and now let‚Äôs test it.\n\ncreate_feature(c(1, 2, 3, NA_real_))\n\n[1] -1  0  1 NA\n\n\nLooks good. Now all of our non-missing values have been correctly transformed.\n\n\nZero Standard Deviation\nHow about another non-trivial bug: A standard deviation of zero. Consider a toy example: Predicting whether or not someone plays basketball.\nIn this example, let‚Äôs say we have both men and women in our training data, and we want to use the height of each person to predict whether that person plays basketball. It probably doesn‚Äôt make sense to standardize the heights without grouping, since we don‚Äôt want to end up with a distribution of heights where most of the women are below average and most of the men are above average. It‚Äôd be much smarter to standardize within group, which would then produce a measure of height relative to the ‚Äúcompetition‚Äù in some sense. Let‚Äôs give that a shot on a trivial data set where every man is the same height.\n\nlibrary(tibble)\nlibrary(dplyr)\n\ndata <- tibble(\n  player_id = 1:6,\n  sex = c(\"M\", \"M\", \"M\", \"F\", \"F\", \"F\"),\n  height = c(77, 77, 77, 70, 71, 72)\n)\n\ndata %>%\n  group_by(sex) %>%\n  mutate(\n    height_std = create_feature(height)\n  ) %>%\n  ungroup() %>%\n  pretty_print()\n\n\n\n \n  \n    player_id \n    sex \n    height \n    height_std \n  \n \n\n  \n    1 \n    M \n    77 \n    NaN \n  \n  \n    2 \n    M \n    77 \n    NaN \n  \n  \n    3 \n    M \n    77 \n    NaN \n  \n  \n    4 \n    F \n    70 \n    -1 \n  \n  \n    5 \n    F \n    71 \n    0 \n  \n  \n    6 \n    F \n    72 \n    1 \n  \n\n\n\n\n\nSo what happened here? In the male group, all of the heights were the same. This resulted in a standard deviation of zero when we went to standardize our height variable, which meant dividing by zero. So we get NaN back ‚Äì Not a Number.\nThis example feels trivial, but in practice it‚Äôs not. If your data had hundreds or thousands of groups, it‚Äôs not all that unlikely to have a situation like this. And note that R doesn‚Äôt throw any kind of warning or error, it just quietly returns you NaN when you‚Äôre expecting a real number.\n\n\n\nInfinity\nR also has a built in value for infinity: Inf. What happens if we try to build our feature when we had an Inf in our data?\n\ncreate_feature(c(1, 2, 3, Inf))\n\n[1] NaN NaN NaN NaN\n\n\nA bunch of NaN. This seems trivial too, but imagine the following example, which you very well might run into in the real world: Computing GDP per capita.\n\ndata <- tibble(\n  gdp = c(1000, 2000, 3000),\n  population = c(0, 50, 100)\n)\n\ndata %>%\n  mutate(\n    gdp_per_capita = gdp / population\n  ) %>%\n  mutate(\n    gdp_per_capita_std = create_feature(gdp_per_capita)\n  ) %>%\n  pretty_print()\n\n\n\n \n  \n    gdp \n    population \n    gdp_per_capita \n    gdp_per_capita_std \n  \n \n\n  \n    1000 \n    0 \n    Inf \n    NaN \n  \n  \n    2000 \n    50 \n    40 \n    NaN \n  \n  \n    3000 \n    100 \n    30 \n    NaN \n  \n\n\n\n\n\nR doesn‚Äôt yell at you for dividing by zero unlike Python for instance, or virtually any other self-respecting programming language. This means that if you accidentally divide by zero somewhere in your data pipeline, you could very well end up with a bunch of NaN if you standardize. And working with real-world data means that dividing by zero happens fairly often, generally because of small issues in the data we‚Äôre working with.\n\nOne Row Per Group\nBack to grouping: It‚Äôs also pretty likely when working with real-world data that you might have a group with only one row. Let‚Äôs see what happens in that case.\n\ndata <- tibble(\n  group = c(1, 2, 2, 3, 3),\n  value = 1:5\n)\n\ndata %>%\n  group_by(group) %>%\n  mutate(\n    value_std = create_feature(value)\n  ) %>%\n  ungroup() %>%\n  pretty_print()\n\n\n\n \n  \n    group \n    value \n    value_std \n  \n \n\n  \n    1 \n    1 \n    NA \n  \n  \n    2 \n    2 \n    -0.7071068 \n  \n  \n    2 \n    3 \n    0.7071068 \n  \n  \n    3 \n    4 \n    -0.7071068 \n  \n  \n    3 \n    5 \n    0.7071068 \n  \n\n\n\n\n\nOnce again, same issue. A single-row group returns NA, since the standard deviation of a single number isn‚Äôt defined.\n\n\n\nLessons\nWhat have we learned here?\nIn short, there are many ways for things to quietly go wrong in your data pipelines, especially in a language like R. Even with a function as simple as standardization, it‚Äôs easy to cook up all kinds of possible corner cases or other issues that would cause your function to return any number of unpredictable results. And when working with real-world data, some of these quirks are inevitable. It‚Äôs virtually impossible that you‚Äôd never run into any of the issues enumerated above when working with messy data for any amount of time. And these examples were about as simple as they could be. In the real world, analytical code is often far more complicated and data far messier than this, which compounds the likelihood of issues like these."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#enter-testing",
    "href": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#enter-testing",
    "title": "Unit Testing Analytics Code",
    "section": "Enter Testing",
    "text": "Enter Testing\nAnd with all of that: Back to testing. Testing is the only way to know that your code is actually doing what you think it‚Äôs doing, and writing tests is a great way to make guarantees about the ability of your code to handle some of these issues that we‚Äôve discussed. Writing tests also lets you ensure that your implementation is correct, and it lets you refactor your code more easily by loudly alerting you if you‚Äôve broken something, which should make you much more confident in the correctness of your implementation.\n\n\n\n\n\n\nIf you‚Äôre not familiar with it, this is a good point to introduce Test-Driven Development. It‚Äôs not something I would always recommend, but at least being familiar with it might make for a good starting point when it comes to learning how to think about writing tests for your code\n\n\n\n\nTesting Our Standardization Function\nAnd with that, let‚Äôs write some tests. Imagine that we knew that we wanted to write our feature engineering function to do standardization. We also know that we want to avoid some of the bugs that we introduced in the examples above. One possible option for handling those cases where our function will do something unexpected would be to throw an error if we get unexpected inputs. That‚Äôs what I‚Äôll illustrate here ‚Äì it‚Äôd let the user learn about the issue quickly and debug. There are many options for how to handle these issues though. You might also fall back on a default value when your code will return a NA or NaN, for instance. Now, on to the tests.\n\nlibrary(testthat)\n\ntest <- function() {\n  test_that(\"Standardized variable is transformed correctly\", {\n    random_normal <- rnorm(1000, 10, 5)\n    \n    ## Expect that the mean of the transformed data is within\n    ## 0.005 of zero\n    expect_lt(\n      abs(mean(create_feature(random_normal))),\n      0.005\n    )\n    \n    ## Expect that the stddev of the transformed data is within\n    ## 0.005 of 1\n    expect_lt(\n      abs(1 - sd(create_feature(random_normal))),\n      0.005\n    )\n    \n    ## Expect that the math is done correctly\n    expect_identical(\n      create_feature(c(1,2,3)),\n      c(-1, 0, 1)\n    )\n  })\n  \n  test_that(\"Inifinity causes an error\", {\n    expect_error(\n      create_feature(c(1, 2, 3 / 0)),\n      \"`x` must not contain any infinite values\"\n    )\n  })\n  \n  test_that(\"Zero stddev causes an error\", {\n    expect_error(\n      create_feature(c(1, 1, 1)),\n      \"`x` must have a non-zero standard deviation\"\n    )\n  })\n  \n  test_that(\"Length one causes an error\", {\n    expect_error(\n      create_feature(c(1)),\n      \"`x` must have more than one unique element\"\n    )\n  })\n}\n\nAnd now let‚Äôs run our test suite.\n\ntest()\n\nTest passed ü•≥\n‚îÄ‚îÄ Failure (<text>:29:5): Inifinity causes an error ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n`create_feature(c(1, 2, 3/0))` did not throw an error.\n\n\nError in `reporter$stop_if_needed()`:\n! Test failed\n\n\nUnsurprisingly, we get some failures. So now let‚Äôs refactor our feature engineering function to pass our tests.\n\nlibrary(rlang)\n\ncreate_feature <- function(x) {\n  \n  mu <- mean(x, na.rm = TRUE)\n  sigma <- sd(x, na.rm = TRUE)\n\n  if (isTRUE(sigma == 0)) abort(\"`x` must have a non-zero standard deviation.\")\n  if (isTRUE(length(unique(x)) == 1L)) abort(\"`x` must have more than one unique element.\")\n  if (isTRUE(any(is.nan(x)))) abort(\"`x` must not contain any `NaN` values.\")\n  if (isTRUE(any(is.infinite(x)))) abort(\"`x` must not contain any infinite values.\")\n\n  (x - mu) / sigma\n}\n\ntest()\n\nTest passed ü•≥\nTest passed ü•≥\nTest passed ü•≥\nTest passed ü•≥\n\n\nWoohoo! Our tests all passed. Now, if the user of our function tries to do something like only providing a single value for x, they get an informative error back.\n\ncreate_feature(c(1))\n\nError in `create_feature()`:\n! `x` must have more than one unique element.\n\n\nIn a less trivial example, consider our grouped computation from before:\n\ndata <- tibble(\n  group = c(1, 2, 2, 3, 3),\n  value = 1:5\n)\n\ndata %>%\n  group_by(group) %>%\n  mutate(\n    value_std = create_feature(value)\n  ) %>%\n  ungroup() %>%\n  pretty_print()\n\nError in `mutate()`:\n‚Ñπ In argument: `value_std = create_feature(value)`.\n‚Ñπ In group 1: `group = 1`.\nCaused by error in `create_feature()`:\n! `x` must have more than one unique element.\n\n\nNot only do we get a helpful error, but in this particular case dplyr also adds helpful context: It tells us which step failed and which group it failed on so that we can effectively debug."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#refactoring",
    "href": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#refactoring",
    "title": "Unit Testing Analytics Code",
    "section": "Refactoring",
    "text": "Refactoring\nNow that we have a test suite, we can also refactor our function and be much more confident that we haven‚Äôt broken anything. Let‚Äôs do an overly complicated refactor.\n\ncreate_feature <- function(x) {\n  \n  ## Compute the mean as the sum of the non-null elements \n  ## divided by the number of non-null elements\n  mu <- sum(x, na.rm = TRUE) / length(na.omit(x))\n  sigma <- sd(x, na.rm = TRUE)\n\n  if (isTRUE(sigma == 0)) abort(\"`x` must have a non-zero standard deviation.\")\n  if (isTRUE(length(unique(x)) == 1L)) abort(\"`x` must have more than one unique element.\")\n  if (isTRUE(any(is.nan(x)))) abort(\"`x` must not contain any `NaN` values.\")\n  if (isTRUE(any(is.infinite(x)))) abort(\"`x` must not contain any infinite values.\")\n\n  (x - mu) / sigma\n}\n\nAnd now let‚Äôs run our tests again.\n\ntest()\n\nTest passed ü•≥\nTest passed ü•≥\nTest passed ü•≥\nTest passed ü•á\n\n\nAwesome! We just did a simple refactor and our tests passed, so I feel good about the correctness of that refactor. This was a very simple example, but you could imagine arbitrarily complex refactors. The value of a test suite increases exponentially as the complexity of the code increases."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#wrapping-up",
    "href": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#wrapping-up",
    "title": "Unit Testing Analytics Code",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nThere are a few important takeaways from this post.\nFirst and most importantly, it‚Äôs important to test your code. There are so many ways that things can go wrong, and writing tests is the only way to really be confident that nothing is. This is especially true in dynamically typed, interpreted languages like R or Python, where data scientists don‚Äôt get the benefits of a compiler or a static type system to catch bugs in their code.\nSecond, analytical code that‚Äôs well-written should be easy to test. Ultimately, lots of the code we write for analytics work takes some data, does something to it, and returns some new data. That type of logic makes for a great use case for unit testing: If your code doesn‚Äôt produce the values you expect, it‚Äôs incorrect.\nLastly, there are many tools in Python (pytest, unittest, etc.) and R (testthat) to make unit testing as simple as writing a few lines of logic and an assertion or two. And then you get all the benefits of having well-tested code, such as the newly-found ease of refactoring said code without being concerned about breaking it.\nHappy testing, and enjoy the peace of mind!"
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-10-running-a-b-tests/index.html",
    "href": "posts/series/a-b-testing/2022-04-10-running-a-b-tests/index.html",
    "title": "Running A/B Tests",
    "section": "",
    "text": "This is the second post in a series on A/B testing. In the last post, I gave a high-level overview of what A/B testing is and why we might do it. This post will go a few steps farther. I‚Äôll discuss how an A/B test is run, what we‚Äôre looking for along the way, and what happens when we call it one way or the other. This will set us up for the next post, which will discuss the mechanics of A/B testing."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-10-running-a-b-tests/index.html#how-can-i-run-an-ab-test",
    "href": "posts/series/a-b-testing/2022-04-10-running-a-b-tests/index.html#how-can-i-run-an-ab-test",
    "title": "Running A/B Tests",
    "section": "How Can I Run An A/B Test?",
    "text": "How Can I Run An A/B Test?\nIn the last post, I laid out a hypothetical A/B test where I was considering changing the underline color for links on my blog from red to blue. As a refresher: Blue was the variant (the new proposed color) and red was the control (the current state of the world). We were testing to see if a blue underline would cause significantly more users to click the links to my blog posts. ‚ÄúBut,‚Äù you ask, ‚Äúhow does the test actually happen?‚Äù That‚Äôs a great question! But first, a disclaimer: I‚Äôm not an engineer, so I can only give a bird‚Äôs eye view of how we do it at CollegeVine. I‚Äôm sure there are many other solutions used by other companies.\nAt CV, we use a tool called LaunchDarkly for running A/B tests. Essentially, LaunchDarkly lets us set up ‚Äúfeature flags‚Äù and show the version of the code that‚Äôs ‚Äúbehind‚Äù them to only certain users. For example, you might imagine you were rolling out a risky new change, and wanted to QA it first. One way we‚Äôve done this kind of thing at CV is to put the risky change behind a feature flag, and then roll it out to our own team. Then, our team can QA and if anything looks off we can either fix the issues or revert the changes before rolling out to external users.\nA/B testing with LD works similarly. Instead of only showing a new version of the code to internal users, we use a feature flag that shows each version of the code to a certain proportion of users, at random. The idea is to use the feature flag in LD to randomly sample users of our site into either the control group or the variant group. Then we track metrics over time to see if the variant is outperforming the control group."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-10-running-a-b-tests/index.html#my-experiment-is-running.-now-what",
    "href": "posts/series/a-b-testing/2022-04-10-running-a-b-tests/index.html#my-experiment-is-running.-now-what",
    "title": "Running A/B Tests",
    "section": "My Experiment Is Running. Now What?",
    "text": "My Experiment Is Running. Now What?\nBack to our hypothetical experiment on my blog. Now, half of users are seeing red-underlined links, and half are seeing blue underlines, at random. So now, we need a way to track the conversion rate of those links. In step a whole bunch of business intelligence (BI) tools, and other tools that brand themselves as being tools for all different flavors of analytics. At CV, we use a tool called Heap for user analytics (including A/B testing).\nLet‚Äôs imagine that my blog were wired up to Heap, and tracking page views on my landing page and clicks of the links on that page to individual posts behind the scenes. In Heap, we could visualize the conversion rate from the landing page to any post in a funnel or in a table, where the conversion rate between the two is the proportion of users who hit the landing page that end up clicking one of the links (‚Äúconverting‚Äù) to a post. We could also view these numbers in a table, where we‚Äôd have one cell that has the total number of sessions on the landing page and another cell with the number of sessions on the posts, and the conversion rate is the latter divided by the former (roughly).\nSince we have our feature flag set up to track which users are being placed in each group, we can also group by that ‚Äúproperty‚Äù in Heap, which lets us separate our analysis into the control and the variant. This means that we can compare the conversion rates for the red underline and the blue underline, which is exactly what we‚Äôre trying to do! Generally, we‚Äôll set up a Heap dashboard with the funnel we‚Äôre interested in so we can track out metrics over time."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-10-running-a-b-tests/index.html#interpreting-the-metrics",
    "href": "posts/series/a-b-testing/2022-04-10-running-a-b-tests/index.html#interpreting-the-metrics",
    "title": "Running A/B Tests",
    "section": "Interpreting the Metrics",
    "text": "Interpreting the Metrics\nNow that the funnel is set up, you‚Äôre watching results stream in. Let‚Äôs imagine that at some point in time, each group has 1000 users (i.e.¬†1000 users have seen the variant and another 1000 have seen the control), and 250 users in the variant group converted while only 200 in the control group did. From there, we can calculate our conversion rates as 25% (variant) and 20% (control). And for the purposes of keeping this post simple, let‚Äôs assume that lift is big enough for us (by some definition of ‚Äúbig enough‚Äù, which we‚Äôll get to in a later post). In that case, we call our test for the variant. In practice, this means we route all traffic to the variant instead of splitting it 50/50, and then we can remove the feature flag from our code and boom! We now have some cool blue underlines for the links on the blog.\nBut back to the lift being big enough: In practice, is knowing that the variant is performing 25% better than the control enough to call the test for the variant? Making this call in a rigorous, informed way is what the rest of the posts in this series will discuss."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-10-calling-a-b-tests/index.html",
    "href": "posts/series/a-b-testing/2022-04-10-calling-a-b-tests/index.html",
    "title": "Calling A/B Tests",
    "section": "",
    "text": "In the last post, I gave a bird‚Äôs eye level overview of the mechanics of running an A/B test. But at the end, we reached a problem: We had two conversion rates ‚Äì 20% and 25% ‚Äì but we didn‚Äôt know if the difference between those was really big enough to make a strong claim that the blue underlines were actually performing better than the red ones in some real world sense. If you‚Äôre asking yourself whether the five percentage point difference between the two conversion rates is statistically significant, then your head‚Äôs in the right place.\nIn this post, we‚Äôll discuss how we can determine whether our test results are statistically significant. But since statistical significance is an often confusing and nebulous topic, we‚Äôll also explore what statistical significance even is (including what p-values are), when it‚Äôs important, and when it might not be."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-10-calling-a-b-tests/index.html#statistical-significance",
    "href": "posts/series/a-b-testing/2022-04-10-calling-a-b-tests/index.html#statistical-significance",
    "title": "Calling A/B Tests",
    "section": "Statistical Significance",
    "text": "Statistical Significance\nMisunderstandings about statistical significance run rampant. It‚Äôs not a reach for me to say that the majority of the time I hear someone mention that something is ‚Äústatistically significant‚Äù I end up rolling my eyes. But before we get into common mistakes and misunderstandings, we need to first establish what statistical significance actually is.\nIntuitively, if something is statistically significant, it‚Äôs unlikely to have happened due to random chance. Not that scary, after all! How unlikely, though, varies wildly depending on the setting. For instance, if we‚Äôre running clinical trials to determine if a new drug is capable of curing cancer, then we want it to be very unlikely that we make a consequential mistake and claim that the drug works when it actually doesn‚Äôt.\nWe use p-values as the indicator of the likelihood of our result being due to random chance. In this instance, we would run our test using the number of page views and the number of conversions for each group, and depending on how we ran our test we might get a p-value of 0.43% back. What this p-value actually means is that the probability of seeing the difference in conversion rates between groups that we do (five percentage points) due to purely random chance is 0.43%. A p-value threshold of 5% is very common, so in this case we‚Äôd call the test for the variant (since 0.43% is below 5%), and we‚Äôd assert that this difference in conversion rates is statistically significant."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-10-calling-a-b-tests/index.html#eye-rolling",
    "href": "posts/series/a-b-testing/2022-04-10-calling-a-b-tests/index.html#eye-rolling",
    "title": "Calling A/B Tests",
    "section": "Eye Rolling",
    "text": "Eye Rolling\nBack to my eye rolling: I often roll my eyes when someone claims that something is statistically significant for two reasons.\nFirst and foremost: Something being statistically significant does not mean that thing is significant. Often we get so hung up on things being statistically significant that we forget that lifting some metric by 0.0001% isn‚Äôt practically significant, since it won‚Äôt make any difference in the end. If 0.0001% more people read my blog posts, what do I care? That‚Äôs something like 1 extra person every hundred years (optimistically).\nSecondly, I often roll my eyes because of the number of choices and assumptions that need to be made along the way, many of which tend to be difficult to defend. One choice, as previously mentioned, is the p-value threshold (alpha) that you choose. In some instances, we want to be very confident that we‚Äôre not leaning into results that are the result of random chance, and so we might use a lower threshold. In other cases, we might be okay with taking on more risk of a false positive result in order to run our tests faster and mitigate the risk of a false negative (saying something does not help when it actually does).\nAnother thing that will affect the results we see is the type of test we‚Äôre running: one-tailed or two-tailed. Often, online calculators like this one will use two-tailed tests by default because they‚Äôre more conservative. But in my opinion, using a two-tailed test doesn‚Äôt actually make any sense. Here‚Äôs why: A two-tailed test checks if the conversion rates of the variant and the control are not equal, which means that we can get a statistically significant result if the variant is significantly worse than the control, in addition to if it‚Äôs significantly better. But in A/B testing, we‚Äôre only going to call a test for the variant when it‚Äôs significantly better, so why do we care about the case where it‚Äôs worse? We want to test the hypothesis that the variant is significantly better than the control, not that it‚Äôs not equal, and that‚Äôs what a one-tailed test does. If you use two-tailed tests, it‚Äôll be harder to get significant results without any real benefits.\n\nYet another consideration is how the statistical test was actually conducted. For instance, if you use a Chi-square test with Yates‚Äôs continuity correction (the default in R, although a little controversial among statisticians), you‚Äôll end up with higher (more conservative) p-values than if you don‚Äôt correct, which is why the p-value I just reported is higher than the one you‚Äôd get from most online calculators that don‚Äôt use the correction.\n\nFinally, and most importantly, is that the mechanics of running the test actually affect the chance that you are reporting a false positive result. For example, if you were to run the test described in the past few posts and calculate the p-values every time a new user visited the page and call the test for the variant the first time it were significant, you‚Äôd have just blown up the chances of a false positive result."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-10-calling-a-b-tests/index.html#a-common-mistake",
    "href": "posts/series/a-b-testing/2022-04-10-calling-a-b-tests/index.html#a-common-mistake",
    "title": "Calling A/B Tests",
    "section": "A Common Mistake",
    "text": "A Common Mistake\nThe most common mistake I see that‚Äôs made by people running A/B tests is using the ‚Äúcall it when it‚Äôs significant‚Äù heuristic. As I mentioned before, checking in on your test often and calling it for the variant the first time you get a significant p-value is a huge problem because the false positive rate of your test compounds the more you check on it. The reason for this is a statistical concept called multiple testing, and there‚Äôs an XKCD comic about it!\nSo we want to avoid checking the test all the time, but this raises another problem: If we can‚Äôt check our test all the time, how do we know when to call it? And this is where test planning comes in. There are a number of online test planners (which generally make shoddy assumptions, like that you‚Äôre running a two-tailed test when you should be running a one-tailed one instead) like this one that take a few parameters and tell you how long to run your test for. And these planners are great! The idea is that if you can plan your test in advance, given that you know your baseline conversion rate and can specify how big of a lift you‚Äôre shooting for, then all you have to do is wait until you hit the sample size number that the calculator gives you back. Once you hit it, you check in on your test, run your p-value calculation, and call the test.\nSo, problem solved, right? Well, not quite. Because while we‚Äôve solved the multiple testing problem where we blow up our false positive rate by checking the test all the time, now we have a new issue: We have to wait until we hit some (potentially big) sample size before we can call our test, and that‚Äôs problematic for teams that want to iterate quickly.\nThe next post in this series is the punch line. It‚Äôll discuss sequential testing, which is the methodology that makes up the guts of how we run A/B tests at CollegeVine. Sequential testing solves the problem of needing to wait until you hit a final sample size to call your test without making any sacrifices on the rigor front, which means you can call your tests quickly and reliably."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-03-25-a-b-testing-a-primer/index.html",
    "href": "posts/series/a-b-testing/2022-03-25-a-b-testing-a-primer/index.html",
    "title": "A/B Testing: A Primer",
    "section": "",
    "text": "This is the first post in a series I‚Äôm planning on writing on A/B testing. In this post, I‚Äôll lay out a top-level overview of what A/B testing is and why companies do it. In future posts, I plan on diving into some common pitfalls, bad habits, and anti-patterns I‚Äôve seen, and the systems we‚Äôve put in place to mitigate them and allow our team to run statistically rigorous, fast A/B tests to make informed product decisions as quickly as possible.\nAt work, we generally try to keep documents like this written at a high level: The objective is for them to be understandable and useful for general audience. That will be the case here too, for the most part."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-03-25-a-b-testing-a-primer/index.html#whats-an-ab-test",
    "href": "posts/series/a-b-testing/2022-03-25-a-b-testing-a-primer/index.html#whats-an-ab-test",
    "title": "A/B Testing: A Primer",
    "section": "What‚Äôs an A/B Test?",
    "text": "What‚Äôs an A/B Test?\nSo, what is an A/B test, anyways? It‚Äôs probably easiest to explain with an example:\nLet‚Äôs imagine that I had been tracking the click rate on my blog posts over time. It‚Äôs pretty terrible ‚Äì let‚Äôs say that the rate that someone clicks into any particular post from the main menu page is 5%. This means that of all of the views of my blog‚Äôs main page, only 5% of those page views actually result in a click on one of my posts. Pretty miserable, right?\nBut today I‚Äôm feeling optimistic. Right now, when a user hovers over a post title, it gets underlined in red. ‚ÄúBut wait!‚Äù I think. What would happen if I made the underline blue instead?\nAnd now, I have an A/B test. In this test, the ‚ÄúA‚Äù group (or the ‚Äúcontrol‚Äù) is the current state of the world: The red underline. The ‚ÄúB‚Äù group (or the ‚Äúvariant‚Äù or ‚Äútreatment‚Äù group) is the proposed change: The blue underline.\nThe basic idea of an A/B test is to run these two versions of my blog side-by-side, measuring the click rate in each version, and seeing which version ends up performing better. If the blue underline version ‚Äì the variant ‚Äì ends up increasing the click rate to my blog posts, then the conclusion is that I‚Äôd be better off permanently changing the underline to blue."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-03-25-a-b-testing-a-primer/index.html#why-test",
    "href": "posts/series/a-b-testing/2022-03-25-a-b-testing-a-primer/index.html#why-test",
    "title": "A/B Testing: A Primer",
    "section": "Why Test?",
    "text": "Why Test?\nIn my trivial example above, the color of the underline doesn‚Äôt seem super consequential (and it‚Äôs not). But this isn‚Äôt always the case. For instance, Facebook changed their notification icon color from blue to red once upon a time, and the rest was history. Amazon might A/B test a new model for recommending products to users, or Netflix a new model for recommending shows. A company doing lots of email marketing might A/B test different types of ways of addressing their emails (e.g.¬†‚ÄúDear Matt‚Äù vs.¬†‚ÄúHey Matt‚Äù), and so, so much more. Changes like these can have enormous business implications, and, as such, A/B testing makes up the backbone of so much of the tech and products we interface with every day. Companies want to maximize their conversion rates, click rates, revenues, etc. and A/B testing is one tool in their tool box for optimizing all of the metrics they care about.\nIf there‚Äôs one takeaway here, it‚Äôs this: Someone wants to make their product ‚Äúbetter‚Äù in some sense, and to figure out whether or not a new idea of theirs is actually better than the current state of the world, they test it.\n\nIn statistics world, generally A/B tests boil down to testing ‚Äúconversion rates‚Äù against each other, which usually means that the tests being run are Chi-square tests of independence of the proportions of success across the two groups. If the variant is significantly better than the control, we call the test for the variant and roll it out to 100% of users. You might also use a t-test to (e.g.) test if the variant results in significantly more sessions than the control does, or you might use a time series technique like Bayesian structural time series to do pre/post testing to compare user behavior before and after a treatment was applied. For the curious, Google has published an awesome R package called CausalImpact (and an associated talk and some papers, I believe) on this."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-03-25-a-b-testing-a-primer/index.html#up-next",
    "href": "posts/series/a-b-testing/2022-03-25-a-b-testing-a-primer/index.html#up-next",
    "title": "A/B Testing: A Primer",
    "section": "Up Next‚Ä¶",
    "text": "Up Next‚Ä¶\nAs I mentioned before, the rest of this series of posts will focus, roughly, on the following topics: 1. Okay, so we know what an A/B test is, but how do we actually run one? 2. What are the most common anti-patterns, pitfalls, and bad habits that I‚Äôve seen, and why are they problematic? 3. What are we doing to correct those issues to allow our team to run fast, statistically rigorous A/B tests?"
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-17-sequential-testing/index.html",
    "href": "posts/series/a-b-testing/2022-04-17-sequential-testing/index.html",
    "title": "Sequential Testing",
    "section": "",
    "text": "The last post proposed a solution to the multiple testing problem that often invalidates A/B test results test planning. The idea is to calculate the sample sizes you need for your test in advance, and then wait for your control and variant groups to hit those sample sizes in order to call the test. This approach is a significant methodological improvement from the ‚Äúcall it when it‚Äôs significant‚Äù heuristic: It prevents you from compounding the false positive rate of your test by checking on it all the time.\nBut there‚Äôs a different issue with planning the test in advance and running it until the end: It‚Äôs slow. I use ‚Äúslow‚Äù to mean ‚Äúslower than it needs to be,‚Äù in the sense that you will likely end up waiting too long to call a test for the variant when you could‚Äôve made the call earlier. This waiting around is expensive ‚Äì the difference between running a test for a day or two and a week or two matters a lot for the teams and businesses running the tests. Often, this big of a time difference can have massive effects on metrics, revenue, learnings, etc., so teams benefit from being able to call their tests faster without sacrificing any statistical rigor.\nBut how do we do that without checking in on the test all the time? Enter sequential testing."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-17-sequential-testing/index.html#sequential-testing",
    "href": "posts/series/a-b-testing/2022-04-17-sequential-testing/index.html#sequential-testing",
    "title": "Sequential Testing",
    "section": "Sequential Testing",
    "text": "Sequential Testing\nSequential testing is a method for running experiments that allows us to evaluate the results of the test we‚Äôre running along the way instead of waiting to hit a pre-determined sample size. Intuitively, you might think about sequential testing like this: If early on in my test I see a massive lift in my metric, I should be able to use a lower p-value than the one I set at the start of my test to call it. It‚Äôs earlier, hence the lower p-value, but the intuitive idea is that the metric lift is so big that the p-value we‚Äôd see would be smaller than some yet undetermined p-value threshold, such that we could call the test.\nIn A/B testing world, this boils down to building checkpoints into our tests. For instance, imagine you have a test that you‚Äôre expecting to take six days to hit the final sample size that you need. If you build in three checkpoints, then you can check in on your test on day two, day four, and day six (the end of the test). On day two, if the p-value for your test is lower than the pre-determined day two p-value needed, you call the test. If it‚Äôs not, you move on to day four and repeat. Once you get to day six, if the test is still insignificant you call it for the control and end the test.\nThis gives us the best of both worlds: We have a setup where we can call the test on day two if the lift is big enough, but we can do so without inflating the false positive rate of our test. In practice, this means often being able to call tests in half, a third, a quarter, etc. of the time it‚Äôd otherwise take, which is hugely valuable for the team running the test.\n\nStatistical note: There are a number of ways to determine what p-value to use at each checkpoint when planning the test. We use the R package rpact for planning tests, and we plan our tests using the O‚ÄôBrien-Fleming method (with alpha spending). This results in p-value thresholds that increase over time and asymptote to a value slightly less than the initial alpha you specified, depending on the number of checkpoints you build into your test. Another popular method is Pocock‚Äôs approach."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-17-sequential-testing/index.html#in-practice",
    "href": "posts/series/a-b-testing/2022-04-17-sequential-testing/index.html#in-practice",
    "title": "Sequential Testing",
    "section": "In practice",
    "text": "In practice\nSo how does this work in practice? We build an internal tool that lets you plan a test given a few inputs:\n\nThe alpha level (we generally use 20%, since we‚Äôre not particularly afraid of false positives and want to be able to run tests quickly)\nThe power (we generally use 95%, since we don‚Äôt want to take on many false negatives)\nThe minimum detectable effect\nThe baseline conversion rate\nThe expected number of users entering the funnel per day\nThe number of checkpoints to build in\nThe split of the test (is it 50/50?)\nThe number of variants (is it a true A/B test? Are there multiple variants being tested?)\n\nWith those inputs, we generate a test plan which you can save, tie to a JIRA card, and send to Slack. Then all you need to do is turn on your test and wait for it to hit the first checkpoint. Once it does, you evaluate the test to get a p-value, compare it to the p-value threshold that the test plan provided at the first checkpoint, and call the test if it‚Äôs significant. If it‚Äôs not, you keep running the test up to the next checkpoint and do the same thing, and so on."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-17-sequential-testing/index.html#the-bottom-line",
    "href": "posts/series/a-b-testing/2022-04-17-sequential-testing/index.html#the-bottom-line",
    "title": "Sequential Testing",
    "section": "The Bottom Line",
    "text": "The Bottom Line\nThe main takeaway from this post is that sequential testing lets us solve two huge problems in A/B testing simultaneously: It lets us run our tests fast, and it lets us do it without sacrificing any statistical rigor. Too often, I see teams committing atrocities against statistics in the name of moving fast when they don‚Äôt need to be ‚Äì using sequential designs for your A/B tests lets you control the false positive and false negative rates of your A/B tests while also allowing you to make calls on those tests as quickly as possible, which is hugely valuable.\nAnd with that, we‚Äôve concluded a four-part series on A/B testing! Hopefully you found this interesting and useful, and have taken something away that will be beneficial for your own work. Or, if I‚Äôm lucky, maybe you‚Äôre even considering overhauling how you run A/B tests."
  },
  {
    "objectID": "posts/2021-12-22-notes-on-hiring-data-analysts-scientists/index.html",
    "href": "posts/2021-12-22-notes-on-hiring-data-analysts-scientists/index.html",
    "title": "Notes on Hiring Data Analysts + Scientists",
    "section": "",
    "text": "In the past four months, I‚Äôve been involved in hiring for two new roles at CollegeVine: a second data scientist, and our first data analyst. I‚Äôve learned a lot along the way: Things that work, things that don‚Äôt, and things to ask in order to maximize the amount of signal we‚Äôre getting from our interviews. This post will sketch out our hiring process (the processes are very similar for our DS and DA roles, with slightly different questions), and I‚Äôll add some notes about things I‚Äôve learned along the way. It‚Äôs been a long time since I‚Äôve written anything! I‚Äôm excited, so here goes."
  },
  {
    "objectID": "posts/2021-12-22-notes-on-hiring-data-analysts-scientists/index.html#our-process",
    "href": "posts/2021-12-22-notes-on-hiring-data-analysts-scientists/index.html#our-process",
    "title": "Notes on Hiring Data Analysts + Scientists",
    "section": "Our Process",
    "text": "Our Process\nOur hiring process only has a few steps. We try to keep things simple and quick:\n\nA phone screen with the hiring manager (30-45 minutes)\nA technical interview with two data scientists (60-90 minutes)\nA technical interview with two software developers (60 minutes, data science only)\nTwo cultural / behavioral interviews\n\nSince my portion of the process is the data science part, I‚Äôll leave the phone screens, behavioral rounds, and system design round out of this post."
  },
  {
    "objectID": "posts/2021-12-22-notes-on-hiring-data-analysts-scientists/index.html#the-data-science-analysis-interview",
    "href": "posts/2021-12-22-notes-on-hiring-data-analysts-scientists/index.html#the-data-science-analysis-interview",
    "title": "Notes on Hiring Data Analysts + Scientists",
    "section": "The Data Science / Analysis Interview",
    "text": "The Data Science / Analysis Interview\nI‚Äôve done a whole bunch of data science interviews from both sides of the table. Some have been better than others. Often, while on the hot seat, I‚Äôve gotten a smorgasborg of questions that felt like they came from the first page of the first Google result for ‚ÄúData science interview questions.‚Äù A handful of examples:\n\nWhat‚Äôs the difference between supervised and unsupervised learning?\nWhat‚Äôs the difference between linear regression and logistic regression?\nWhat‚Äôs a p-value?\nHow do you know when something is an outlier?\n\nAnd many, many more. In my view, these questions are fine. They ask about a relevant concept to data science and put your skills to the test. But I have two issues with them. First, they‚Äôre neither challenging nor questions that are easy to build on in terms of difficulty. Second, they‚Äôre not going to be good questions to figure out someone‚Äôs ability level.\n\nCreating Challenging Questions\nWhen I say the questions above aren‚Äôt challenging, I mean that these are the kinds of questions that someone who‚Äôs taken a single stats or machine learning class would be able to answer. This is fine if you‚Äôre trying to figure out if someone has taken one of said classes, but that isn‚Äôt our goal. We‚Äôre trying to determine your ability to succeed as a data scientist or analyst. This means that we need to know more than just your academic background: We need to know how you reason about stats and ML, and how you think in general. How do you approach a statistics problem where you haven‚Äôt seen the solution before? Can you intuit an answer and defend it?\nAs a result, we‚Äôve designed our questions to be challenging enough that you wouldn‚Äôt have seen them in a class you took, and to build off of one another. For example, we often ask the following question:\n\nImagine you‚Äôre looking at a distribution of the heights of men in Spain. How might you find which men are outliers?\n\nSure, easy question. There are a handful of fine answers here, but we‚Äôre generally looking for something along the lines of using a Z-Score or the IQR. Either of those shows that you‚Äôve seen some rule of thumb in a statistics class and realize that you can apply it to this problem.\nBut then, we ask a follow-up:\n\nNow, let‚Äôs imagine you‚Äôre looking at the distribution of incomes of men in Spain, instead of their heights. Does your approach change at all?\n\nThis is a question that most candidates need to think about for a little bit. At first, it seems simple. But we give you a hint: We explicitly ask if your approach to the problem would change, which is a push to think about how it might change. Many candidates struggle with this question, seemingly for a few reasons:\n\nThey understand the hint, but don‚Äôt immediately realize why the Z-Score or the IQR approach breaks down, so they feel stuck.\nThey understand why those approaches don‚Äôt work, but it doesn‚Äôt jump out at them what they should do about it.\n\nThese types of responses aren‚Äôt surprising to us: In statistics classes, you normally work with normally distributed data where nice properties and rules of thumb hold up, but now we have a problem: Everyone knows that incomes are skewed, and so now what do we do? Some candidates stick to their guns and insist the IQR or Z-score would still be fine. Here‚Äôs how I‚Äôd answer the question:\n\nFirst, I‚Äôd make a point that incomes are right-skewed. Everyone probably has this image in their head already, but it‚Äôs important.\nNext, I‚Äôd note that the IQR / Z-score approach would break down, since the rules of thumb we use about 95% of the data (e.g.) lying within 2 standard deviations of the mean only works on a normal distribution, which incomes do not follow. This means we can‚Äôt just arbitrarly say ‚ÄúOh, this person is more than 2 standard deviations from the mean, so he‚Äôs an outlier!‚Äù anymore.\nFinally, I‚Äôd think about other approaches. One might be something fancy like an isolation forest, but I think there‚Äôs a simpler approach that‚Äôd work: Since incomes are severely right skewed, we could try taking the log of the incomes to see if the logged income looks roughly normally distributed. If it does, we can fall back on our IQR or Z-score approach.\n\nThe point here is to ask the candidate a question that makes them think intuitively about having to solve a real-world problem (in this case, one we face all the time) that they probably haven‚Äôt seen before, which gives us a lot of signal about their statistical foundations and intuitions.\nWe follow up this follow-up with another:\n\nNow, let‚Äôs imagine we‚Äôre looking for outliers in the heights of men again, but this time they‚Äôre from Sweden and Spain. Does your approach change?\n\nSimilar question here, with no real clear, immediate answer that jumps out at most candidates. A reasonable response would be to just separate the distributions into Swedish and Spanish (since we know that Swedes are taller than Spaniards, on average), and then fall back on the Z-score or IQR approach again. Again, not a particularly challenging, in the weeds, or niche technical question by any stretch, but definitely one that lets us really get a sense for your intuitive ability.\n\nTL;DR: We build questions that aren‚Äôt tricky (in the trick question sense), but should‚Äôt have an immediately obvious canned answer. These types of questions should give us a window into how you reason intuitvely about statistics and machine learning concepts at all levels of complexity.\n\n\n\nLayering Questions\nThis point is a nice segue into the second issue I noted above: It‚Äôs important to build questions that have multiple layers of difficulty to them, so that if someone immediately answers your question, you don‚Äôt completely shift gears and move to a different topic. Instead, we want to keep digging, so that we can figure out just how much you know. The question I laid out above is a good example of a simple, relatively straightforward question with multiple layers.\nHere‚Äôs another example:\n\nImagine you‚Äôre asking people their symptoms and trying to figure out if they have COVID or not. Sometimes you‚Äôll say someone has COVID when they don‚Äôt, and sometimes you‚Äôll say they don‚Äôt when they do. What are these two types of mistakes called? And which one do you think is worse? Why?\n\nThis is a question about Type I and Type II error (also known as false positives and false negatives, respectively). Most candidates realize this right away, and then make an argument for why they think a Type II error (the false negative) is a worse mistake. Generally, the argument centers on someone who is infected unknowingly spreading COVID. That‚Äôs a great answer. It shows that they can reason about different types of mistakes and can make an argument for why we might want to minimize one or the other. But this isn‚Äôt a particularly challenging question.\nWe ask a follow-up:\n\nNow, let‚Äôs imagine you get some COVID test results back from the people whose symptoms you were asking about. What‚Äôs wrong with this statement: ‚ÄòIf COVID tests have a 10% false negative rate and you got a COVID test and it‚Äôs negative, that means there‚Äôs a 10% chance it‚Äôs wrong and you‚Äôre actually positive.‚Äô?\n\nThis question is a little more challenging, and it builds of the types of error we discussed in the previous question. Here, you need to realize what a false negative is, and it‚Äôs easy to get the conditioning flipped. In this case, the false negative rate of the test being 10% means that the probability that you test negative given that you have COVID is 10%. This is not the same as saying that the probability that you have COVID given that you test negative is 10%. In the second case, the conditioning is flipped around backwards. Most candidates get hung up on this, and rightfully so. It‚Äôs a tricky question to work out without pen and paper.\nFor those that get through that question, we have another, more difficult follow-up:\n\nOkay, so you got a negative test result, but you know the false negative rate of COVID tests is 10%. Imagine you wanted to calculate the actual chance you were positive given that you just got your negative result. What other information would you need to do your calculation?\n\nFor the Bayesians in the back, this question should be a slam dunk. It‚Äôs an obvious Bayes‚Äô Rule question: we‚Äôre asking you to calculate p( COVID | negative test ), so you can use Bayes‚Äô Rule to find the answer. It turns out the other information you‚Äôd need to do this calculation (in addition to the false negative rate) are a true negative rate and a prior, and you‚Äôre golden.\nLastly, once a candidate successfully identified Bayes‚Äô Rule and (hopefully) discussed priors, we‚Äôd ask them how they‚Äôd elicit this prior. There‚Äôs no ‚Äúright‚Äù answer here, but there are a couple options that are better than others:\n\nAsk an expert\nUse something relatively uninformative\nTake an ‚Äúempirical‚Äù approach and use something like the overall positive test rate\n\nAny of these answers would be totally reasonable, given that there‚Äôs a whole literature on prior elicitation.\nAnd that‚Äôs one example of a multi-layer question we might ask in a data science interview. The vast majority of candidates won‚Äôt get through all the parts, and that‚Äôs totally fine! Errr, maybe ‚Äúfine‚Äù isn‚Äôt actually the right word: That‚Äôs the goal. The point is that we‚Äôre constructing a question that lets us learn a lot about you: We learn how much you know about Type I / II error and how you reason about them. We learn about if you understand conditional probability and Bayes‚Äô Rule. And we learn how you reason about prior elicitation. We also learn how you argue for decisions you make, and how you communicate complicated statistical concepts (Bayes‚Äô Rule and Type I / II error aren‚Äôt simple). And finally, we learn something about the depth of your knowledge. The first part of this question you‚Äôd probably know the answer to if you‚Äôd taken an introductory statistics or ML class. The second part you‚Äôd likely see in a probability course or an inferential statistics course. The third part would also probably come up in one of those two courses, and if not, then certainly in a Bayesian statistics course. And finally, the fourth part you‚Äôd likely only see in a very advanced inferential statistics course or a Bayesian course. So not only do we see how you reason about statistics, how much you know, and how you communicate difficult concepts, we also learn something about the types of problems you‚Äôve seen or worked on in the past, whether they be in classes or in a former job or internship. This is a lot of useful information."
  },
  {
    "objectID": "posts/2021-12-22-notes-on-hiring-data-analysts-scientists/index.html#the-take-home",
    "href": "posts/2021-12-22-notes-on-hiring-data-analysts-scientists/index.html#the-take-home",
    "title": "Notes on Hiring Data Analysts + Scientists",
    "section": "The Take Home",
    "text": "The Take Home\nThe other piece to our interview process is a take home project. For data science, we have a take-home that you can find in our CollegeVine DS Hiring Github repo. For data analysts, we ask them to bring in any project they‚Äôve done in the past: Anything from a homework assignment to another company‚Äôs take home.\nWe spend about 25-30 minutes going through the take home project, and we‚Äôre looking for a few main things. For the most part, candidates are good at describing what they‚Äôve done, so we‚Äôre generally trying to dig to figure out why they made the decisions they did. Can they defend them? How did they think through the pros and cons of each decision? What‚Äôs their thought process like? The idea here is that everyone in a data-related role will need to make decisions where there‚Äôs no clear ‚Äúright‚Äù answer, so we want to see why you chose to make a pie chart instead of a box plot, or chose to use XGBoost instead of logistic regression. Can you talk me through the pros and cons of each option?\nIn the data science take home we give, there are also some tricks we‚Äôre trying to test the candidates out on. In an effort to not give away our whole bag of tricks, I invite everyone to give the exercise a shot! I‚Äôd even be happy to take a look through your projects to see what you come up with. The short of it is that there are some issues in the data that, if you don‚Äôt notice them and do something to fix them, will end up ruining the predictions your model produces. We don‚Äôt expect anyone to catch all of these issues in the short amount of time we ask them to spend on the problem, but we hope that they‚Äôll be thoughtful about how they‚Äôd solve them once we point them out in the interview.\nFinally, there‚Äôs one elephant in the room here that‚Äôs important to address: Many people feel negatively about take homes. So do we ‚Äì they tend to be long and unnecessarily complicated, and often feel like a waste of the candidate‚Äôs time. In our view, though, the take home is necessary for one main reason: It lets you familiarize yourself with a data science problem beforehand, so that when you get to the interview we can hit the ground running. In a sense, we‚Äôre giving you the rules of the game in advance so we can start playing right away. This lets us avoid any confusion or annoying ambiguity with regards to the types of problems were asking about, and to entirely avoid requiring candidates to reason through questions about entirely hypothetical problems that they‚Äôve never seen before. For these reasons, and also because there‚Äôs historically been lots of signal that we‚Äôve gotten from how candidates respond to our questions about the project they bring in, we‚Äôve decided to stick with the take home. In addition, we don‚Äôt screen out any candidates on the basis of their take home. We don‚Äôt ask them to be submitted in advance, so every candidate who does a take home gets an interview."
  },
  {
    "objectID": "posts/2021-12-22-notes-on-hiring-data-analysts-scientists/index.html#wrapping-up",
    "href": "posts/2021-12-22-notes-on-hiring-data-analysts-scientists/index.html#wrapping-up",
    "title": "Notes on Hiring Data Analysts + Scientists",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nIn a nutshell, that‚Äôs our whole data scientific technical interview! We discuss a take home you bring in, and then we talk through some miscellaneous statistics and machine learning questions like the ones we discussed above. In general, we‚Äôre not looking for any specific expertise or knowledge ‚Äì we‚Äôre a start up, after all. Instead, we‚Äôre testing candidates to see how they reason about problems that are similar to the ones they‚Äôll work on at CollegeVine, and how they explain the ins and outs of different possible solutions to those problems. We‚Äôre looking at their intuitions about statistics and machine learning, their ability to think on their feet when faced with questions they don‚Äôt immediately know the answer to, and their curiosity and creativity when it comes to solving challenging questions. At the end of the day, it‚Äôs these traits, not any hyper-specific technical knowledge, that‚Äôll make for a great CV data team member."
  },
  {
    "objectID": "posts/2021-06-08-working-with-your-fitbit-data-in-r/index.html#introduction",
    "href": "posts/2021-06-08-working-with-your-fitbit-data-in-r/index.html#introduction",
    "title": "Working With Your Fitbit Data in R",
    "section": "Introduction",
    "text": "Introduction\nfitbitr 0.1.0 is now available on CRAN! You can install it with\ninstall.packages(\"fitbitr\")\nor you can get the latest dev version with\n## install.packages(\"devtools\")\ndevtools::install_github(\"mrkaye97/fitbitr\")\nfitbitr makes it easy to pull your Fitbit data into R and use it for whatever interests you: personal projects, visualization, medical purposes, etc.\nThis post shows how you might use fitbitr to pull and visualize some of your data."
  },
  {
    "objectID": "posts/2021-06-08-working-with-your-fitbit-data-in-r/index.html#sleep",
    "href": "posts/2021-06-08-working-with-your-fitbit-data-in-r/index.html#sleep",
    "title": "Working With Your Fitbit Data in R",
    "section": "Sleep",
    "text": "Sleep\nFirst, you should either generate a new token with generate_token() or load a cached token with load_cached_token().\n\nlibrary(fitbitr)\nlibrary(lubridate)\nlibrary(tidyverse)\n\n## Dates to use throughout post\nstart <- as_date(\"2020-01-01\")\nend <- as_date(\"2021-10-18\")\n\ngenerate_fitbitr_token()\n\nAnd then you can start pulling your data!\n\nsleep <- get_sleep_summary(\n  start_date = end - months(3),\n  end_date = end\n)\n\nhead(sleep)\n\n\n\n \n  \n    log_id \n    date \n    start_time \n    end_time \n    duration \n    efficiency \n    minutes_to_fall_asleep \n    minutes_asleep \n    minutes_awake \n    minutes_after_wakeup \n    time_in_bed \n  \n \n\n  \n    34207402675 \n    2021-10-18 \n    2021-10-17 23:01:00 \n    2021-10-18 06:30:00 \n    26940000 \n    91 \n    0 \n    391 \n    58 \n    0 \n    449 \n  \n  \n    34193579435 \n    2021-10-17 \n    2021-10-16 23:03:30 \n    2021-10-17 08:11:00 \n    32820000 \n    95 \n    0 \n    472 \n    75 \n    4 \n    547 \n  \n  \n    34183584553 \n    2021-10-16 \n    2021-10-15 22:46:30 \n    2021-10-16 06:57:30 \n    29460000 \n    94 \n    0 \n    424 \n    67 \n    0 \n    491 \n  \n  \n    34174304493 \n    2021-10-15 \n    2021-10-14 23:50:00 \n    2021-10-15 08:20:30 \n    30600000 \n    94 \n    0 \n    438 \n    72 \n    0 \n    510 \n  \n  \n    34159751655 \n    2021-10-14 \n    2021-10-13 23:34:00 \n    2021-10-14 09:18:00 \n    35040000 \n    98 \n    0 \n    524 \n    60 \n    0 \n    584 \n  \n  \n    34146865838 \n    2021-10-13 \n    2021-10-12 23:50:00 \n    2021-10-13 08:32:30 \n    31320000 \n    94 \n    0 \n    461 \n    61 \n    1 \n    522 \n  \n\n\n\n\n\nOnce you‚Äôve loaded some data, you can visualize it!\n\nlibrary(zoo)\nlibrary(scales)\nlibrary(ggthemes)\n\nsleep <- sleep %>%\n  mutate(\n   date = as_date(date),\n   start_time = as_datetime(start_time),\n   end_time = as_datetime(end_time),\n   sh = ifelse(hour(start_time) < 8, hour(start_time) + 24, hour(start_time)), #create numeric times\n   sm = minute(start_time),\n   st = sh + sm/60,\n   eh = hour(end_time),\n   em = minute(end_time),\n   et = eh + em/60,\n   mst = rollmean(st, 7, fill = NA), #create moving averages\n   met = rollmean(et, 7, fill = NA),\n   year = year(start_time)\n)\n\nsleep %>%\n    ggplot(aes(x = date)) +\n    geom_line(aes(y = et), color = 'coral', alpha = .3, na.rm = T) +\n    geom_line(aes(y = st), color = 'dodgerblue', alpha = .3, na.rm = T) +\n    geom_line(aes(y = met), color = 'coral', na.rm = T) +\n    geom_line(aes(y = mst), color = 'dodgerblue', na.rm = T) +\n    scale_y_continuous(\n      breaks = seq(0, 30, 2),\n      labels = trans_format(\n        function(x) ifelse(x > 23, x - 24, x), \n        format = scales::comma_format(suffix = \":00\", accuracy = 1)\n      )\n    ) +\n    labs(x = \"Date\", y = 'Time') +\n    theme_fivethirtyeight() +\n    scale_x_date(date_breaks = '1 month', date_labels = '%b', expand = c(0, 0)) +\n    facet_grid(. ~ year, space = 'free', scales = 'free_x', switch = 'x') +\n    theme(panel.spacing.x = unit(0,\"line\"), strip.placement = \"outside\")\n\n\n\n\n\n\n\n\nThis bit of code makes a nicely formatted plot of the times you went to sleep and woke up over the past three months. You can also use fitbitr to expand the time window with a little help from purrr (the Fitbit API rate limits you, so you can‚Äôt request data for infinitely long windows in a single request).\n\n## Pull three months of data\nsleep <- map_dfr(\n  3:0,\n  ~ sleep_summary(\n    end - months(.x), \n    end - months(.x) + months(1)\n  )\n)\n\nAfter pulling the data, we can use the same code again to visualize it.\n\nsleep <- sleep %>%\n  mutate(\n   date = as_date(date),\n   start_time = as_datetime(start_time),\n   end_time = as_datetime(end_time),\n   sh = ifelse(hour(start_time) < 8, hour(start_time) + 24, hour(start_time)), #create numeric times\n   sm = minute(start_time),\n   st = sh + sm/60,\n   eh = hour(end_time),\n   em = minute(end_time),\n   et = eh + em/60,\n   mst = rollmean(st, 7, fill = NA), #create moving averages\n   met = rollmean(et, 7, fill = NA),\n   year = year(start_time)\n) %>%\n  distinct()\n\nsleep %>%\n    ggplot(aes(x = date)) +\n    geom_line(aes(y = et), color = 'coral', alpha = .3, na.rm = T) +\n    geom_line(aes(y = st), color = 'dodgerblue', alpha = .3, na.rm = T) +\n    geom_line(aes(y = met), color = 'coral', na.rm = T) +\n    geom_line(aes(y = mst), color = 'dodgerblue', na.rm = T) +\n    scale_y_continuous(\n      breaks = seq(0, 30, 2),\n      labels = trans_format(\n        function(x) ifelse(x > 23, x - 24, x), \n        format = scales::comma_format(suffix = \":00\", accuracy = 1)\n      )\n    ) +\n    labs(x = \"Date\", y = 'Time') +\n    theme_fivethirtyeight() +\n  scale_x_date(date_breaks = '1 month', date_labels = '%b', expand = c(0, 0)) +\n  facet_grid(. ~ year, space = 'free', scales = 'free_x', switch = 'x') +\n  theme(panel.spacing.x = unit(0,\"line\"), strip.placement = \"outside\")"
  },
  {
    "objectID": "posts/2021-06-08-working-with-your-fitbit-data-in-r/index.html#heart-rate-and-steps",
    "href": "posts/2021-06-08-working-with-your-fitbit-data-in-r/index.html#heart-rate-and-steps",
    "title": "Working With Your Fitbit Data in R",
    "section": "Heart Rate and Steps",
    "text": "Heart Rate and Steps\nYou can also pull your heart rate data with fitbitr. Maybe we‚Äôre curious about seeing how the number of minutes spent in the ‚Äúfat burn,‚Äù ‚Äúcardio,‚Äù and ‚Äúpeak‚Äù zones correlates with the number of steps taken that day. Let‚Äôs find out!\n\nhr <- map_dfr(\n  3:0,\n  ~ heart_rate_zones(\n    end - months(.x), \n    end - months(.x) + months(1)\n  )\n)\n\nsteps <- map_dfr(\n  3:0,\n  ~ steps(\n    end - months(.x), \n    end - months(.x) + months(1)\n  )\n)\n\nFirst, we can examine the heart rate data:\n\nhead(hr)\n\n\n\n \n  \n    date \n    zone \n    min_hr \n    max_hr \n    minutes_in_zone \n    calories_out \n  \n \n\n  \n    2021-07-18 \n    Out of Range \n    30 \n    113 \n    1440 \n    2530.16460 \n  \n  \n    2021-07-18 \n    Fat Burn \n    113 \n    141 \n    0 \n    0.00000 \n  \n  \n    2021-07-18 \n    Cardio \n    141 \n    176 \n    0 \n    0.00000 \n  \n  \n    2021-07-18 \n    Peak \n    176 \n    220 \n    0 \n    0.00000 \n  \n  \n    2021-07-19 \n    Out of Range \n    30 \n    113 \n    1408 \n    2689.45124 \n  \n  \n    2021-07-19 \n    Fat Burn \n    113 \n    141 \n    9 \n    86.59917 \n  \n\n\n\n\n\nand the steps data:\n\nhead(steps)\n\n\n\n \n  \n    date \n    steps \n  \n \n\n  \n    2021-07-18 \n    5620 \n  \n  \n    2021-07-19 \n    7537 \n  \n  \n    2021-07-20 \n    5513 \n  \n  \n    2021-07-21 \n    9014 \n  \n  \n    2021-07-22 \n    10883 \n  \n  \n    2021-07-23 \n    2975 \n  \n\n\n\n\n\nNow, let‚Äôs plot them against each other.\n\ndf <- hr %>%\n  filter(zone != \"Out of Range\") %>%\n  group_by(date) %>%\n  summarize(total_minutes = sum(minutes_in_zone), .groups = \"drop\") %>%\n  inner_join(steps, by = \"date\")\n  \ndf %>%\n  mutate(steps = as.numeric(steps)) %>%\n  filter(log(total_minutes) > 1) %>%\n  ggplot(\n    aes(\n      steps,\n      total_minutes\n    )\n  ) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F) +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\n\n\n\n\nOr maybe it‚Äôd be interesting to predict your zone minutes from your steps:\n\npredictions <- df %>%\n  mutate(steps = as.numeric(steps)) %>%\n  lm(total_minutes ~ steps, data = .) %>%\n  broom::tidy() %>%\n  mutate(across(where(is.numeric), round, 5))\n\nhead(predictions)\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    23.09761 \n    5.77502 \n    3.99957 \n    0.00011 \n  \n  \n    steps \n    0.00252 \n    0.00056 \n    4.52922 \n    0.00001"
  },
  {
    "objectID": "posts/2021-06-08-working-with-your-fitbit-data-in-r/index.html#wrapping-up",
    "href": "posts/2021-06-08-working-with-your-fitbit-data-in-r/index.html#wrapping-up",
    "title": "Working With Your Fitbit Data in R",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nAnd that‚Äôs it! Hopefully this helped show how fitbitr makes pulling your data easy, and gets you curious about the insights you can glean from your own data. The Fitbit API gives you access to so much interesting information about yourself, your habits, your fitness, and so much more, and fitbitr is just meant to be a door into that gold mine."
  },
  {
    "objectID": "posts/2023-03-09-on-auc-roc/index.html",
    "href": "posts/2023-03-09-on-auc-roc/index.html",
    "title": "Interpreting AUC-ROC",
    "section": "",
    "text": "AUC goes by many names: AUC, AUC-ROC, ROC-AUC, the area under the curve, and so on. It‚Äôs an extremely important metric for evaluating machine learning models and it‚Äôs an uber-popular data science interview question. It‚Äôs also, at least in my experience, the single most commonly misunderstood metric in data science.\nI‚Äôve heard several common misunderstandings or flat-out falsehoods from people in all kinds of roles discussing AUC. The biggest offenses tend to come from overcomplicating the topic. It‚Äôs easy to see the Wikipedia page for the ROC curve and be confused, intimidated, or some combination of the two. ROC builds off of other fundamental data science concepts ‚Äì the true and false positives rates of a classifier ‚Äì so it‚Äôs certainly not a good place to start learning about metrics for evaluating the performance of models.\nThe most common cause for confusion about AUC seems to come from the plot of the ROC curve, and nothing particularly special about AUC itself. Generally, I‚Äôll hear AUC explained as being the area under the ROC curve, and that it‚Äôs all about testing how well your model balances false positives and false negatives. That‚Äôs all well and good, but it doesn‚Äôt give someone new to AUC any intuition about what AUC actually means in practice. For instance, let‚Äôs imagine we‚Äôre trying to predict the chance that a student is accepted at Carleton College ‚Äì a quite common problem at CollegeVine! How does saying ‚ÄúAUC tells me about how my model is balancing false negatives and false positives‚Äù tell me anything about how well my model is doing at predicting that student‚Äôs chances?\nThe main issue I have with this factual-yet-unhelpful explanation of AUC is just that: While it may be true, it doesn‚Äôt get to the point. And even worse, it‚Äôs sometimes used as a crutch: A fallback answer when someone feels stuck when asked how to interpret AUC in real, practical terms.\nSo in this post, I‚Äôll focus on just one thing, then: Answering the question above about how to interpret AUC."
  },
  {
    "objectID": "posts/2023-03-09-on-auc-roc/index.html#what-is-auc",
    "href": "posts/2023-03-09-on-auc-roc/index.html#what-is-auc",
    "title": "Interpreting AUC-ROC",
    "section": "What is AUC?",
    "text": "What is AUC?\nAs I mentioned, it‚Äôs usually not helpful to try to explain AUC to someone by telling them that it‚Äôs just the area under the ROC curve, or that it‚Äôs a metric you can use for predicting probabilities as opposed to predicting classes, or that it‚Äôs a metric trying to balance false positives and false negatives. None of those things get to the crux of the problem.\nSo what is AUC, then? It‚Äôs pretty simple: Let‚Äôs imagine a model \\(M\\) being evaluated on data \\(X\\) where \\(X\\) contains some instances of the true class and some instances of the false class. The AUC of \\(M\\) on \\(X\\) is the probability that given a random item from \\(X\\) belonging to the true class (\\(T\\)) and another random item from \\(X\\) belonging to the false class (\\(F\\)), that the model predicts that the probability of \\(T\\) being true (belonging to the true class) is higher than the probability of \\(F\\) being true (belonging to the true class).\nLet‚Äôs go back to the example about Carleton admissions, and let‚Äôs imagine that we have a model that gives a probability of admission to Carleton given some information about a student. If I give the model one random accepted student and one random rejected student, the AUC of the model is the probability that the accepted student had a higher chance of acceptance (as estimated by the model) than the rejected student did.\nFor more on this, I‚Äôd refer everyone to this fantastic blog post by the team at Google, which does a great job at explaining further and/or better."
  },
  {
    "objectID": "posts/2023-03-09-on-auc-roc/index.html#a-simple-implementation",
    "href": "posts/2023-03-09-on-auc-roc/index.html#a-simple-implementation",
    "title": "Interpreting AUC-ROC",
    "section": "A Simple Implementation",
    "text": "A Simple Implementation\nThe easiest way to convey this idea might be to show a simple implementation of AUC. Below is some R code.\nFirst, let‚Äôs start by writing a function to do exactly what‚Äôs described above. Again, here‚Äôs the algorithm given some evaluation data:\n\nChoose a random item from the true class.\nChoose a random item from the false class.\nMake a prediction on each of the two items.\nIf the predicted probability for the actually true item is greater than the predicted probability for the actually false item, return true. Otherwise, return false. If they‚Äôre equal, flip a coin.\nRepeat 1-4 many times, and calculate the proportion of the time your model guessed correctly. This is your AUC.\n\nNow, let‚Äôs write this in R with a little help from some vectorization.\n\nlibrary(rlang)\nlibrary(dplyr)\nlibrary(tibble)\n\n## Our AUC implementation\n## In this implementation, we take a data frame containing a \"truth\" (i.e. whether\n## the example is _actually_ in either the true class or the false class)\n## and an \"estimate\" (our predicted probability).\n## This implementation is in line with how {{yardstick}} implements all of its metrics\ninterpretable_auc <- function(data, N, truth_col = \"truth\", estimate_col = \"estimate\") {\n  \n  ## First, subset the data down to just trues and just falses, separately\n  trues <- filter(data, .data[[truth_col]] == 1)\n  falses <- filter(data, .data[[truth_col]] == 0)\n  \n  ## Sample the predicted probabilities for N `true` examples, with replacement\n  random_trues <- sample(trues[[estimate_col]], size = N, replace = TRUE)\n  \n  ## Do the same for N `false` examples\n  random_falses <- sample(falses[[estimate_col]], size = N, replace = TRUE)\n\n  ## If the predicted probability for the actually true\n  ##  item is greater than that of the actually false item,\n  ##  return `true`. \n  ## If the two are equal, flip a coin.\n  ## Otherwise, return false.\n  true_wins <- ifelse(\n    random_trues == random_falses,\n    runif(N) > 0.50,\n    random_trues > random_falses\n  )\n  \n  ## Compute the percentage of the time our model was \"right\"\n  mean(true_wins)\n}\n\nNext, we can test our simple implementation against yardstick on some real data. For the sake of demonstration, I just used the built-in mtcars data. Here‚Äôs how the data looks:\n\nlibrary(knitr)\nlibrary(kableExtra)\n\n## Doing a little data wrangling\ndata <- mtcars %>%\n  transmute(\n    vs = as.factor(vs),\n    mpg,\n    cyl\n  ) %>%\n  as_tibble()\n\ndata %>%\n  slice_sample(n = 6) %>%\n  kable(\"html\", caption = 'Six rows of our training data') %>%\n  kable_styling(position = \"center\", full_width = TRUE)\n\n\n\nSix rows of our training data\n \n  \n    vs \n    mpg \n    cyl \n  \n \n\n  \n    0 \n    15.2 \n    8 \n  \n  \n    0 \n    15.2 \n    8 \n  \n  \n    1 \n    18.1 \n    6 \n  \n  \n    0 \n    14.7 \n    8 \n  \n  \n    0 \n    15.5 \n    8 \n  \n  \n    0 \n    19.7 \n    6 \n  \n\n\n\n\n\nNow, let‚Äôs fit a few logistic regression models to the data to see how our AUC implementation compares to the yardstick one.\n\nlibrary(purrr)\nlibrary(yardstick)\n\n## Simplest model -- Just an intercept. AUC should be 50%\nmodel1 <- glm(vs ~ 1, data = data, family = binomial)\n\n## Adding another predictor\nmodel2 <- glm(vs ~ mpg, data = data, family = binomial)\n\n## And another\nmodel3 <- glm(vs ~ mpg + cyl, data = data, family = binomial)\n\n## Make predictions for all three models\npreds <- tibble(\n  truth = data$vs,\n  m1 = predict(model1, type = \"response\"),\n  m2 = predict(model2, type = \"response\"),\n  m3 = predict(model3, type = \"response\")\n)\n\n## For each model, compute AUC with both methods: Yardstick (library) and \"homemade\"\nmap_dfr(\n  c(\"m1\", \"m2\", \"m3\"),\n  ~ {\n    yardstick <- roc_auc(preds, truth = truth, estimate = !!.x, event_level = \"second\")$.estimate\n    homemade <- interpretable_auc(preds, N = 100000, truth_col = \"truth\", estimate_col = .x)\n    tibble(\n      model = .x,\n      yardstick = round(yardstick, digits = 2),\n      homemade = round(homemade, digits = 2)\n    )\n  }\n) %>%\n  kable(\"html\", caption = 'Yardstick vs. Our Implementation') %>%\n  kable_styling(position = \"center\", full_width = TRUE)\n\n\n\nYardstick vs. Our Implementation\n \n  \n    model \n    yardstick \n    homemade \n  \n \n\n  \n    m1 \n    0.50 \n    0.50 \n  \n  \n    m2 \n    0.91 \n    0.91 \n  \n  \n    m3 \n    0.95 \n    0.95 \n  \n\n\n\n\n\nAs we‚Äôve seen here, AUC actually shouldn‚Äôt be all that much of a cause for confusion! The way I like to frame it is this: The AUC of your model is how good your model is at making even-odds bets. If I give your model two options and ask it to pick which one it thinks is more likely, a ‚Äúbetter‚Äù model (by AUC standards) will be better at identifying the true class more often.\nIn real terms, that‚Äôs a meaningful, good thing. If we‚Äôre trying to predict the probability of a cancer patient having cancer, it‚Äôs important that our model can distinguish between people with cancer and people without it when given one person from each class. If it couldn‚Äôt - meaning the model was either randomly guessing or doing worse than random - the AUC would be 50% (or below 50%, in the worse-than-random disaster scenario)."
  },
  {
    "objectID": "posts/2023-03-09-on-auc-roc/index.html#additional-thoughts",
    "href": "posts/2023-03-09-on-auc-roc/index.html#additional-thoughts",
    "title": "Interpreting AUC-ROC",
    "section": "Additional Thoughts",
    "text": "Additional Thoughts\nI also often hear the misconception that AUC is sensitive to things like class imbalance. This means that if the true class makes up a disproportionately large (or small) proportion of the evaluation data, that can skew the AUC. But based on the intuition we just built before, that‚Äôs of course not true. The key thing to remember is that the model is given one true and one false example. In choosing those, it doesn‚Äôt matter if the true class only makes up 0.005% of all of the examples in the evaluation data: AUC is only evaluating the model on its ability to determine which of the two is the true class.\nHowever, there is one thing related to class imbalance, and just sample size in general, that would affect AUC, which is the raw number of examples of each class in the evaluation data. If, for instance, you had only a single instance of the true class in the evaluation set, then the AUC of the model is entirely determined by how good the predictions of the model are on that single example. For instance, if we have a single true class and the model predicts a 100% probability of it being true, then, assuming the predictions for all of the other examples in the evaluation set are not 100%, the AUC of the model as evaluated on that data is 100%. This isn‚Äôt necessarily because the model is ‚Äúgood‚Äù in any sense, but just because the model is over-indexing to a single good prediction in the evaluation set. In practice though, this AUC estimate wouldn‚Äôt generalize. As we got more data, the predictions for all the true classes would certainly not all be 100%, so the AUC of the model would go down over time.\nFortunately, there‚Äôs an easy fix for this problem. AUCs are a point estimate, but we could also estimate a margin of error or a confidence interval for our AUC. For a situation where we only have a single instance of the true class in the evaluation set, the margin of error for our AUC would be very wide."
  },
  {
    "objectID": "posts/2023-03-09-on-auc-roc/index.html#wrapping-up",
    "href": "posts/2023-03-09-on-auc-roc/index.html#wrapping-up",
    "title": "Interpreting AUC-ROC",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nHopefully this post helped give a better intuition for what AUC actually is! A couple of major takeaways:\n\nAUC doesn‚Äôt need to be this super complicated thing about trading off between false positives and negatives and trying many different classification thresholds and such. In my opinion, it‚Äôs much simpler to just think about it as the likelihood of a guess that your model makes between two choices being correct.\nAUC isn‚Äôt affected by class imbalances."
  },
  {
    "objectID": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html",
    "href": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html",
    "title": "Deploying MLFlow on Heroku (with Heroku Postgres, S3, and nginx for basic auth)",
    "section": "",
    "text": "Disclaimer: I followed this guide to setting up MLFlow on Heroku initially. However, there were certain aspects of it that are either outdated or do not work, so this post remedies those issues."
  },
  {
    "objectID": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#mlflow",
    "href": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#mlflow",
    "title": "Deploying MLFlow on Heroku (with Heroku Postgres, S3, and nginx for basic auth)",
    "section": "MLFlow",
    "text": "MLFlow\nMLFlow is an open source tool for the entire machine learning lifecycle. It lets you create and experiment with models, write notes and descriptions, track parameters, metrics, and artifacts, and deploy to production all through an easy-to-use API and an intuitive UI. You can make calls to a running MLFlow service through the R API, the Python API, the Java API, or via the command line (cURL) or your favorite language by interacting with the REST API."
  },
  {
    "objectID": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#overview",
    "href": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#overview",
    "title": "Deploying MLFlow on Heroku (with Heroku Postgres, S3, and nginx for basic auth)",
    "section": "Overview",
    "text": "Overview\nMLFlow is surprisingly easy to set up and deploy to Heroku. There are a few steps to follow to get everything running: 1. Dockerize an MLFlow instance 2. Set up an artifact store 3. Set up a database 4. Secure the instance with basic auth"
  },
  {
    "objectID": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#prerequisites",
    "href": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#prerequisites",
    "title": "Deploying MLFlow on Heroku (with Heroku Postgres, S3, and nginx for basic auth)",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis guide assumes you already have AWS (specifically S3) set up. It also assumes some knowledge of shell scripting, Docker, and Heroku."
  },
  {
    "objectID": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#creating-a-heroku-app",
    "href": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#creating-a-heroku-app",
    "title": "Deploying MLFlow on Heroku (with Heroku Postgres, S3, and nginx for basic auth)",
    "section": "Creating a Heroku App",
    "text": "Creating a Heroku App\nFirst, let‚Äôs create a Heroku app from the CLI. For the purposes of this example, I‚Äôm going to call the app my-mlflow-example. You‚Äôll need to choose your own app name.\nheroku create my-mlflow-example\nNext, let‚Äôs attach a Heroku Postgres instance.\nheroku addons:create heroku-postgresql:hobby-dev --app my-mlflow-example\nCreating this hobby-dev Heroku Postgres instance will also automatically set the DATABASE_URL environment variable in your app‚Äôs configuration.\nNext, we‚Äôll need some other environment variables to be set. You can set your config like this:\nheroku config:set \\\n  S3_URI=s3://YOUR-S3-URI \\\n  AWS_SECRET_ACCESS_KEY=YOUR-SECRET \\\n  AWS_ACCESS_KEY_ID=YOUR-KEY \\\n  AWS_DEFAULT_REGION=YOUR-REGION \\\n  MLFLOW_TRACKING_USERNAME=YOUR-USERNAME \\\n  MLFLOW_TRACKING_PASSWORD=YOUR-PASSWORD \\\n  --app my-mlflow-example\nYou can repeat fewer lines of code by creating a .env file that looks like this:\nS3_URI=s3://YOUR-S3-URI AWS_SECRET_ACCESS_KEY=YOUR-SECRET ...\nand then using heroku config:set .env --app my-mlflow-example.\nGreat! At this point, your Heroku app should be all set up. Now all we need to do is create the Docker image that will run MLFlow."
  },
  {
    "objectID": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#setup",
    "href": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#setup",
    "title": "Deploying MLFlow on Heroku (with Heroku Postgres, S3, and nginx for basic auth)",
    "section": "Setup",
    "text": "Setup\nFirst thing‚Äôs first, let‚Äôs create a folder called my-mlflow-example where we‚Äôll store all the files we‚Äôll need. I‚Äôll do that with:\nmkdir my-mlflow-example && cd my-mlflow-example\nNext, let‚Äôs create a few files we‚Äôll need:\ntouch Dockerfile run.sh requirements.txt nginx.conf_template\nYou‚Äôll also want to make your run.sh script executable.\nchmod +x run.sh\nGreat, now we‚Äôve to all the files we‚Äôll need to deploy our MLFlow instance!\n\nThe Dockerfile\nThe Dockerfile will contain all of the library installs and files you need to run your MLFlow instance. The Dockerfile you‚Äôll need to write for MLFlow should look something like this:\nFROM continuumio/miniconda3\n\n## Copy files into the image\nCOPY run.sh run.sh\nCOPY requirements.txt requirements.txt\nCOPY nginx.conf_template /etc/nginx/sites-available/default/nginx.conf_template\n\n## Install Postgres\nRUN apt-get -y update && \\\n  apt-get -y upgrade && \\\n  apt-get install -y postgresql\n\n## Install nginx and dependencies\nRUN apt-get -y update && \\\n  apt-get install -y make vim \\\n  automake gcc g++ subversion \\\n  musl-dev nginx gettext apache2-utils\n\n## Install pip and dependencies\nRUN conda install -c anaconda pip && \\\n  pip install --upgrade pip && \\\n  pip install -r requirements.txt && \\\n  conda update -n base -c defaults conda && \\\n  conda env list && \\\n  pip freeze list\n\n## Run your `run.sh` script on container boot\nCMD ./run.sh\n\n\nThe requirements.txt File\nNext, copy the following lines into your requirements.txt:\nmlflow\npsycopg2-binary\nboto3\nThis file will tell pip which libraries to install in your docker image in the RUN pip install -r requirements.txt line above.\n\n\nThe nginx Template\nNext, we‚Äôll create a template for the nginx.conf file that we‚Äôll eventually use in the container for basic auth. One important issue here: This file is a template because Heroku randomly assigns a port on dyno start, which means we can‚Äôt hard code any ports for nginx (since we don‚Äôt know them ahead of time). Instead of hard-coding, we specify a couple of placeholders: $HEROKU_PORT and $MLFLOW_PORT. We‚Äôll replace these with the proper ports on container startup.\nYour nginx.conf_template should look like this:\nevents {}\nhttp {\n  server {\n        listen $HEROKU_PORT;\n\n        access_log /var/log/nginx/reverse-access.log;\n        error_log /var/log/nginx/reverse-error.log;\n\n        location / {\n            auth_basic \"Restricted Content\";\n            auth_basic_user_file /etc/nginx/.htpasswd;\n\n            proxy_pass                          http://127.0.0.1:$MLFLOW_PORT/;\n            proxy_set_header Host               $host;\n            proxy_set_header X-Real-IP          $remote_addr;\n            proxy_set_header X-Forwarded-For    $proxy_add_x_forwarded_for;\n        }\n    }\n}\n\n\nRun Script\nFinally, let‚Äôs create a script, run.sh, which will run when the Heroku dyno starts.\nexport HEROKU_PORT=$(echo \"$PORT\")\nexport MLFLOW_PORT=5000\n\nenvsubst '$HEROKU_PORT,$MLFLOW_PORT' < /etc/nginx/sites-available/default/nginx.conf_template > /etc/nginx/sites-available/default/nginx.conf\n\nhtpasswd -bc /etc/nginx/.htpasswd $MLFLOW_TRACKING_USERNAME $MLFLOW_TRACKING_PASSWORD\n\nkillall nginx\n\nmlflow ui \\\n  --port $MLFLOW_PORT \\\n  --host 127.0.0.1 \\\n  --backend-store-uri $(echo \"$DATABASE_URL\" | sed \"s/postgres/postgresql/\") \\\n  --default-artifact-root $S3_URI &\n\nnginx -g 'daemon off;' -c /etc/nginx/sites-available/default/nginx.conf\nThere are a few things happening here: 1. We set the HEROKU_PORT environment variable from the randomly assigned PORT that Heroku creates on dyno startup 2. We assign MLFLOW_PORT to 5000. The chances that Heroku assigns exactly port 5000 are low. If you want, you can add a few more lines to first check if Heroku assigns 5000 as the port, and if it does, choose any other port instead (e.g.¬†1000). 3. We substitute the port placeholder variables in our nginx.conf_template file with the actual environment variable values for the ports, so that nginx knows where to listen and direct traffic. 4. We create a new .htpasswd file from the MLFLOW_TRACKING_USERNAME and MLFLOW_TRACKING_PASSWORD environment variables that we set in the Heroku config. This file will be used by nginx to check inputted usernames and passwords against. 5. We start the MLFlow UI in the background, telling it to run on the MLFLOW_PORT variable we created, and pointing it to our Heroku Postgres instance for the backend store and our S3 bucket for artifact storage. Note: By default, Heroku Postgres provides a URL that begins with postgres. This is not compatible with SQLAlchemy, so we substitute postgresql (which is compatible) for postgres. 6. We start nginx for basic auth."
  },
  {
    "objectID": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#deploying",
    "href": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#deploying",
    "title": "Deploying MLFlow on Heroku (with Heroku Postgres, S3, and nginx for basic auth)",
    "section": "Deploying",
    "text": "Deploying\nDeployment is simple, and can be done with a few lines of bash:\nheroku login\nheroku container:login\nheroku container:push web --app my-mlflow-example\nheroku container:release web --app my-mlflow-example\nAnd that‚Äôs it! Running those four lines should build your Docker image, push it to Heroku, and release it as your app. Once it releases, Heroku will boot up a dyno and you should be able to go to my-mlflow-example.herokuapp.com and see the MLFlow UI."
  },
  {
    "objectID": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#using-mlflow",
    "href": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#using-mlflow",
    "title": "Deploying MLFlow on Heroku (with Heroku Postgres, S3, and nginx for basic auth)",
    "section": "Using MLFlow",
    "text": "Using MLFlow\nNow that your instance is deployed, you should have no problem using MLFlow for your whole ML lifecycle. For example, in R you might want to create a new experiment. You could do something like this:\n## install.packages(\"mlflow\")\n\nlibrary(mlflow)\n\n## Set up some environment variables\n## This way, your R session will know where to\n##  look for your MLFlow instance and will have\n##  the proper credentials set up\nSys.setenv(\n  MLFLOW_TRACKING_URI = \"https://my-mlflow-example.herokuapp.com\"\n  MLFLOW_TRACKING_USERNAME = \"YOUR-USERNAME\",\n  MLFLOW_TRACKING_PASSWORD = \"YOUR-PASSWORD\"\n)\n\nmlflow_create_experiment(\"my-first-experiment\")\nAnd with that, you should be able to harness all of the awesome power of MLFlow for all of your ML lifecycle needs!"
  },
  {
    "objectID": "posts/2021-01-10-what-i-ve-been-drinking-in-quarantine/index.html",
    "href": "posts/2021-01-10-what-i-ve-been-drinking-in-quarantine/index.html",
    "title": "What I‚Äôve Been Drinking in Quarantine (Plus a Cocktail Lesson)",
    "section": "",
    "text": "The past ten-or-so months of limited activity due to Covid-19 have been a slog for everyone, to say the least. I‚Äôve been fortunate to have avoided the worst of it, having spent much of the past ten months living in rural northwestern Connecticut.\nMany of us have had lots of time to explore hobbies over this time, and something that has gained lots of popularity ‚Äì editor‚Äôs note: unsurprisingly ‚Äì is drinking! In particular, there seems to have been a surge in home bartending and cocktail lessons happening during Covid, since hobbyist bartending ‚Äì which has been, and continues to be, a hobby of mine ‚Äì is a great Covid activity! By bartending, you get to learn about cocktails and build your skills to impress your friends when social gatherings start happening again, and you get to drink during it! What could be better than that?"
  },
  {
    "objectID": "posts/2021-01-10-what-i-ve-been-drinking-in-quarantine/index.html#beers-ive-been-loving",
    "href": "posts/2021-01-10-what-i-ve-been-drinking-in-quarantine/index.html#beers-ive-been-loving",
    "title": "What I‚Äôve Been Drinking in Quarantine (Plus a Cocktail Lesson)",
    "section": "Beers I‚Äôve Been Loving",
    "text": "Beers I‚Äôve Been Loving\n\nGreen, Very Green, Juice Machine, and Haze by Tree House Brewing Company in Charlton, MA. Bascially, you can‚Äôt go wrong with Tree House. It‚Äôs one of the best breweries in the country, is all over the top ratings on Untappd, and makes almost univerally awesome beer. The ones I‚Äôve listed are all very hazy, juicy, fruity (although not as much as some others, like Saturated and Iridescent) New England Style IPAs. That means they‚Äôre a little less bitter, a little less hoppy, and a little more like drinking orange juice than some of the other IPAs you‚Äôve probably had elsewhere (Goose Island, Dogfish Head, etc.).\nFocal Banger and Heady Topper from The Alchemist in Stowe, VT. Similar to Tree House, The Alchemist is a wildly popular New England brewery with some awesome beers. I‚Äôve liked all of the ones I‚Äôve tried, but Heady Topper and Focal Banger are especially awesome. I actually prefer the two of them to most of the Tree House beers (bar Very Green, probably). They‚Äôre both a little less fruity than the Tree Houses, which I prefer.\nYou Drive Us Wild, by Grimm in New York City. This is another beer with a similar profile to the Tree House / Alchemist groups (are you sensing a pattern?). Grimm is also an awesome brewery, and I‚Äôve loved almost everything of theirs that I‚Äôve had. Magnetic Compass and Tesseract are other highlights."
  },
  {
    "objectID": "posts/2021-01-10-what-i-ve-been-drinking-in-quarantine/index.html#liquors-and-amari-ive-been-drinking",
    "href": "posts/2021-01-10-what-i-ve-been-drinking-in-quarantine/index.html#liquors-and-amari-ive-been-drinking",
    "title": "What I‚Äôve Been Drinking in Quarantine (Plus a Cocktail Lesson)",
    "section": "Liquors and Amari I‚Äôve Been Drinking",
    "text": "Liquors and Amari I‚Äôve Been Drinking\nThese are my go-to glasses to sip on. Generally, liquor before or during dinner, and amaro after dinner. ‚ÄúAmaro‚Äù is the Italian word for bitter, so reader beware: the amari (especially Fernet) are very bitter.\n\nLagavulin 8 Year\nDel Maguey Chichicapa\nTequila Ocho Plata\nCynar\nCampari\nFernet Branca"
  },
  {
    "objectID": "posts/2021-01-10-what-i-ve-been-drinking-in-quarantine/index.html#cocktails",
    "href": "posts/2021-01-10-what-i-ve-been-drinking-in-quarantine/index.html#cocktails",
    "title": "What I‚Äôve Been Drinking in Quarantine (Plus a Cocktail Lesson)",
    "section": "Cocktails",
    "text": "Cocktails\nSo, now for the main event. As promised, some great cocktails to try! A preface: I like a weird profile of cocktail. I‚Äôm not huge on sweet drinks, so the overly sweet margaritas from your neighborhood cantina aren‚Äôt what you‚Äôll find here. I like sour, bitter, smoky, spicy, and herbal with just a touch of sweet. I‚Äôll start with a few simple drinks (just a few easy to find ingredients), and then go through a couple of favorites that are a little more niche. At the end, I‚Äôll give some general pointers on general cocktail making things and ingredients.\n\nNegroni\nThe Negroni is my all time favorite drink. It‚Äôs bitter, sweet, complex, and easy to make! Traditionally, it‚Äôs equal parts gin, Campari, and sweet vermouth:\n\n1oz gin\n1oz Campari\n1oz sweet vermouth\nOrange or lemon twist garnish\n\nAdd as much ice as you can fit into a beaker or other glass that you can stir in, and add the ingredients. Stir about thirty seconds until the glass is well chilled. Strain into the glass of your choosing. Garnish.\nFor a Negroni, you probably want a neutral, London dry style gin. I‚Äôve found that Beefeater works great, and isn‚Äôt particularly expensive. The vermouth is the most important piece here, as it gives all kinds of interesting flavors to the drink depending on the brand you use. You‚Äôll want to spend the extra few dollars to get something great, like Carpano Antica.\nPersonally, as a mezcal lover, I often find myself swapping out the gin in my Negronis for Del Maguey Vida, which is a high-quality mezcal that‚Äôs great for making cocktails with. I‚Äôve also found that using apple brandy is delicious as well, and using bourbon gets you close to a Boulevardier. Feel free to experiment!\n\n\nManhattan\nThe Manhattan is another classic. I use rye in mine, but you can get away with bourbon too (although you might get weird from bartenders are certain bars for doing so).\n\n2oz rye whiskey\n1oz sweet vermouth\n4-6 dashes of bitters\nMaraschino cherry garnish\n\nAdd as much ice as you can fit into a beaker or other glass that you can stir in, and add the ingredients (preferably bitters-first so they don‚Äôt just sit on top of the ice). Stir about thirty seconds until the glass is well chilled. Strain into the glass of your choosing. Garnish.\nSince the Manhattan is so simple, you really need to use high quality ingredients. I‚Äôve found Rittenhouse Rye to be a fantastic rye, especially given the price. As before, you probably want Carpano Antica for the vermouth.\n\n\nMargarita\nYet another classic, everyone‚Äôs familiar with a margarita. It‚Äôs simple and delicious.\n\n2oz tequila\n1oz lime juice\n.75oz triple sec, Cointreau, or 1:1 simple syrup\n\nAdd ingredients to a shaker with as much ice as you can fit and shake vigorously for 30 seconds until the shaker is well chilled. Double strain into your glass of choice. Optionally, you can salt the rim of your glass.\nYou have some options for this one. Again, as a mezcal lover, I often swap the tequila for mezcal (or go 50/50). I also prefer less sweet drinks, so if .75oz of sugary stuff isn‚Äôt enough for you, just add some more!\n\n\nMoscow Mule\nThe copper cup drink, and a super easy, delicious vodka cocktail.\n\n2oz vodka\n3-5oz ginger beer, depending on how strong you like it\nsqueeze of lime juice\n\nFor this one, I normally just add the ingredients to a glass, stir, and sip away! Using more ginger beer will mellow out the drink a lot.\nNow, with some delicious, easy classics that won‚Äôt let you down out of the way, on to some more involved drinks! Some of these require harder-to-find ingredients (or just more ingredients), but I think they‚Äôre all delicious and worth a shot.\n\n\nLast Word\nThe Last Word is an interesting one. It‚Äôs sweet, sour, herbal, and definitely not everybody‚Äôs cup of tea. It‚Äôs another easy equal-parter:\n\n.75oz lime juice\n.75oz green Chartreuse\n.75oz Luxardo Maraschino liqueur\n.75oz gin\nMaraschino cherry garnish\n\nAdd the ingredients to a cocktail shaker with as much ice as you can fit, and shake vigorously for about 30 seconds until the shaker is well chilled. Double strain into your glass of choice.\nThis is a weird mix of ingredients, but it comes together to make a complex, delicious drink! For the adventurous, it‚Äôs definitely worth a shot.\n\n\nJungle Bird\nThe Jungle Bird is a bitter drink fan‚Äôs tiki drink. It‚Äôs a delicious mix of tiki and bitter, and is a real crowd-pleaser.\n\n1.5oz dark rum\n1.5oz pineapple juice\n.75oz Campari\n.5oz lime juice\n.5oz 1:1 simple syrup\n\nAdd the ingredients to a cocktail shaker with as much ice as you can fit, and shake vigorously for about 30 seconds until the shaker is well chilled. Double strain into your glass of choice.\nThe Jungle Bird is another favorite of mine. It‚Äôs not too bitter, not too sweet, and you also get some lime and pineapple in there, which I love both of. It‚Äôs a tough one to go wrong with, since it‚Äôs not as strong as something like a Manhattan, less bitter than a Negroni, and less sweet than a Margarita can be.\n\n\nCorpse Reviver #2\nAnother personal favorite of mine! The middle child of a trio of Corpse Revivers, I find this one to be totally delicious. Probably named for being the drink that‚Äôll get you on your feet the next morning (need confirmation on that).\n\n1.5oz gin\n1.5oz Lillet Blanc\n1.5oz lemon juice\n.75oz triple sec or Cointreau\n<.25 (small dash) absinthe, Pernod, or green Chartreuse\n\nAdd the ingredients to a cocktail shaker with as much ice as you can fit, and shake vigorously for about 30 seconds until the shaker is well chilled. Double strain into your glass of choice.\nThis is another personal favorite. It‚Äôs sour, a little sweet, and very complex. The Lillet adds a lot of character and plays really well with the lemon and the herbal notes from the absinthe.\n\n\nEnzoni\nThe Enzoni is a remix of the Negroni that‚Äôs a little sweeter, and uses fresh grapes! It‚Äôs equally delicious, and similar in profile to a Jungle Bird (a little sweet and less bitter than the Negroni).\n\n1oz gin\n1oz Campari\n.75oz lemon juice\n.5oz 1:1 simple syrup\n5 white grapes\n\nMuddle the grapes with the simple syrup in the bottom of a cocktail shaker. Then, add as much ice as you can with the rest of the ingredients, and shake vigorously for about 30 seconds until the shaker is well chilled. Double strain into your glass of choice.\nThe Enzoni is an awesome drink. The acidity from the lemon and the sweetness and flavor from the grapes make this a unique and delicious drink.\n\n\nBitter Giusseppe\nFinally, a bitters-forward drink! The Bitter Giusseppe is simple, and checks a lot of boxes for me. It‚Äôs sour and bitter, and it‚Äôs also a low alcohol content drink!\n\n2oz Cynar\n1oz sweet vermouth\n.25oz lemon juice\n4 dashes of bitters\n\nAdd as much ice as you can fit into a cocktail shaker with the ingredients and shake vigorously for about 30 seconds until the shaker is well chilled. Double strain into your glass of choice.\nAs usual, you probably want to be using Carpano Antica as your vermouth here. It adds a lot of interesting flavors to the drink. This one is awesome for an easy sipper, and the acidity from the lemon really takes it over the top"
  },
  {
    "objectID": "posts/2021-01-10-what-i-ve-been-drinking-in-quarantine/index.html#general-pointers",
    "href": "posts/2021-01-10-what-i-ve-been-drinking-in-quarantine/index.html#general-pointers",
    "title": "What I‚Äôve Been Drinking in Quarantine (Plus a Cocktail Lesson)",
    "section": "General Pointers",
    "text": "General Pointers\n\nCocktail Making\n\nThe reason we shake or stir our drinks is not to mix the ingredients together. Well, it is, but that‚Äôs not the primary reason. We‚Äôre shaking or stirring to dilute the drink. If you want an example of the importance of dilution, try making a Negroni in a glass without any ice, and just stir it together and taste it. Unless you like an extremely alcohol-forward, bitter drink (which you might, I do), you‚Äôll probably like it far better after stirring with ice. The dilution and chilling of the drink does a lot to enhance the flavor, make the drink more enjoyable, and take the edge off. Don‚Äôt skip it!\nYour ingredients matter. High quality ingredients will make better tasting drinks. You really don‚Äôt want to be trying to mask bad rye in a Manhattan, for example. You want to be showcasing a great rye!\nSpeaking of ingredients: squeeze your own juice, or at least your own lemon and lime juice. The stuff from the bottle with all of the preservatives is just not the same. If you don‚Äôt believe me, do a blind taste test of a margarita made with fresh lime juice vs.¬†one with bottled juice, and see what you think. It‚Äôs worth it.\nYou can (and should) make your own simple syrup! It‚Äôs easy and takes two minutes, so there‚Äôs no reason to spend $7 on a small bottle from the store. All of my recipes call for 1:1 syrup, which means 1 part sugar, 1 part water. If you‚Äôre making it, that just means put a cup of sugar and a cup of water in a pot and heat it up gently until the sugar is dissolved. It will keep in the fridge for about a month. If you make 2:1 syrup, cut the amounts in the recipes in half. 2:1 syrup will keep in the fridge for far longer than 1:1 syrup.\n\n\n\nIngredients\n\nBuy lemons and limes. It makes all the difference in your drinks.\nThere are a lot of different types of liquor discussed here. These are the brands I like, but feel free to experiment with others! The key is to make drinks you like:\n\nGin I use Beefeater for everything. It‚Äôs relatively inexpensive, available everywhere, and a great, neutral-flavored London dry gin. For a more herbal gin, go for Hendricks.\nTequila This is a blog post in itself. At the very least, get a 100% agave tequila. Espol√≥n and Olmeca Altos are good choices. If you want sipping tequila, you want to be buying something from a NOM (producer) who doesn‚Äôt use diffusers or autoclaves. This means NOT Clase Azul, Casamigos, or Patron. Good tequilas are fermented in barrels after the agave is roasted in ovens made of brick or stone, and NOMs using diffusers and autoclaves are cutting corners by using chemicals and high-pressure chambers to decrease their costs and speed up the process. The product suffers as a result, and they often mask bad product by adding sugar to their tequilas, which is why many people will tell you that Clase Azul is ‚Äúsmooth.‚Äù It is. That‚Äôs added sugar making it taste like that. There are a number of great (harder to find) tequilas that are both great for drinking and an opportunity to support distillers doing things the right way. A few that I love are Fortaleza, Tapatio, Tequila Ocho, and Siete Leguas.\nMezcal Everything by Del Maguey is great. Chichicapa is an incredible sipping mezcal (but more expensive), and I use Vida for all of my mezcal cocktails.\nVodka Expensive vodka is not necessarily good vodka. Personally, I strongly dislike Tito‚Äôs, Absolut, and Grey Goose. I‚Äôve found that Tower (from Texas), Smirnoff, and Russian Standard all work fine, and are all far less expensive. A good vodka should taste and smell like nothing, and that‚Äôs basically what all three of those vodkas will give you. If you want to impress your friends, fill a Grey Goose bottle with Smirnoff.\nRye As I said before, I‚Äôve had great success with Rittenhouse. It‚Äôs a great rye, and it‚Äôs not very expensive. Knob Creek is also great if you want to spend the extra money.\nBourbon If you want to use bourbon in a Manhattan (or just to sip), two that I like a lot are Buffalo Trace and Maker‚Äôs Mark.\nRum For white rum, I normally just use Bacardi Superior. Plantation is another good one, albeit slightly more expensive. Dark rum preferences will depend on your taste, as some are much sweeter than others. Again, Plantation makes good rum. Or, if you‚Äôre ever traveling abroad post-Covid and want to bring back Havana Club Especial duty free, I‚Äôd recommend that. We can‚Äôt buy it in the U.S. because of the trade embargo with Cuba, which is a shame for many reasons, including Havana Club being great rum.\n\nOther ingredients\n\nVermouth Dolin is fine, Carpano is great. Spend the extra money, it‚Äôs worth it.\nGinger Beer, Tonic, etc. My personal favorite brand for mixers is Fever Tree. Their stuff is a little more expensive, but it‚Äôs worth it. It‚Äôs all great, and will make your drinks even better.\n\nPro-tip: Lots of better bars and restaurants will have a secret ‚Äúbartender‚Äôs choice‚Äù option that isn‚Äôt listed on the menu. This is a great way to broaden your cocktail horizons. Historically, I‚Äôve just asked for things like ‚ÄúA mezcal drink that‚Äôs a little smoky, sour, or bitter, but not too sweet‚Äù and have discovered some great drinks! The key is letting a knowledgeable bartender know in broad strokes what you like, and letting them be creative!\nExperiment, and make drinks you like! At the end of the day, you‚Äôre drinking for you, so why not enjoy your drink while you‚Äôre at it?"
  },
  {
    "objectID": "posts/2021-02-07-what-s-new-in-slackr-2-1-0/index.html",
    "href": "posts/2021-02-07-what-s-new-in-slackr-2-1-0/index.html",
    "title": "What‚Äôs New in slackr 2.1.0",
    "section": "",
    "text": "slackr 2.1.0+ is live! There are a whole bunch of exciting changes that we (mostly Andrie de Vries and I) have made to improve the package a bunch."
  },
  {
    "objectID": "posts/2021-02-07-what-s-new-in-slackr-2-1-0/index.html#changes",
    "href": "posts/2021-02-07-what-s-new-in-slackr-2-1-0/index.html#changes",
    "title": "What‚Äôs New in slackr 2.1.0",
    "section": "Changes",
    "text": "Changes\nHere are some of the things that are new in slackr 2.1.0+. For more info on the package, check out the Github repo and the pkgdown site.\n\nEase of Use Improvements\n\nWe‚Äôve dramatically improved error messaging, so long gone are the days of errors like No 'id' column found in 'x'! Now, error messages should be far more helpful, with some hints about what might be going wrong.\nWe‚Äôve updated the package documentation significantly, so now there‚Äôs a far more informative README, some vignettes, and a pkgdown site.\nWe‚Äôve more clearly described the different use cases for slackr, in order to better help users set up slackr in a way that makes sense for them.\n\n\n\nNew Features\n\nWe‚Äôve fixed a bunch of bugs that were preventing things like icon_emoji and username from working, so those are fixed now!\nWe‚Äôve brought back some old functions that were removed in slackr 2.0.0: slackr_history() and slackr_delete(). See the docs for descriptions of what these functions can do.\n\n\n\nBack-End Changes\nWe‚Äôve made a ton of changes for how slackr interacts with the Slack API:\n\nWe now allow paging, which is especially helpful when you have a workspace of more than 1000 channels.\nWe cache requests to get lists of channels and users so that we don‚Äôt need to repeat common API calls. This speeds up calls to slackr_***() and limits how often you need to actually hit the API.\nWe‚Äôve gotten rid of a really nasty implementation of channel caching (writing a local cache to the disk) in favor of the method described above.\nWe‚Äôve factored out API calls into a separate function, which makes the package easier to understand and test.\nSpeaking of testing, we‚Äôve implemented a whole bunch of unit tests, and will be working on more.\n\n\n\nDeprecations\n\nWe‚Äôve deprecated a bunch of camel case functions in favor of their snake case counterparts for simplicity. Don‚Äôt worry! These are soft-deprecated for now. They won‚Äôt go away fully until a future version of slackr\nWe‚Äôve deprecated text_slackr in favor of slackr_msg, since they do basically the same thing."
  },
  {
    "objectID": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html",
    "href": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html",
    "title": "Exploring the Tail Behavior of ESPN‚Äôs Win Probability Model",
    "section": "",
    "text": "It‚Äôs College Football Playoff season, which means I‚Äôve been watching a lot of games lately. And I find myself complaining pretty often about how badly calibrated I think ESPN‚Äôs win probability model is. In particular, I‚Äôve noted a bunch of examples ‚Äì or at least enough for it to feel like a bunch ‚Äì of games where ESPN‚Äôs model gives a team a win probability that feels way too extreme in a situation where they‚Äôre clearly winning. I‚Äôm not talking about giving a team an 80% chance when they should have a 60% chance. The cases I‚Äôve been curious about are something more like teams getting a 99.7% chance of winning when, at least in my opinion, they should be getting something more like a 98% chance."
  },
  {
    "objectID": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#introduction",
    "href": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#introduction",
    "title": "Exploring the Tail Behavior of ESPN‚Äôs Win Probability Model",
    "section": "Introduction",
    "text": "Introduction\n\nFor classification problems like predicting win probabilities, model calibration is, at the highest level, the answer to the question ‚ÄúWhen my model says that Michigan has a 62% chance to win, do they actually end up winning about 62% of the time?‚Äù A well-calibrated model will have relatively low error over the long run. As we get more and more data, we‚Äôd expect that the amount of error in our calibration numbers would go down, and hopefully the predicted probabilities start to converge to the actual win probabilities as the games play out. For more on calibration, check out this cool FiveThirtyEight post.\n\nYou might be reading this thinking that the difference (both in absolute terms and ratio terms) between 60 and 80 percent is way bigger than the difference between, say, 98 and 99.7. And you‚Äôd be right. But I‚Äôd encourage you to think about it like this: The team that‚Äôs the underdog in the latter case has either a 2% chance (the first case) or a 0.3% chance (the second case). If you‚Äôre applying the same ratio of win probabilities back of the napkin math, that increase feels a lot bigger.\n\nFor the statistically inclined folks in the back, the ‚Äúright‚Äù way to do this is just to use odds ratios, which would show that going from 98% to 99.7% is a massive magnitude odds (or log-odds) increase.\n\nSo in a nutshell, what I‚Äôve been curious about is the tail behavior of the ESPN model ‚Äì I‚Äôm trying to answer the question of how ESPN‚Äôs model does at predicting events we know are unlikely. How often, for instance, does a team that ESPN gives a 0.5% mid-game chance of winning actually end up winning? My suspicion, based on my anecdotal evidence from watching games and complaining over the years, has been that the model is badly calibrated in the tails. I‚Äôve been on the record arguing that ESPN‚Äôs model gives win probabilities greater than 99% way too often, and can usually be heard saying things like ‚ÄúWell, they‚Äôre almost definitely going to win. But 99%? I‚Äôm not sure‚Ä¶‚Äù\nSo then, to the question. I looked into a couple of things: 1. How well-calibrated is their model in general? 2. When ESPN gives a team a very high chance of winning (>98%), how often do they actually win? 3. Does the model perform better or worse for ranked teams?"
  },
  {
    "objectID": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#calibration",
    "href": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#calibration",
    "title": "Exploring the Tail Behavior of ESPN‚Äôs Win Probability Model",
    "section": "Calibration",
    "text": "Calibration\nFirst, how well-calibrated is the model in general? I usually like to look at calibration plots to evaluate models, similar to the ones in the FiveThirtyEight post above.\nThis first plot is the overall calibration of the model at kickoff time. What we‚Äôre looking for are the points to roughly lie along the dashed line, which is the line y = x.\n\n\n\nTwo main things to notice in that plot:\n\nThe model, on aggregate, is quite well calibrated.\nThe model looks like it‚Äôs off by a bit in the lower tail, where it appears to be predicting win probabilities that are too low. That‚Äôs a sample size issue. For instance, there were 64 games where the model gave the home team a worse than 5% chance to win, and the home team ended up winning 6.25% in those games. But generating a confidence interval for that proportion gives us a range of 1.25%-12%, which is too wide to scold the model for that mistake\n\nWe can also look at the same plot, broken down by the teams playing. For instance, the following plot is broken down by whether neither team is ranked, one team is, or both teams are:\n\n\n\nIn this case, even with relatively wide error bars, we see that the model seems to perform worse for games where both teams are ranked. And it‚Äôs pretty clearly the best in games where neither team is ranked."
  },
  {
    "objectID": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#edge-cases",
    "href": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#edge-cases",
    "title": "Exploring the Tail Behavior of ESPN‚Äôs Win Probability Model",
    "section": "Edge Cases",
    "text": "Edge Cases\nNext, I was curious about how often teams given very high chances of winning ended up doing so. Anecdotally, I‚Äôve found myself complaining the most about games like the Oregon - Oregon State game from 2022 where ESPN gives Oregon a 98.3% chance of winning when they‚Äôre up 18 with 6:53 left in the third. Of course, I‚Äôm leaning into confirmation bias. But it‚Äôs hard to not think to yourself that with more than 20 minutes of football to go, Oregon State only wins that game in a wild comeback less than two in one hundred times. I‚Äôm not sure what I‚Äôd view as a more ‚Äúcorrect‚Äù estimate of their win probability, but seventeen in a thousand seems low to me. Maybe even thirty in a thousand (3%) would be better.\nOne note is that the 3% probability I‚Äôd probably lobby for doesn‚Äôt feel that different from the 1.7% that ESPN gave, but that‚Äôs an odds ratio of 1.79, which is a big difference in practice. For instance, that‚Äôs a similar odds ratio to what you‚Äôd get if you went from a 35% chance to a 50% chance, which is significant. In FiveThirtyEight world, that‚Äôs the difference between being right in the middle of the ‚Äútoss-up‚Äù category vs.¬†being solidly in the ‚Äúlean Oregon‚Äù category.\nSo anyways, back to Oregon - Oregon State. I was curious about games like that: Games where, with more than, say, five minutes to go and one team leading by at most three scores (24 points), how often ESPN was right when they gave the leading team a better than 98% chance of winning the game.\nAs it turns out, ESPN‚Äôs model is doing pretty well in the tails on the whole. See the table below:\n\n\n\n\n\n\n\n\n\n\n\nRanked\nOverall Win %\nWin % CI Lower Bound\nWin % CI Upper Bound\nN\n\n\n\n\nBoth\n0.54%\n0%\n1.63%\n184\n\n\nOne\n1%\n0.4%\n1.7%\n1002\n\n\nNeither\n1.07%\n0.7%\n1.52%\n2427\n\n\nAll\n1.02%\n0.72%\n1.36%\n3613\n\n\n\n\nRanked corresponds to how many of the teams in the game were ranked (i.e.¬†‚Äúboth‚Äù means ‚Äúboth teams were ranked‚Äù). ‚Äúall‚Äù is all of the data pooled together.\nThe main takeaway from the table above is that when ESPN gives a team a <2% chance of winning a game, that tends to not be a severe underestimate as I was expecting. Across the crosstabs I checked, even the high end of a 95% confidence interval for the proportion of the time that the underdog would go on to win was below the 2% threshold."
  },
  {
    "objectID": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#wrapping-up",
    "href": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#wrapping-up",
    "title": "Exploring the Tail Behavior of ESPN‚Äôs Win Probability Model",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nAll told, I didn‚Äôt end up confirming my suspicions. At least from a cursory look through the data, ESPN‚Äôs model seems to be performing quite well in the tails. Or, at the very least, it‚Äôs not making predictions that are as ridiculous as I had thought they were. I still have my suspicions and will surely continue finding individual cases that don‚Äôt make sense to me intuitively, but after poking around a little I at least feel less concerned about the model making egregious predictions ‚Äì as far as I can tell, it‚Äôs doing a pretty good job on average."
  },
  {
    "objectID": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#future-work",
    "href": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#future-work",
    "title": "Exploring the Tail Behavior of ESPN‚Äôs Win Probability Model",
    "section": "Future Work",
    "text": "Future Work\nA couple of other things jump out at me as being worth exploring:\n\nHow well did the model do vs.¬†my intuitions? In games where I was on the record as thinking the win probabilities given were far too high (or low), how do I perform?\nHow does ESPN‚Äôs model perform by other common ML metric standards? For instance, does its AUC outperform (e.g.) Vegas? (Almost certainly not). Or how negative is the model‚Äôs Brier Skill Score when using Vegas as a baseline?\nDoes the model perform better or worse for certain teams? Maybe some teams are being consistently overrated or underrated by the model."
  },
  {
    "objectID": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#appendix",
    "href": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#appendix",
    "title": "Exploring the Tail Behavior of ESPN‚Äôs Win Probability Model",
    "section": "Appendix",
    "text": "Appendix\nYou can find the code to reproduce this analysis on my Github."
  },
  {
    "objectID": "posts/2023-03-25-balancing-classes/balancing-classes.html",
    "href": "posts/2023-03-25-balancing-classes/balancing-classes.html",
    "title": "Balancing Classes in Classification Problems",
    "section": "",
    "text": "In my last post I wrote about common classifications metrics and, especially, calibration.\nWith calibration in mind, this post will show why balancing your classes ‚Äì which is an all-too-common practice when working on classification problems ‚Äì is generally a bad idea and leads to poorly calibrated models."
  },
  {
    "objectID": "posts/2023-03-25-balancing-classes/balancing-classes.html#some-example-data",
    "href": "posts/2023-03-25-balancing-classes/balancing-classes.html#some-example-data",
    "title": "Balancing Classes in Classification Problems",
    "section": "Some Example Data",
    "text": "Some Example Data\n\n\n\nFor the purposes of this example, I‚Äôll use the Wisconsin breast cancer data. The data is built into the mlbench package in R and scikit-learn in python. You can also get it from the UCI Machine Learning Repository.\nI‚Äôll only be using cl_thickness, which is the indicator for clump thickness.\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(janitor)\nlibrary(purrr)\nlibrary(mlbench)\n\ndata(BreastCancer)\n\ndata <- BreastCancer %>%\n  clean_names() %>%\n  transmute(\n    cl_thickness = as.numeric(cl_thickness), \n    class\n  ) %>%\n  as_tibble()\n\ndata %>%\n  slice_sample(n = 5) %>%\n  pretty_print()\n\n\n\n \n  \n    cl_thickness \n    class \n  \n \n\n  \n    1 \n    benign \n  \n  \n    3 \n    benign \n  \n  \n    5 \n    malignant \n  \n  \n    4 \n    benign \n  \n  \n    1 \n    benign \n  \n\n\n\n\n\nThe data is imbalanced: There are far more (about 2x) benign tumors than malignant ones in the sample.\n\ndata %>%\n  count(class) %>%\n  mutate(prop = n / sum(n)) %>%\n  pretty_print()\n\n\n\n \n  \n    class \n    n \n    prop \n  \n \n\n  \n    benign \n    458 \n    0.6552217 \n  \n  \n    malignant \n    241 \n    0.3447783"
  },
  {
    "objectID": "posts/2023-03-25-balancing-classes/balancing-classes.html#model-fitting",
    "href": "posts/2023-03-25-balancing-classes/balancing-classes.html#model-fitting",
    "title": "Balancing Classes in Classification Problems",
    "section": "Model Fitting",
    "text": "Model Fitting\nWith that class imbalance in mind, let‚Äôs get to model fitting. The first thing I‚Äôll do is fit a simple logistic regression model to predict the class (either malignant or benign) from the clump thickness.\nFirst, I‚Äôve written a bit of tidymodels helper code below for reuse later.\n\nlibrary(tidymodels)\n\nfit_model <- function(data, spec) {\n  spec <- set_mode(spec, \"classification\")\n  \n  rec <- recipe(\n    class ~ cl_thickness,\n    data = data\n  )\n  \n  wf <- workflow() %>%\n    add_model(spec) %>%\n    add_recipe(rec)\n  \n  fit(wf, data)\n}\n\npredict_prob <- function(model, data) {\n  predict(model, data, type = \"prob\")$.pred_malignant\n}\n\nNow, I‚Äôll fit a simple logistic regression model by specifying logistic_reg() as the model specification in fit_model().\n\nlibrary(probably)\n\nunbalanced_model <- fit_model(\n  data,\n  logistic_reg()\n)\n\npreds <- tibble(\n  truth = data$class,\n  truth_int = as.integer(data$class) - 1,\n  estimate = predict_prob(unbalanced_model, data)\n)\n\nAnd now we can make a calibration plot of our predictions. Remember, the goal is to have the points on the plot lie roughly along the line y = x. Lying below the line means that our predictions are too high, and above the line means our predictions are too low.\n\ncal_plot_breaks(preds, truth = truth_int, estimate = estimate)\n\n\n\n\nAwesome! Even with the class imbalance, our model‚Äôs probability predictions are well-calibrated. In other words, when we predict that there‚Äôs a 25% chance that a tumor is malignant, it‚Äôs actually malignant about 25% of the time."
  },
  {
    "objectID": "posts/2023-03-25-balancing-classes/balancing-classes.html#balancing-the-training-data",
    "href": "posts/2023-03-25-balancing-classes/balancing-classes.html#balancing-the-training-data",
    "title": "Balancing Classes in Classification Problems",
    "section": "Balancing the Training Data",
    "text": "Balancing the Training Data\nSo then, what happens if we balance the training data as we‚Äôre so often told to do? First, let‚Äôs balance by undersampling from the majority class.\n\nminority_class <- data %>%\n  count(class) %>%\n  filter(n == min(n))\n\nbalanced <- data %>%\n  group_split(class, .keep = TRUE) %>%\n  map_dfr(\n    ~ {\n      if (.x$class[1] == minority_class$class) {\n        .x\n      } else {\n        slice_sample(\n          .x,\n          n = minority_class$n,\n          replace = FALSE\n        )\n      }\n    }\n  )\n\nbalanced %>%\n  count(class) %>%\n  pretty_print()\n\n\n\n \n  \n    class \n    n \n  \n \n\n  \n    benign \n    241 \n  \n  \n    malignant \n    241 \n  \n\n\n\n\n\nNow we have the same number of observations for each class. Let‚Äôs go ahead and fit another logistic regression model, but this time on the balanced data.\n\nbalanced_model <- fit_model(balanced, logistic_reg())\n\npreds$balanced_preds <- predict_prob(\n  balanced_model,\n  data\n)\n\ncal_plot_breaks(preds, truth = truth_int, estimate = balanced_preds)\n\n\n\n\nAll of a sudden, our model is very poorly calibrated. We‚Äôre consistently overpredicting the probability of a tumor being malignant. Why is that? Think back to what we just did: We removed a bunch of examples of benign tumors from our training data.\nLet‚Äôs think about that from first principles for a minute. If you had no information at all, a reasonable guess for whether or not a tumor is malignant would be the overall proportion of tumors that are malignant. In the unbalanced data, that number was about 34%. But after balancing, it‚Äôs now 50%. That means that we‚Äôve just biased our ‚Äúno-information‚Äù prediction upwards by about 16 percentage points (or 50%). And so it shouldn‚Äôt be surprising that in our calibration plot above, we see that we‚Äôre consistently over-predicting. Our probabilities are too high because the baseline rate at which the true class appears in our training data has just increased significantly.\n\nAn important note which I‚Äôll circle back to later is that this intuition about a baseline guess is directly rated to the intercept term of the logistic regression model you fit."
  },
  {
    "objectID": "posts/2023-03-25-balancing-classes/balancing-classes.html#smote",
    "href": "posts/2023-03-25-balancing-classes/balancing-classes.html#smote",
    "title": "Balancing Classes in Classification Problems",
    "section": "SMOTE",
    "text": "SMOTE\n‚ÄúBut no!‚Äù you might be thinking. ‚ÄúWhy would you just undersample directly? You‚Äôre supposed to use an algorithm like SMOTE to overcome your class imbalance problem.‚Äù\nGreat! Let‚Äôs see if using SMOTE fixes our calibration issues. I‚Äôll first use SMOTE to intelligently oversample the minority class.\n\nlibrary(themis)\n\nsmote <- recipe(class ~ cl_thickness, data = data) %>%\n  step_smote(class) %>%\n  prep() %>%\n  bake(new_data = NULL)\n\nsmote %>%\n  count(class) %>%\n  pretty_print()\n\n\n\n \n  \n    class \n    n \n  \n \n\n  \n    benign \n    458 \n  \n  \n    malignant \n    458 \n  \n\n\n\n\n\nNow that we have balanced classes thanks to SMOTE, let‚Äôs fit another logistic regresion model and see if it‚Äôs any better-calibrated.\n\nsmote_model <- fit_model(smote, logistic_reg())\n\npreds$smote_preds <- predict_prob(\n  smote_model,\n  data\n)\n\ncal_plot_breaks(preds, truth = truth_int, estimate = smote_preds)\n\n\n\n\nInteresting ‚Äì same problem. With SMOTE, we still make very similar errors to the ones we made in the case where we naively undersampled from our majority class. But let‚Äôs think back to first principles again, because the exact same rationale applies. When we undersampled, we ended up artificially increasing the baseline rate of malignant tumors in our training data, which resulted in predictions that were too high. With SMOTE, we‚Äôre doing the exact same thing: We‚Äôve stil rebalanced our data to 50/50, we‚Äôve just done it a fancier way. So of course we‚Äôll have the same problem with overprediction."
  },
  {
    "objectID": "posts/2023-03-25-balancing-classes/balancing-classes.html#a-random-forest",
    "href": "posts/2023-03-25-balancing-classes/balancing-classes.html#a-random-forest",
    "title": "Balancing Classes in Classification Problems",
    "section": "A Random Forest",
    "text": "A Random Forest\n‚ÄúBut no!‚Äù you might be thinking. ‚ÄúYou need to use a more complicated model like a random forest, because logistic regression won‚Äôt pick up on complexities in your data well enough to be well-calibrated.‚Äù\nGreat! Let‚Äôs try a random forest:\n\nrf <- fit_model(smote, rand_forest())\n\npreds$rf_preds <- predict_prob(\n  rf,\n  data\n)\n\ncal_plot_breaks(preds, truth = truth_int, estimate = rf_preds)\n\n\n\n\nSame issue again, albeit not quite as severe. And the same logic holds. In fact, it‚Äôs even more straightforward with tree-based models. In a decision tree, you would determine a predicted probability by seeing what proportion of the labels in the leaf node that you end up in based on the features belong to the positive class. But then there‚Äôs the same logic as before: We‚Äôve just artifically increased the number of instances of the positive class dramatically, so of course the proportion of labels belonging to the positive class in our leaf nodes will increase."
  },
  {
    "objectID": "posts/2023-03-25-balancing-classes/balancing-classes.html#coefficients",
    "href": "posts/2023-03-25-balancing-classes/balancing-classes.html#coefficients",
    "title": "Balancing Classes in Classification Problems",
    "section": "Coefficients",
    "text": "Coefficients\nLooking at the coefficients of our three models can help understand what‚Äôs going on here.\n\nlist(\n  \"Original\" = unbalanced_model, \n  \"Undersampled\" = balanced_model, \n  \"Smote\" = smote_model\n) %>%\n  map(tidy) %>%\n  imap(~ select(.x, \"Term\" = term, !!.y := \"estimate\")) %>%\n  reduce(inner_join, by = \"Term\") %>%\n  pretty_print()\n\n\n\n \n  \n    Term \n    Original \n    Undersampled \n    Smote \n  \n \n\n  \n    (Intercept) \n    -5.1601677 \n    -4.0441444 \n    -4.4363999 \n  \n  \n    cl_thickness \n    0.9354593 \n    0.8190776 \n    0.9159715 \n  \n\n\n\n\n\nIn all three models, the coefficient associated with the clump thickness is very similar. This should make sense intuitively: Our sampling was at random, so the relationship between the clump thickness and whether or not the tumor was malignant shouldn‚Äôt change at all.\nThe thing that does change, though, is the intercept term. In both the model where we undersampled from the majority class and in the SMOTE model, the intercept term is significantly higher than it is in the original model on the unbalanced data. This should feel similar intuitively to the idea of the baseline guess from before. As its core, the intercept term in your logistic regression model is the guess you‚Äôd make with ‚Äúno‚Äù information (in this particular case, no information means a clump thickness of 0).\nWe can illustrate this more clearly with three intercept-only models:\n\nunbalanced_intercept_only <-  glm(class ~ 1, data = data, family = binomial)\nundersampled_intecept_only <- glm(class ~ 1, data = balanced, family = binomial)\nsmote_intercept_only <-       glm(class ~ 1, data = smote, family = binomial)\n\nNow, let‚Äôs compare the intercept coefficients of these three models on a probability (read: not a log-odds) scale.\n\nconvert_log_odds_to_probability <- function(x) {\n  odds <- exp(x)\n  odds / (1 + odds)\n}\n\nunbalanced_intercept <-   coef(unbalanced_intercept_only)\nundersampled_intercept <- coef(undersampled_intecept_only)\nsmote_intercept <-        coef(smote_intercept_only)\n\nintercepts <- tibble(\n  original =     convert_log_odds_to_probability(unbalanced_intercept),\n  undersampled = convert_log_odds_to_probability(undersampled_intercept),\n  smote =        convert_log_odds_to_probability(smote_intercept)\n)\n\npretty_print(intercepts)\n\n\n\n \n  \n    original \n    undersampled \n    smote \n  \n \n\n  \n    0.3447783 \n    0.5 \n    0.5 \n  \n\n\n\n\n\nAnd it‚Äôs just as we expected: The intercept coefficient in the SMOTE model and the undersampling model are exactly 1/2, which corresponds to the fact that we balanced the classes to be exactly 50/50. And the intercept in the original model with the unbalanced classes is exactly the percentage of the data made up by the true class (malignant)."
  },
  {
    "objectID": "posts/2023-03-25-balancing-classes/balancing-classes.html#when-to-rebalance",
    "href": "posts/2023-03-25-balancing-classes/balancing-classes.html#when-to-rebalance",
    "title": "Balancing Classes in Classification Problems",
    "section": "When To Rebalance",
    "text": "When To Rebalance\nThere are some times where re-balancing the classes in your training data might make sense. One application that comes to mind is if you have strong prior information that your training data is actually biased, and is over-representing one of the two classes.\nFor instance, let‚Äôs imagine we have data from 1000 breast cancer patients and we know a priori that about 20% of tumors are malignant, but in the training data, maybe 40% of the tumors we have are malignant. Depending on the long-term goal of the project, it might make sense to undersample from the malignant cases to get the overall rate of tumors being malignant down to around the 20% prior.\nThe rationale behind doing this would be that if you wanted your model to generalize well to future cases (outside of your training set) and you knew that in the broader population about 20% of cases are malignant, your biased training data could very well result in biased predictions out-of-sample even if your predictions look good in-sample.\nAnother case where rebalancing can make sense is if you plan to use a technique like Platt Scaling or Isotonic Regression to re-calibrate your predictions ex-post. These methods are a bit beyond the scope of this post, but they‚Äôre both fantastic ways to make sure a model is giving well-calibrated predictions while using classifiers that don‚Äôt guarantee calibration, such as tree-based models. For instance, a combination of upsampling with SMOTE, using an often poorly-calibrated classifier like a tree booster such as XGBoost, and then re-calibrating ex-post with a Platt Scaler can result in a rare win-win scenario: Well-calibrated predictions, but also improved performance on normal classification metrics like the F1 score or AUC."
  },
  {
    "objectID": "posts/2023-03-25-balancing-classes/balancing-classes.html#conclusion",
    "href": "posts/2023-03-25-balancing-classes/balancing-classes.html#conclusion",
    "title": "Balancing Classes in Classification Problems",
    "section": "Conclusion",
    "text": "Conclusion\nIn the previous post, I wrote about calibration. In short, I think calibration is the single most important metric in evaluating the performance of classification models.\nAnd so with that in mind, the main takeaway of this post is that you should be very careful about trying to ‚Äúfix‚Äù the ‚Äúproblem‚Äù of class imbalances when you‚Äôre working on classification problems. If I could summarize the principle that I would follow in just a sentence, it would be that class imbalances often reflect important information about the prevalence of your classes in the real world, and it‚Äôs often risky to dismiss that information in the name of having data that‚Äôs split equally. In other words, class imbalances are usually not a problem at all. They‚Äôre a feature, not a bug."
  },
  {
    "objectID": "posts/2023-03-20-metrics-calibration/calibration.html",
    "href": "posts/2023-03-20-metrics-calibration/calibration.html",
    "title": "Calibration and Evaluating Classification Models",
    "section": "",
    "text": "There‚Äôs something of a complexity trajectory in evaluating classification models that I‚Äôve observed over the past few years. It starts with accuracy. But soon after learning about accuracy, data scientists are taught that accuracy is problematic for two reasons:\n\nIt doesn‚Äôt work well with unbalanced classes. This is the ‚Äúif 95% of people don‚Äôt have cancer and you always predict ‚Äòno cancer‚Äô, your model isn‚Äôt actually good‚Äù argument.\nIt doesn‚Äôt make any distinction between types of errors. In particular, it weighs false positives and false negatives equally, which may not be appropriate for the problem being solved.\n\nThese are both perfectly valid drawbacks of using accuracy as a metric. So then we move on. Next stop: precision and recall."
  },
  {
    "objectID": "posts/2023-03-20-metrics-calibration/calibration.html#precision-and-recall",
    "href": "posts/2023-03-20-metrics-calibration/calibration.html#precision-and-recall",
    "title": "Calibration and Evaluating Classification Models",
    "section": "Precision and Recall",
    "text": "Precision and Recall\nPrecision and recall are the two next most common classification metrics I‚Äôve seen. Precision is the percentage of the time that your model is correct when it labels something as true. Recall is the percentage of the actual true examples that your model labels as true. These metrics are important for different reasons.\nA very precise model doesn‚Äôt make very many Type I errors. For instance, if you‚Äôre predicting whether or not someone has cancer, a very precise model is ‚Äútrustworthy‚Äù in the sense that if it tells you that they do have cancer, they most likely do. You might think about precision as a metric in hiring: You probably want your hiring process to be very good at evaluating good candidates. A high-precision hiring process would mean that when you think you‚Äôve found a person who would be a great fit on your team, you‚Äôre very likely correct.\nRecall is a bit different: It‚Äôs the percentage of the true labels that your model finds. A high-recall model suffers few false negatives: When something actually belongs to the true class, your model very often predicts it as such. You might think about this in the context of our cancer example from before. A higher recall model would mean that your model catches more of the cancer cases.\nDepending on your use case, you might optimize for one of these or the other. Or you could use a blend of the two, the most common of which is the F1 score, which is the harmonic mean of precision and recall. The idea of the F1 score is to optimize for a balance of both precision and recall, as opposed to optimizing for one at the cost of the other."
  },
  {
    "objectID": "posts/2023-03-20-metrics-calibration/calibration.html#predicting-probabilities",
    "href": "posts/2023-03-20-metrics-calibration/calibration.html#predicting-probabilities",
    "title": "Calibration and Evaluating Classification Models",
    "section": "Predicting Probabilities",
    "text": "Predicting Probabilities\nYou might be reading this thinking about how this is all about predicting classes, but very often we care about predicting probabilities. For instance, at CollegeVine we make predictions about each student‚Äôs chances of getting into their favorite colleges and universities. It‚Äôs not useful for students if we predict an acceptance or a rejection. After all, they want to know their chance, and to make a class prediction would mean that we determine a cutoff point at which we decide that if someone‚Äôs chance is above that threshold, they‚Äôll get in. And if not, they won‚Äôt.\nThe problem is that there is no such threshold. More likely, college admissions is a bit of a game of chance: If five students each have a 20% chance of getting in to Carleton, for instance, I‚Äôd expect that about one of the five would get in on average. But it‚Äôd be disingenuous to make a class prediction, and I‚Äôm not sure how we‚Äôd even do that. For the five previously mentioned students, we‚Äôd expect one to get in. But if we were predicting classes we‚Äôd either predict all five to be accepted or all five to be rejected depending on where we set our threshold, and neither of those is the most likely scenario.\nWith all of that in mind, what metrics do we use instead? There are three metrics that we look at when we‚Äôre evaluating our models: The Brier Skill Score, AUC, and calibration. I‚Äôve already written about AUC, and for the purposes of this post, I‚Äôm going to focus on the one I view as the most important: Calibration."
  },
  {
    "objectID": "posts/2023-03-20-metrics-calibration/calibration.html#calibration",
    "href": "posts/2023-03-20-metrics-calibration/calibration.html#calibration",
    "title": "Calibration and Evaluating Classification Models",
    "section": "Calibration",
    "text": "Calibration\nThe most important metric we track is calibration, which we‚Äôll often evaluate by looking at a calibration plot. Below is an example of such a plot from FiveThirtyEight\n\nThe idea behind calibration is to answer the basic question ‚ÄúWhen my model said that someone had a 31% chance of winning an election, did they actually win about 31% of the time?‚Äù As you get more data, you can group your predictions into buckets to answer this question. For instance, if we‚Äôre predicting college chances we might take all the times that we said someone had between a 30 and 35% chance of getting accepted, and we‚Äôd calculate the actual proportion of the time that they were accepted. If it‚Äôs about 32.5% (close to the mean prediction in our bucket), we‚Äôd say that our model is making well-calibrated predictions in that bucket. If our model makes well-calibrated predictions of admissions chances across all buckets, it‚Äôs fair to say that it‚Äôs well-calibrated in general.\nCalibration is important because it directly affects how we interpret our model‚Äôs predictions. If we‚Äôre making probability predictions and they‚Äôre not well-calibrated, then when we say something has a 25% chance of happening, or a 50% chance of happening, or a 90% chance of happening, those numbers aren‚Äôt actually meaningful. It might be the case that the 50% probablity event happens more than the 25% one and less that the 90% one, but that isn‚Äôt even a guarantee. It also is probably not the case with a badly-calibrated model that the 50% predicted probability event happens twice as often as the 25% one.\nFor instance, let‚Äôs imagine we‚Äôre working on a classic machine learning example (and real-world) problem: Email spam detection. Ultimately, we need to predict a class: Given an email, we need our model to tell us if it‚Äôs spam or not. But for a large proportion of classifiers, this requires setting a probability threshold. For instance, if our model says there‚Äôs a greater than 50% chance that some email is spam, we mark it as spam. If false positives (a real email being marked as spam) are more problematic than false negatives (a spam email being marked as not spam), then we might increase our probability threshold to 80%, for example, which would make it ‚Äúharder‚Äù for an email to be marked as spam. But with the higher threshold, the emails that we do mark as spam we‚Äôre more confident about. Often times, we‚Äôll use domain knowledge to determine this threshold. I‚Äôve had many conversations where we set arbitrary thresholds based on our experience or our gut instincts about how the system we‚Äôre working in should work. Often, those conversations end with something like ‚Äú80% feels about right‚Äù and we go with that.\nHopefully you‚Äôre starting to see the issue here: If our model‚Äôs predictions are poorly calibrated, then it‚Äôs not possible to make a decision like that. We can‚Äôt lean into our domain knowledge about any particular number being a threshold that makes sense, because the probabilities we‚Äôre predicting don‚Äôt actually mean anything in practice. In other words, the fact that we don‚Äôt know if when we say 80%, it‚Äôs actually 95% or 35% or some other number makes it impossible to make decisions based on our predictions. In short, if our predictions aren‚Äôt well-calibrated, it‚Äôs not possible to reason about them in any meaningful way. They can also very easily be misleading.\nAs I‚Äôve mentioned before, this is especially important is the probabilities themselves are the prediction. If you‚Äôre telling a student that their chances of getting into Carleton are 27%, it goes without saying that when you say they have a 27% chance, that if they were to apply four times they‚Äôd get in about once on average. If they get in about one in eight times instead, the miscalibrated prediction could have a meaningful, negative effect on their college outcomes. For instance, if you severely overpredict a student‚Äôs chances and they end up applying to fewer schools as a result, there‚Äôs an increased likelihood of them getting in nowhere, which would be a particularly bad outcome. In this case, better-calibrated predictions lead directly to better decision-making.\nIn statistics world, it might be helpful to think of poor calibration as a bias issue: You might think of better-calibrated predictions as being less biased, in the sense that the expected value of the outcome (the actual long-run frequency of a student being accepted) is closer to what your prediction was."
  },
  {
    "objectID": "posts/2023-03-20-metrics-calibration/calibration.html#wrapping-up",
    "href": "posts/2023-03-20-metrics-calibration/calibration.html#wrapping-up",
    "title": "Calibration and Evaluating Classification Models",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nHopefully I‚Äôve convinced you that the calibration of your classifier‚Äôs predictions is important and practically meaningful. If a model‚Äôs predictions are poorly calibrated, it‚Äôs difficult (or impossible) to reason about them in a practical sense. Miscalibrated predictions can also be misleading, because we often naturally interpret probabilities as long-run frequencies. For instance ‚ÄúIf I flip this coin 100 times, it‚Äôll come up heads about half of the time.‚Äù You might think about working with poorly calibrated predictions being similar to flipping a biased coin when you‚Äôre unaware of its biases.\nIn the next post, I‚Äôll talk about class imbalances and why balancing your classes can be a particularly bad idea."
  },
  {
    "objectID": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html",
    "href": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html",
    "title": "Our 2021 Big Data Bowl Submission",
    "section": "",
    "text": "This post is my team‚Äôs 2021 NFL Big Data Bowl submission. My team was made up of me, Hugh McCreery (Baltimore Orioles), John Edwards (Seattle Mariners), and Owen McGrattan (DS student at Berkeley). I‚Äôm proud of what we‚Äôve put forth here, and hopefully you find it interesting. At the end (after the Appendix), I‚Äôve added some overall thoughts on things we were curious about or feel like we could have done better, as well as what we view as the biggest strengths and weaknesses of our submission. Enjoy!"
  },
  {
    "objectID": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#introduction",
    "href": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#introduction",
    "title": "Our 2021 Big Data Bowl Submission",
    "section": "Introduction",
    "text": "Introduction\nOur project aims to measure the ability of defensive backs at performing different aspects of their defensive duties: deterring targets (either by reputation or through good positioning), closing down receivers, and breaking up passes. We do this by fitting four models, two for predicting the probability that a receiver will be targeted on a given play and two for predicting the probability that a pass will be caught, which we then use to aggregate the contributions of defensive backs over the course of the season."
  },
  {
    "objectID": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#modeling-framework",
    "href": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#modeling-framework",
    "title": "Our 2021 Big Data Bowl Submission",
    "section": "Modeling Framework",
    "text": "Modeling Framework\nWe opted to use XGBoost for all of our models. At a high level, we chose a tree booster for its ability to find complex interactions between predictors, something we anticipated would be necessary for this project. Tree boosting also allows for null values to be present, which helped us divvy up credit in the catch probability models (more on this later). Finally, tree boosting is a relatively simple, easy-to-tune algorithm that generally performs extremely well, as was the case for us."
  },
  {
    "objectID": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#catch-probability",
    "href": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#catch-probability",
    "title": "Our 2021 Big Data Bowl Submission",
    "section": "Catch Probability",
    "text": "Catch Probability\nOur catch probability model has two distinct components: The catch probability at throw time ‚Äì as in, the chance that the pass is caught at the time the quarterback releases the ball ‚Äì and the catch probability at arrival time ‚Äì the chance the pass is caught at the time the ball arrives. These probabilities are distinct since a lot can happen between throw release and throw arrival. First, we will walk through the features that are used in building each model.\nFor the throw time model (which we will refer to as the ‚Äúthrow‚Äù model) and the arrival time model (the ‚Äúarrival‚Äù model), the most important predictors by variable importance were what we expected entering this project: the distance of the receiver to the closest defender, the position of the receiver in the \\(y\\) direction (i.e.¬†distance from the sideline), the distance of the throw, the position of the football in the \\(y\\) direction at arrival time (this is mostly catching throws to the sideline and throw aways), the velocity of the throw, the velocity and acceleration of the targeted receiver, and a composite receiver skill metric. For the arrival model, we use many of the same features. However, we do not account for the distance of the defenders to the throw vector ‚Äì which accounts for the ability to break up a pass mid-flight ‚Äì because the throw has already arrived.\nThese two models both perform quite well, and far better than random chance. The throw model accurately predicts \\(74\\%\\) of all passes, with strong precision (\\(84\\%\\)) and recall (\\(76\\%\\)), and an AUC of \\(0.81\\). As can be expected, our arrival model outperforms the throw model in all measures - accurately predicting \\(78\\%\\) of all passes with a precision of \\(88\\%\\), a recall of \\(79\\%\\), and an AUC of \\(.87\\). All of these metrics were calculated on a held-out data set not used in model training. Below are plots of the calibration of the predictions of each of the models on the same held-out set.\n \nWe can do a few particularly interesting things with the predictions from these two models in tandem. Namely, we can use the two to calculate marginal effects of the play of the defensive backs. A simple example is as follows: For a given pass attempt, our throw model estimates that there is a \\(80\\%\\) chance of a catch. By the time that pass arrives, however, our arrival model estimates instead that there is a \\(50\\%\\) chance of a catch. Ultimately, the play results in a drop. In total, our defense can get credit for \\(+.8\\) drops, but we can break it down into \\(+.3\\) drops worth from closing on the receiver and \\(+.5\\) drops from breaking up the play, based on how the individual components differ. In other words, we subtract the probability of a catch at arrival time from the probability at throw time to get the credit for closing down the receiver, and we subtract the true outcome of the play from the probability of a catch at arrival time to get the credit for breaking up the pass.\nThe main challenge comes not from calculating the overall credit on the play, but from the distribution of credit among the defenders. In the previous example where we have to credit the defense with \\(+.8\\) drops added, who exactly on the defense do we give that credit to? There are a couple of heuristics that might make sense. One option would be to just split the credit up evenly among the defense, but this would be a bad heuristic because some defenders will have more of an impact on a pass being caught than others, and thus deserve more credit. We might also give all of the credit to the nearest defender, but that would be unfair to players who are within half a yard of the play and are also affecting its outcome but would get no credit under this heuristic. Ultimately, we opted to use the models to engineer the credit each player deserves by seeing how the catch probabilities would change if we magically removed them from the field, which we believe to be a better heuristic than the previous ones described. To implement this heuristic, we remove one defender from our data and re-run the predictions to see how big the magnitude of the change in catch probability is. The bigger the magnitude difference, the more credit that player gets. Then, we calculate the credit each defender gets with \\(credit_{i} = \\frac{min(d_{i}, 0)}{\\sum_{i} min(d_{i}, 0)}\\) where \\(d_{i}\\) is the catch probability without that player on the field minus the catch probability with him on the field. In other words, if one player gets \\(75\\%\\) of the credit for a play and the play is worth \\(+.8\\) drops added, then that player gets \\(.8 \\cdot .75 = +.6\\) drops of credit, and the remaining \\(+.2\\) drops are divvied up amongst the other defenders in the same fashion."
  },
  {
    "objectID": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#target-probability",
    "href": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#target-probability",
    "title": "Our 2021 Big Data Bowl Submission",
    "section": "Target Probability",
    "text": "Target Probability\nOur target model is based on comparing the probabilities that a receiver is targeted before the play begins and when the ball is thrown with the actual receiver targeted. We can use these probabilities to make estimates of how well the defender is covering (are receivers less likely to be thrown the ball because of the pre-throw work of a defensive back?) and how much respect they get from opposing offenses (do quarterbacks tend to make different decisions at throw time because of the defensive back?).\nTo determine the probability of a receiver being targeted before the play, we chose to take a naive approach. Each receiver on the field is assigned a ‚Äútarget rate‚Äù of \\(\\frac{targets}{\\sqrt{plays}}\\), which is then adjusted for the other receivers on the field and used as the only feature for a logit model. The idea of this rate was to construct a statistic that rewarded receivers for showing high target rates over a large sample but also gave receivers who play more often more credit.\nThe model for target probability at the time of throw was a tree booster similar to the two catch probability models. This model uses positional data, comparing the receiver position to the QB and the three closest defenders along with a variety of situational factors such as the distance to the first down line, time left, weather conditions, and how open that receiver is relative to others on the play to determine how likely that receiver is to be targeted.\nThe pre-snap model, which by design only considers the players on the field for the offense, performs relatively well for the lack of information with an AUC of \\(.59\\) and is well-calibrated on a held-out dataset. The pre-throw model performs much better given the extra information, with \\(89\\%\\) recall, \\(94\\%\\) precision, and \\(.94\\) AUC. Calibration plots of the models on held-out data are below.\n \nWe can estimate how a defensive back is impacting the decisions made by the quarterback through two effects. The first, comparing the target probability before the play to the target probability at the time of the throw is meant to estimate how well the receiver is covered on the play. For example, consider a receiver with a target probability of \\(20\\%\\) before the snap who ends up open enough to get a target probability of \\(50\\%\\) when the ball is thrown. This difference is attributed to the closest defensive back who would be credited with \\(-0.3\\) coverage targets. If on the same play another receiver had a pre-snap target probability \\(30\\%\\) but a target probability of \\(0\\%\\) at throw time, the closest defensive back would be credited with \\(+0.3\\) coverage targets. The other effect attempts to measure how the quarterback is deterred from throwing when that particular defensive back is in the area of the receiver by comparing the probability of a target to the actual result. So if a certain receiver has a target probability of \\(60\\%\\) at the time of the throw and isn‚Äôt targeted, the closest defender is credited with \\(+0.6\\) deterrence targets."
  },
  {
    "objectID": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#results",
    "href": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#results",
    "title": "Our 2021 Big Data Bowl Submission",
    "section": "Results",
    "text": "Results\nHaving produced these four models that gave us estimates of the influence of the defensive backs on a given play, we can accumulate the results over an entire season to produce an estimate of individual skill across the dimensions described by the models. As there is no straightforward way to measure the relative value of these skills, we chose to combine the individual skill percentiles for each defender as a measure of their overall skill. With that as the estimate, these are our top 15 pass-defending defensive backs in 2018:\n\n\n\nleaderboard\n\n\nA full leaderboard of these can be found on our Shiny app in the Overall Rankings tab. To help display these results and a few other metrics (such as how difficult the defensive assignment was based on pre-snap target probability and actual separation from the receiver at throw time) we developed player cards for each qualifying defender. For example, this is our card for #1 defender Richard Sherman:\n\n\n\nsherman_card\n\n\nThese cards can also be found on our Shiny app under Player Cards."
  },
  {
    "objectID": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#further-work",
    "href": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#further-work",
    "title": "Our 2021 Big Data Bowl Submission",
    "section": "Further Work",
    "text": "Further Work\nThere are several things that we didn‚Äôt consider or that would be interesting extensions of this project. Two clear ones are interceptions added and fumbles added, as those are hugely impactful football plays that can swing the outcomes of games. We also only considered raw changes in aggregating the player stats (i.e.¬†targets prevented and drops added), but using EPA added instead would certainly be a better metric, since not all drops are created equal. In addition, it is not clear that a drop added is worth the same as a target deterred ‚Äì an assumption we made ‚Äì and EPA would help solve this problem too. It would also be interesting to test how similar our metrics are between seasons to confirm that our metrics are measuring the stable skill of a defender."
  },
  {
    "objectID": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#appendix",
    "href": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#appendix",
    "title": "Our 2021 Big Data Bowl Submission",
    "section": "Appendix",
    "text": "Appendix\nAll of our code is hosted in two Github repos: hjmbigdatabowl/bdb2021, which hosts our modeling code, and hjmbigdatabowl/bdb2021-shiny, which hosts the Shiny app.\nThere is extensive documentation for our code on our pkgdown site.\nThe Shiny app can be found here, which lets you explore our models, results, and player ratings."
  },
  {
    "objectID": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#thoughts",
    "href": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#thoughts",
    "title": "Our 2021 Big Data Bowl Submission",
    "section": "Thoughts",
    "text": "Thoughts\nFirst, a disclaimer for everything that follows: I‚Äôm extremely proud of what we submitted, and the work that we did. We spent hundreds of hours working on this project over about three months, which is an enormous undertaking for four people with other full-time commitments. Especially in the final weeks, I was spending well over ten hours a week working on this project, and probably closer to fifteen or twenty. All of that said, in the grand scheme of things, the 300 hours or so collective hours that we spent conceiving of and working on this project is nowhere near the amount of time and effort that could be poured into a project that a data science team of four was working on full-time for three months. Given more time, I believe there are significant ways in which we could have improved our final product.\n\nStrengths\n\nCommunication and interpretability of the results. At the end of the day, when you work in a front office (as Hugh, John, and I know), you need coaches to believe what you‚Äôre telling them, and building a product that they can understand and explain is a major step in that direction. Overly complex or convoluted methodology will only get your work ignored, and I think that we did a particularly good job of building a product that is interpretable and useful.\nBuilding a model that evaluates something useful. At the end of the day, I‚Äôm happy with the statistics we chose to put forward. In my view, barring special events like fumbles forced, tackling, and interceptions (all valuable things that should be worked on further to expand this project), the way that we evaluate defensive back performance by looking at what we view as the four major components ‚Äì deterring passes by reputation, deterring passes with good positioning and coverage, closing down receivers, and breaking up passes ‚Äì are the four most important skills for defensive backs in the NFL.\nDivvying up credit in a clever way. I also think that the way we opted to divvy up the credit among the defenders in the two catch probability models was particularly clever, and not something that most teams would do. As we laid out in our submission, it doesn‚Äôt make sense to divvy credit evenly or give all credit to the nearest defender to the target, and I think that the approach that we took was both novel and easily defensible.\n\n\n\nWeaknesses + Potential Improvements\n\nClearly, weighing each of the four components of defense that we measured equally is the wrong way to go. A better strategy would be to try to correlate them with defensive EPA or a similar stat and use the correlation coefficients / R-Squared values / MSEs / etc. to weigh the four components based on how strongly they predict EPA. The problem, though, is that we don‚Äôt have a defensive EPA, which means we‚Äôd need to model it. In addition, it‚Äôs unclear how to model a counterfactual. What I mean by that is that for a statistic like deterrence, we need to measure the EPA caused by a defender not being thrown at, which is an inherently difficult thing to model. So difficult, in fact, that I believe that the first NFL team to come up with a good way of doing this will have a Moneyball-esque leg up against the competition until other teams catch up (Ravens analysts, I‚Äôm looking at you).\nThere are other aspects of defensive performance that we‚Äôre not measuring. Two important categories are turnovers generated and YAC prevented. Both of these are hugely valuable for preventing points, and both are models that we didn‚Äôt have time to build.\nStability testing. We‚Äôre probably interested in how stable our metrics are across seasons. For example, does Stephon Gilmore being a 96 overall this year predict his rating next year at all? Since we only have one season of data, the answer is that we don‚Äôt know. This type of stability testing would be useful for predicting future performance, though.\n\n\n\nGeneral Thoughts\nAgain, I‚Äôm proud of what we‚Äôve done here. In particular, spot-checking some of the numbers has been fascinating. There‚Äôs often very little correlation between metrics, which seems good, and often the correlations can be negative. The way to think about this, as my teammate Hugh eloquently put it, is that we should think about the negative correlations as being similar to \\(ISO\\) vs.¬†\\(K%\\) in the MLB: A player who is fantastic in coverage and thus often deters throws necessarily will have fewer opportunities to break up passes, meaning he will see a lower score for closing, breakups, or both. We see this type of behavior with Gilmore, Peterson, Sherman, and more. It‚Äôs also been interesting to find that our models think the same corners are really good as the eye test does, which is an encouraging sign. Interestingly, though, some of our top-ranked defensive backs are players who don‚Äôt tend to make top lists, and some of our lowest-ranked ones (like Jason McCourty and Jalen Ramsey), do. This is reminiscent of the attitude changes about certain MLB players in the early 2010s as we began developing more advanced statistics to measure performance (i.e.¬†Derek Jeter‚Äôs defense being terrible and Michael Bourn being sneakily good).\nAnd with that, I‚Äôve wrapped up my addendum to our submission. Feel free to reach out with any questions. My contact info is on my blog, my Github, my LinkedIn, etc. Thanks for reading!"
  },
  {
    "objectID": "posts/2021-01-13-a-gentle-introduction-to-markov-chains-and-mcmc/index.html",
    "href": "posts/2021-01-13-a-gentle-introduction-to-markov-chains-and-mcmc/index.html",
    "title": "A Gentle Introduction to Markov Chains and MCMC",
    "section": "",
    "text": "Introduction\nEvery other Friday at work we have a meeting called All Hands. During the first half of All Hands a member of the team gives a presentation, which is split up into two pieces: A personal presentation ‚Äì your favorite food, TV shows, books, etc. ‚Äì and a mini-lesson, which can be about any topic of interest that‚Äôs unrelated to work. Yesterday it was my turn, and I gave my mini-lesson on Markov Chains and Markov Chain Monte Carlo. This post memorializes what I covered.\n\n\nMarkov Chains\nFirst, what is a Markov Chain? It‚Äôs easiest to break it down into it‚Äôs component parts. A Markov Chain is a chain, or sequence of events, that follow the Markov Property. And the Markov Property is pretty intuitive: The Markov Property says that the next state in a sequence (chain) is only dependent on the current state. Statisticians would call this ‚Äúmemorylessness,‚Äù and we can write out the property in its true mathematical form below, where \\(X\\) is a random variable and \\(x\\_{t}\\) is the probability distribution that \\(X\\) takes on at time \\(t\\).\n\\[\np(x_{t+1} | x_{t}, x_{t-1}, x_{t-2}, .. x_{0}) = p(x_{t+1} | x_{t})\n\\]\nIn plain English, all this definition is saying is that if you are following the Markov Property, then where you go next is only determined by where you are now, and how you got to where you are has no impact. At the end of my talk, one of my coworkers commented that this property is actually quite beautiful in a real-world sense, and I feel the same way. It certainly could have been the example of a guiding principle that I choose to follow that I used in my personal presentation.\nSo, now that we know what a Markov Chain is, let‚Äôs walk through an example. The most commonly seen type of Markov Chain is called a random walk. Simply, a random walk is a Markov Chain where the next state is just determined by the current state plus some random noise. I‚Äôve coded up an example below:\n\n## library(purrr)\n## library(magrittr)\n## library(ggplot2)\n\nrandomly_walk <- function(.ix = c(), n_steps = 100) {\n  results <- numeric(n_steps)\n  for (i in 1:(n_steps-1)) {\n    results[i + 1] <- results[i] + rnorm(1, 0, 1)\n  }\n  \n  return(results)\n}\n\n(\n  random_walk <- randomly_walk() %>%\n    tibble(position = .)\n)\n\n# A tibble: 100 √ó 1\n   position\n      <dbl>\n 1   0     \n 2  -1.13  \n 3   1.52  \n 4  -0.0912\n 5  -0.348 \n 6  -1.66  \n 7  -0.884 \n 8  -0.491 \n 9   0.179 \n10  -0.812 \n# ‚Ä¶ with 90 more rows\n\n\nThat table shows a random walk with 100 steps, generated by adding standard normal noise to the position after each step. Let‚Äôs plot it and see what it looks like.\n\nrandom_walk %>%\n  rownames_to_column(var = 'time') %>%\n  mutate(time = time %>% as.numeric()) %>%\n  ggplot(aes(time, position)) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nCool! The walk starts at 0, and then jumps around randomly a bunch until \\(t=100\\). It‚Äôll be more interesting once we simulate 20 random walks.\n\nn_steps <- 500\nn_chains <- 20\ntwenty_walks <- map(\n  1:n_chains, \n  randomly_walk,\n  n_steps\n) %>%\n  tibble(position = .) %>%\n  unnest(cols = c(position)) %>%\n  mutate(\n    time = rep(1:n_steps, n_chains),\n    walk = map(1:n_chains, rep, n_steps) %>% \n      unlist() %>% \n      as.factor()\n  )\n\ntwenty_walks %>%\n  ggplot() +\n  aes(time, position, color = walk) +\n  geom_line() +\n  theme_minimal() + \n  theme(legend.position = 'none')\n\n\n\n\n\n\n\n\nSo, what‚Äôs going on here? It basically looks how we‚Äôd expect. At any given time point, the mean position of the 20 is about zero, but the standard deviation of those positions goes up over time. Specifically, at any given time \\(t\\), the standard deviation of the positions should be roughly equal to \\(\\sqrt t\\), because of how the variance is compounding. Remember, these random walks are Markov Chains because at every time \\(t\\), I defined the position \\(y\\_{t+1}\\) to be \\(y\\_{t} + \\mathcal{N}(0, 1)\\), or the the next position is the current position plus a standard normal noise (i.e.¬†zero-centered with unit variance).\nCool, so now we have an idea of what a Markov Chain is and how a random walk is an example of one. Now, why do we care? What kinds of problems can we solve with Markov Chains? It turns out that one thing we can use them to do is to calculate intractable integrals. What does this mean? Well, remembering back to a calculus class once upon a time, we know if we have some function \\(f(x) = 2x\\), we can integrate that function by following a one of a couple of rules. In this case, that rule is to raise the coefficient in front of the \\(x\\) to turn it into a power, such that the new exponent equals the old one plus one, and the new coefficient equals the old one divided by the new exponent. For \\(f(x)\\), we find \\(F(x) = \\int f(x) = x^{2} + c\\), where \\(c\\) is a constant. However, in many applications, such as Bayesian statistics, we run into functions of hundreds or thousands of parameters that are intractable to integrate. In other words, even really, really powerful calculators can‚Äôt integrate them: there are just too many parameters. So, we‚Äôre stuck. How do we integrate a function that even a super powerful calculator can‚Äôt? In steps Markov Chain Monte Carlo, coming to the rescue.\n\n\nMarkov Chain Monte Carlo\nIt turns out that we can use Markov Chains to approximate the integral in cases where we can‚Äôt calculate it directly. This is an incredible powerful discovery, and one that we‚Äôve only been able to really take advantage of in the past twenty or so years, as computing power has grown exponentially. So, how do we actually do it? Let‚Äôs frame it as a simple problem that‚Äôs isomorphic to the actual problem at hand.\nImagine you are the ruler of an island kingdom, which has four islands. Island 1 has population 1, Island 2 has population 2, Island 3 has population 3, and Island 4 has population 4. And, imagine that you want to spend time on each island proportional to the percentage of the total population of your kingdom that it makes up. In other words, you want to spend 10% of your time on Island 1, and so on. But, you have a problem: You don‚Äôt know how to add. Imagine that the only mathematical operation you know how to do is divide. Can you figure out a way to spend your time how you want without being able to calculate the total population of your kingdom?\nMost likely, how you‚Äôd solve this problem isn‚Äôt immediately obvious, but there are a few brilliant algorithms that help us achieve our goal. One of them is proposed to you by two of your friends, Metropolis and Hastings. I‚Äôve coded up their suggestion below:\n\nrun_rwmh <- function(n_iters = 1000, island_populations = 1:4) {\n  locations <- numeric(n_iters)\n  \n  ## randomly choose an island to start on\n  locations[1] <- sample(island_populations, 1)\n\n  for (i in 1:(n_iters-1)) {\n    \n    ## propose a new island to go to\n    proposal_island <- sample(setdiff(island_populations, locations[i]), 1) \n    \n    ## if that island has more people, always go\n    if (proposal_island > locations[i]) {\n      locations[i + 1] <- proposal_island\n    } else {\n      ## if it has fewer people, flip a coin with probability\n      ##   proportional to the ratio of the populations to\n      ##   decide whether to go or stay\n      acceptance_probability <- proposal_island / locations[i]\n      locations[i + 1] <- \n        sample(\n          c(proposal_island, locations[i]), 1, \n          prob = c(acceptance_probability, 1 - acceptance_probability)\n        )\n    }\n  }\n  return(locations)\n}\n\nHere‚Äôs the algorithm your friends propose:\n\nPick a random island to start on.\nOn each day, randomly select a new island to go to (the proposal island).\nDo one of the following, depending on the populations of the islands:\n\nIf the proposal island has more people than the current island, go to the proposal island.\nIf it has fewer people, then flip a coin with probability equal to the proposal island‚Äôs population divided by the current island‚Äôs. If the coin comes up heads, go to the proposal island.\n\nDo it again a bunch of times.\n\nSo, how does this algorithm perform? Let‚Äôs try it out!\n\nrun_rwmh(n_iters = 10) %>%\n  tibble(island = .) %>%\n  group_by(island) %>%\n  summarize(days_spent = n(), .groups = 'drop') %>%\n  mutate(day_proportion = days_spent / sum(days_spent))\n\n# A tibble: 4 √ó 3\n  island days_spent day_proportion\n   <dbl>      <int>          <dbl>\n1      1          1            0.1\n2      2          1            0.1\n3      3          3            0.3\n4      4          5            0.5\n\n\nUnsurprisingly, with only 10 iterations the algorithm does not perform particularly well. But what about if we give it a lot more time? Let‚Äôs try 10,000 iterations.\n\nsome_islands <- run_rwmh(n_iters = 10000) %>%\n  tibble(island = .) %>%\n  group_by(island) %>%\n  summarize(days_spent = n(), .groups = 'drop') %>%\n  mutate(day_proportion = days_spent / sum(days_spent),\n         error_margin = day_proportion / (island / sum(island)) - 1)\n\nmean_error_margin <- mean(some_islands$error_margin)\nsd_error_margin <- sd(some_islands$error_margin)\nsome_islands\n\n# A tibble: 4 √ó 4\n  island days_spent day_proportion error_margin\n   <dbl>      <int>          <dbl>        <dbl>\n1      1       1000          0.1        0      \n2      2       1958          0.196     -0.0210 \n3      3       3063          0.306      0.0210 \n4      4       3979          0.398     -0.00525\n\n\nMuch better! After 10,000 iterations, we‚Äôre spending almost the exact proportion of time on each island that we want to be, as evidenced by the tiny error margins. In addition, the standard deviation of the error margins is 0.01735, which is tiny. That‚Äôs awesome! But what about if the system is more complex? Like, what if we had 100 islands?\n\nmore_islands <- run_rwmh(n_iters = 10000, island_populations = 1:100) %>%\n  tibble(island = .) %>%\n  group_by(island) %>%\n  summarize(days_spent = n(), .groups = 'drop') %>%\n  mutate(day_proportion = days_spent / sum(days_spent),\n         error_margin = day_proportion / (island / sum(island)) - 1)\n\nmean_error_margin <- mean(more_islands$error_margin)\nsd_error_margin <- sd(more_islands$error_margin)\nmore_islands\n\n# A tibble: 100 √ó 4\n   island days_spent day_proportion error_margin\n    <dbl>      <int>          <dbl>        <dbl>\n 1      1          1         0.0001      -0.495 \n 2      2          8         0.0008       1.02  \n 3      3          9         0.0009       0.515 \n 4      4         19         0.0019       1.40  \n 5      5          8         0.0008      -0.192 \n 6      6         13         0.0013       0.0942\n 7      7         14         0.0014       0.0100\n 8      8         11         0.0011      -0.306 \n 9      9         19         0.0019       0.0661\n10     10         28         0.0028       0.414 \n# ‚Ä¶ with 90 more rows\n\n\nNo problem! Even with the extra islands, the mean error margin is still zero, and the standard deviation of the error margins is 0.23184, which is also small, but not as small as the simpler system. It‚Äôs true that a more complex system (i.e.¬†more islands) would mean that we need more iterations to converge in probability to the proportions we‚Äôre shooting for, but the algorithm will still work with enough time. Let‚Äôs try running it one more time on the complex system, but this time with a million iterations.\n\nmore_iters <- run_rwmh(n_iters = 1000000, island_populations = 1:100) %>%\n  tibble(island = .) %>%\n  group_by(island) %>%\n  summarize(days_spent = n(), .groups = 'drop') %>%\n  mutate(day_proportion = days_spent / sum(days_spent),\n         error_margin = day_proportion / (island / sum(island)) - 1)\n\nmean_error_margin <- mean(more_iters$error_margin)\nsd_error_margin <- sd(more_iters$error_margin)\nmore_iters\n\n# A tibble: 100 √ó 4\n   island days_spent day_proportion error_margin\n    <dbl>      <int>          <dbl>        <dbl>\n 1      1        199       0.000199      0.00495\n 2      2        432       0.000432      0.0908 \n 3      3        614       0.000614      0.0336 \n 4      4        726       0.000726     -0.0834 \n 5      5        985       0.000985     -0.00515\n 6      6       1157       0.00116      -0.0262 \n 7      7       1409       0.00141       0.0165 \n 8      8       1601       0.00160       0.0106 \n 9      9       1695       0.00170      -0.0489 \n10     10       1952       0.00195      -0.0142 \n# ‚Ä¶ with 90 more rows\n\n\nLooks like that did the trick! The standard deviation of the error margins fell to 0.02015, just as we expected.\nThis algorithm is called the Metropolis-Hastings Algorithm, and it‚Äôs one of many in the class of Markov Chain Monte Carlo algorithms. Some others are the Gibbs Sampler and Hamiltonian Monte Carlo, both of which are frequently used in Bayesian statistics for estimating the parameters of regression models with hundreds of thousands of parameters. In short, these algorithms allow us to solve problems that were literally impossible to solve only two decades ago or so, which is an amazing feat!\n\n\nRecap\n\nMarkov Chains are not that scary! They‚Äôre just a memoryless sequence of events, meaning that where you came from doesn‚Äôt impact where you go next.\nMarkov Chain Monte Carlo algorithms like the Metropolis-Hastings can be quite simple, and let us solve impossibly hard problems."
  },
  {
    "objectID": "posts/2021-02-11-highlights-from-rstudio-global/index.html",
    "href": "posts/2021-02-11-highlights-from-rstudio-global/index.html",
    "title": "Highlights From rstudio::global",
    "section": "",
    "text": "rstudio::global, this year‚Äôs iteration of the annual RStudio conference, was a few weeks ago. Here were some highlights:"
  },
  {
    "objectID": "posts/2021-02-11-highlights-from-rstudio-global/index.html#talks",
    "href": "posts/2021-02-11-highlights-from-rstudio-global/index.html#talks",
    "title": "Highlights From rstudio::global",
    "section": "Talks",
    "text": "Talks\nThere were a few talks I really loved:\n\nUsing R to Up Your Experimentation Game, by Shirbi Ish-Shalom. On experimentation, sequential testing, taking big swings, and being statistically rigorous\nMaintaining the House the Tidyverse Built, by Hadley Wickham. On building and maintaining the Tidyverse, and what package maintenance in the real world is like when you have millions of downloads.\noRganization: How to Make Internal R Packages Part of Your Team, by Emily Riederer. On how using internal packages (like collegeviner at CollegeVine!) can improve your R workflow and make teamwork in R dramatically easier, smoother, and more efficient.\nFairness and Data Science: Failures, Factors, and Futures, by Grant Fleming. On model fairness, bias, and evaluation techniques, and why they‚Äôre important to get right."
  },
  {
    "objectID": "posts/2021-02-11-highlights-from-rstudio-global/index.html#cool-new-things",
    "href": "posts/2021-02-11-highlights-from-rstudio-global/index.html#cool-new-things",
    "title": "Highlights From rstudio::global",
    "section": "Cool New Things",
    "text": "Cool New Things\n\nfinetune, Max Kuhn‚Äôs new tune-adjacent package, is live (albeit a little buggy)! It has some cool new model tuning algorithms, including racing methods with tune_race_anova() and tune_race_win_loss(), in addition to my personal favorite: tune_sim_anneal() for Simulated Annealing! Link to the talk\nMajor improvements to shiny, including some serious caching upgrades that‚Äôll improve performance dramatically! Link to the talk"
  },
  {
    "objectID": "posts/2021-02-11-highlights-from-rstudio-global/index.html#other-highlights",
    "href": "posts/2021-02-11-highlights-from-rstudio-global/index.html#other-highlights",
    "title": "Highlights From rstudio::global",
    "section": "Other Highlights",
    "text": "Other Highlights\n\nMeeting a bunch of people in the breakout sessions! This year, there were virtual ‚Äútables‚Äù where you could drag your avatar to ‚Äúsit down‚Äù, and once you were close enough to a table you could hear all of its conversation."
  },
  {
    "objectID": "series.html",
    "href": "series.html",
    "title": "Series",
    "section": "",
    "text": "Heavily inspired by The Missing Semester of Your CS Education, this is a series on all of the aspects of doing data science that you probably didn‚Äôt learn in class or from doing research. I‚Äôve learned a lot in the past few years about what it takes to do data science professionally and, frankly, my data science education had some gaping holes. The goal of this series is to help others fill some of those holes (to the best of my ability) by sharing some of the things I‚Äôve learned along the way from great mentors and terrible mistakes.\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\nApr 1, 2023\n\n\nWriting Internal Libraries for Analytics Work\n\n\n13 min\n\n\n\n\nApr 5, 2023\n\n\nUnit Testing Analytics Code\n\n\n13 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "series.html#ab-testing",
    "href": "series.html#ab-testing",
    "title": "Series",
    "section": "A/B Testing",
    "text": "A/B Testing\nA series on common pitfalls in A/B testing and how sequential testing solves many common A/B testing problems.\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\nMar 25, 2022\n\n\nA/B Testing: A Primer\n\n\n3 min\n\n\n\n\nApr 9, 2022\n\n\nRunning A/B Tests\n\n\n4 min\n\n\n\n\nApr 10, 2022\n\n\nCalling A/B Tests\n\n\n6 min\n\n\n\n\nApr 17, 2022\n\n\nSequential Testing\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "series.html#all-posts",
    "href": "series.html#all-posts",
    "title": "Series",
    "section": "All Posts",
    "text": "All Posts\nTo see all of my blog posts, go to the posts page"
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Matt Kaye",
    "section": "",
    "text": "I like writing code for fun, to explore interesting questions and problems, and to improve the tools I use.\nBroadly speaking, I‚Äôd break down this work into three buckets: R packages I author or maintain, contributions to open-source libraries, and other side projects."
  },
  {
    "objectID": "code.html#r-packages",
    "href": "code.html#r-packages",
    "title": "Matt Kaye",
    "section": "R Packages",
    "text": "R Packages\n\nslackr is an R wrapper of the Slack API\nfitbitr is an R wrapper of the Fitbit API\nlightMLFlow is an opinionated R wrapper of the MLFlow REST API"
  },
  {
    "objectID": "code.html#open-source-contributions",
    "href": "code.html#open-source-contributions",
    "title": "Matt Kaye",
    "section": "Open-Source Contributions",
    "text": "Open-Source Contributions\n\nAdding JSON schema validation to MLFlow\nAdding a MAP@K implementation to recmetrics\nAdding a minimal CI/CD process to recmetrics"
  },
  {
    "objectID": "code.html#miscellaneous-side-projects",
    "href": "code.html#miscellaneous-side-projects",
    "title": "Matt Kaye",
    "section": "Miscellaneous Side Projects",
    "text": "Miscellaneous Side Projects\n\nbdb2021 is my team‚Äôs submission to the 2021 NFL Big Data Bowl\nugly-kmeans is an over-the-top (yet imperfect) R solution to one of our data science technical interview questions: (roughly) implement K-Means\nespn-cfb-win-prob runs diagnostics on ESPN‚Äôs college football win probability model"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Matt Kaye",
    "section": "",
    "text": "Hi! I‚Äôm Matt\n\nI‚Äôm a data scientist at CollegeVine\n\n\nI care about doing better data science by leaning into domain knowledge and utilizing software engineering best practices"
  },
  {
    "objectID": "blogroll.html",
    "href": "blogroll.html",
    "title": "Blogroll",
    "section": "",
    "text": "R-Bloggers"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Matt Kaye",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nUnit Testing Analytics Code\n\n\nHow do you know your code actually works?\n\n\n\n\nR\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2023\n\n\n13 min\n\n\n\n\n\n\n\n\nWriting Internal Libraries for Analytics Work\n\n\nOr packages, or modules, or whatever you wish to call them\n\n\n\n\nR\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nApr 1, 2023\n\n\n13 min\n\n\n\n\n\n\n\n\nBalancing Classes in Classification Problems\n\n\nAnd why it‚Äôs generally a bad idea\n\n\n\n\nR\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nApr 1, 2023\n\n\n9 min\n\n\n\n\n\n\n\n\nCalibration and Evaluating Classification Models\n\n\nMetrics for probability predictions\n\n\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nMar 20, 2023\n\n\n7 min\n\n\n\n\n\n\n\n\nInterpreting AUC-ROC\n\n\n\n\n\n\n\ndata science\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMar 9, 2023\n\n\n9 min\n\n\n\n\n\n\n\n\nExploring the Tail Behavior of ESPN‚Äôs Win Probability Model\n\n\n\n\n\n\n\ndata science\n\n\nstatistics\n\n\nsports analytics\n\n\n\n\n\n\n\n\n\n\n\nJan 9, 2023\n\n\n8 min\n\n\n\n\n\n\n\n\nSequential Testing\n\n\n\n\n\n\n\ndata science\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nApr 17, 2022\n\n\n4 min\n\n\n\n\n\n\n\n\nCalling A/B Tests\n\n\n\n\n\n\n\ndata science\n\n\nstatistics\n\n\na/b testing\n\n\n\n\n\n\n\n\n\n\n\nApr 10, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\nRunning A/B Tests\n\n\n\n\n\n\n\ndata science\n\n\nstatistics\n\n\na/b testing\n\n\n\n\n\n\n\n\n\n\n\nApr 9, 2022\n\n\n4 min\n\n\n\n\n\n\n\n\nA/B Testing: A Primer\n\n\n\n\n\n\n\ndata science\n\n\nstatistics\n\n\na/b testing\n\n\n\n\n\n\n\n\n\n\n\nMar 25, 2022\n\n\n3 min\n\n\n\n\n\n\n\n\nDeploying MLFlow on Heroku (with Heroku Postgres, S3, and nginx for basic auth)\n\n\n\n\n\n\n\nmlops\n\n\n\n\n\n\n\n\n\n\n\nDec 26, 2021\n\n\n6 min\n\n\n\n\n\n\n\n\nNotes on Hiring Data Analysts + Scientists\n\n\n\n\n\n\n\ndata science\n\n\nhiring\n\n\n\n\n\n\n\n\n\n\n\nDec 22, 2021\n\n\n13 min\n\n\n\n\n\n\n\n\nWorking With Your Fitbit Data in R\n\n\n\n\n\n\n\nR\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nJun 8, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\nHighlights From rstudio::global\n\n\n\n\n\n\n\nR\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2021\n\n\n1 min\n\n\n\n\n\n\n\n\nWhat‚Äôs New in slackr 2.1.0\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nFeb 7, 2021\n\n\n1 min\n\n\n\n\n\n\n\n\nA Gentle Introduction to Markov Chains and MCMC\n\n\n\n\n\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2021\n\n\n9 min\n\n\n\n\n\n\n\n\nWhat I‚Äôve Been Drinking in Quarantine (Plus a Cocktail Lesson)\n\n\n\n\n\n\n\nfood + bev\n\n\n\n\n\n\n\n\n\n\n\nJan 10, 2021\n\n\n14 min\n\n\n\n\n\n\n\n\nOur 2021 Big Data Bowl Submission\n\n\n\n\n\n\n\ndata science\n\n\nsports analytics\n\n\n\n\n\n\n\n\n\n\n\nJan 7, 2021\n\n\n14 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "stats-in-the-wild.html",
    "href": "stats-in-the-wild.html",
    "title": "Statistics in the Wild",
    "section": "",
    "text": "Since 92.4% of statistics are made up‚Ä¶\n\n\n\n\n\n\nDisclaimer: This is a rolling compilation of a number of uses of statistics by the media, on TV broadcasts, and from other sources, mainly having to do with sports and politics. Some are good, some are bad, and some are laughable. Decisions about which belong in each category are left to the reader. Kindly excuse any slight misquotes, and enjoy!\n\n\n\n\nAll-time Stat: Oakland Athletics outfielder Khris Davis hit .247 for four years in a row.\n4/1/2023 San Diego State is the seventh unique team from California to make the Final Four, tying Pennsylvania for the most unique Final Four schools from a single state.\n3/19/2023 - CBS Broadcast of Michigan State vs.¬†Marquette: Tom Izzo is 23-7 when he only has one day to prepare for an NCAA tournament game (so in the Elite 8 or the round of 32).\n3/18/2023 - CBS Broadcast of Kansas vs.¬†Arkansas: Kansas is 47-0 all time when up by 8+ points at half.\nFebruary 2021 - Sanofi & GSK: ‚Äú100% Efficacy‚Äù\n\n\n\n\n\nDecember 2021 - LinkedIn: ‚ÄúCorrelation sometimes equals causation‚Äù\n\n\n\n\n\nMarch-April 2021 - NYTimes: Linear extrapolation to ‚Äúproject‚Äù the proportion of the U.S. population that will be vaccinated by future dates.\n\n\n\n\n\nMarch-November 2020 - Everyone: A 20% false negative rate on COVID-19 rapid tests means that when a test says you‚Äôre negative, there‚Äôs a 20% chance you‚Äôre positive.\n11/7/20 - @realDonaldTrump A soon-to-be quarantined Twitter account:\n\n10/3/20 - Twitter:\n\n9/30/20: As of today, the Minnesota Twins have lost 18 consecutive playoff games.\n9/28/20 - FanGraphs: Fernando Tatis Jr.‚Äôs BABIP and wRC+ by month in 2020.\n\n\n\n\n\n9/28/2020: Shane Bieber‚Äôs 2020 K/9 rate was 14.20. Although the season was shortened, if he had kept up that pace it would have been the highest K/9 rate for a starting pitcher of all time.\n9/26/2020 - NY Times, paraphrased: A majority of voters favor the open Supreme Court seat being filled after the election, our poll found.\n9/15/20 - Twitter:\n\n9/12/20 - Lakers vs.¬†Rockets Broadcast on ESPN: The Lakers are 31-1 in series once they go up 3-1.\n9/6/20: As of the end of the Orioles game on 9/5/2020, DJ Stewart had a batting average of .105 and a slugging percentage of .421.\n8/8/20 - Nate Silver: The whole conversation between Silver and Morris.\n\n7/30/20 - NY Times, paraphrased: U.S. economic output fell by 9% last quarter. That translates to a 36% annual rate of decline.\n7/23/20 - ESPN Opening Day Broadcast of Yankees-Nationals: Max Scherzer is the first pitcher since 1893 to strike out ten or more batters in three consecutive opening day starts.\n7/13/20 - The Economist: As of today, July 13, 2020, The Economist‚Äôs presidential prediction (co-signed by Gelman) gives Biden a 91% chance of winning the election.\n\n\n\n7/9/20 - This paper on activity and social distancing / quarantine (Hint: look at the N sizes).\n6/11/20 - r/DataIsBeautiful:\n\nAdult Obesity Rate in America 2.0 [OC] by u/isu_asenjo in dataisbeautiful\n\n\n6/11/20 - NPR Up First Podcast: ‚ÄúJoe Biden has substantial leads in a wave of new polls, and many of them are ten point leads. And as to the ‚ÄòWhat about 2016?‚Äô response that I can already hear listeners saying into my ears, here‚Äôs the big difference: Joe Biden is over 50% in a lot of those polls, and that is a mark that Hillary Clinton very rarely got to, that makes the lead a lot more durable.‚Äù The ‚ÄúWhat about 2016‚Äù critique being referenced is the fact that many news outlets (Huffington Post, NY Times, etc.) gave Clinton greater than a 95% chance of winning the election (FiveThirtyEight gave her around a 65-70% chance, at best). Obviously, she ended up losing, sparking outrage towards and confusion over those seemingly overly-optimistic projections. As of today, June 11th, 2020, PredictIt gives Biden roughly a 55% chance of winning.\n6/6/20 - CNBC: A V-shaped recovery!\n\n5/14/20 - FiveThirtyEight Politics Podcast: When asked who they voted for in an election, a larger share of people respond that they had voted for the winning candidate than the share that actually voted for said candidate.\n5/5/20 - CEA: (oh my god, people are going to die because of this)\n\n4/2/20 - FiveThirtyEight: This FiveThirtyEight article.\n4/2/20 - The New York Times: These maps.\n3/15/20 - FiveThirtyEight: Teams with Cristiano Ronaldo on them are 5-0 against Atletico Madrid in Champions League knockout stage games in the last 10 years. Everyone else is 0-9.\n1/13/20 - CBS Broadcast of Titans - Chiefs: Right before the Chiefs are about to go for it on 4th and 2 from the Titans‚Äô 27‚Ä¶ Other teams have gone for it on fourth down five times against the Titans defense this postseason. All five have been stops. Side note: the Chiefs got the first.\n1/13/20 - ESPN Broadcast of LSU - Clemson: Travis Etienne has never scored a touchdown in a dome.\n1/13/20 - ESPN Broadcast of LSU - Clemson: Clemson has won 50 straight games when scoring first.\n1/13/20 - ESPN Broadcast of LSU - Clemson: [Sophomore] Trevor Lawrence has never thrown an interception in a postseason game.\n12/28/19 - ESPN Broadcast of LSU - Oklahoma: Joe Burrow gets first downs on 47% of his rushing attempts.\n12/16/19 - FiveThirtyEight: The Lakers average 112.9 points per 100 possessions with both LeBron and Anthony Davis on the floor. They average 114.8 with only LeBron.\n8/13/19 - MLB Network: Discussing Gleyber Torres‚Äôs immense success against Orioles pitching‚Ä¶ ‚ÄúIf it were up to Gleyber Torres, the Yankees would play the Orioles every day of the week and twice on Sunday.‚Äù\n7/15/19 - MLB Network: The night before, the Rays had a team no-hitter broken up by a single through the shift in the 9th inning. The hit went to where the second baseman would normally be positioned. A misquote, but the general gist‚Ä¶ ‚ÄúThe craziest part about this single was that its hit probability was only 16%.‚Äù\n7/12/19 - MLB Network: Only one team in the Majors has not intentionally walked anyone during the 2019 season. Houston.\n7/9/19 - MLB Network: ‚ÄúGleyber Torres is only the third ever Yankee to make two All Star games before the age of 23. The other two are Mickey Mantle and Joe DiMaggio.‚Äù\nEarly 2019 - Effectively Wild Podcast, paraphrased: Mike Trout has amassed about 65 WAR throughout his career, and is projected to have another 70 or so over the next ten seasons. Essentially, this means he has already had one Hall of Fame career, and is about to have another.\nPre-2019 Season: Only 10 players have ever had 5+ seasons of 9+ WAR. They are: Babe Ruth, Rogers Hornsby, Barry Bonds, Willie Mays, Alex Rodriguez, Ty Cobb, Lou Gehrig, Honus Wagner, Ted Williams, and Mike Trout."
  }
]