[
  {
    "objectID": "my-three-favorite.html",
    "href": "my-three-favorite.html",
    "title": "My Three Favorite, Alphabetically",
    "section": "",
    "text": "ALGORITHMS: Bogo Sort, Logistic Regression, Metropolis-Hastings\nARTISTS (Musical, According to Spotify): Bad Bunny, Imagine Dragons, Taylor Swift\nAUTHORS: Fredrik Backman, Khaled Hosseini, Sally Rooney\nBEERS: Heady Topper, King Sue, Very Green\nBOOKS (Fiction): The Kite Runner, Neverwhere, A Storm of Swords\nBOOKS (Non-Fiction): Algorithms to Live By, The Signal and the Noise, Factfulness\nCITIES (American): Bozeman, Cambridge, New York City\nCITIES (European): Barcelona, Budapest, Stockholm\nCITIES (Other): Medellín, Oaxaca, Vancouver\nCOLLEGE COURSES: Advanced Algorithms, Mathematical Structures, Price Theory\nCONDIMENTS: Calabrian Chiles, Preserved Lemons, Secret Aardvark Hot Sauce\nCUISINES (Couldn’t pick three): Italian, Japanese, Lebanese, Mexican\nDATA STRUCTURES: Graph, Skip List, Tibble\nECONOMIC SUBFIELDS: Labor Economics, Monetary Theory, Urban Economics\nFICTIONAL CHARACTERS: Elaine Benes, Oberyn Martell, Samwise Gamgee\nFOODS: Bananas, Maitake Mushrooms, Peanut Butter\nKITCHEN UTENSILS: 7” Santoku Knife, Tasting Spoon, Tongs\nMOVIES: Birdman, Inside Out, Moonrise Kingdom\nNON-CITY PLACES: Acadia National Park, Baseball Hall of Fame, Little Cottonwood Canyon\nPARADIGMS: Bayesian, Behavioral, Functional\nPIECES OF COOKWARE: 3-Quart Saute Pan, Cast Iron Skillet, Dutch Oven\nR PACKAGES: brms, dplyr, purrr\nSERIOUSEATS RECIPES: Halal Cart Chicken, Pozole Verde with Chicken, Red Wine-Braised Short Ribs\nSKI MOUNTAINS: Alta, Big Sky, Jackson Hole\nSPIRITS: Fernet, Islay Scotch, Mezcal\nSPORTS TO WATCH: Baseball, College Football, Soccer\nSUBREDDITS: AdvancedRunning, MaleLivingSpace, UnexpectedFactorial\nTV SHOWS: Atlanta, Chef’s Table, Seinfeld\nWEBSITES: Fangraphs, FiveThirtyEight, Serious Eats"
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-13-experiment-tracking/experiment-tracking.html",
    "href": "posts/series/doing-data-science/2023-05-13-experiment-tracking/experiment-tracking.html",
    "title": "Experiment Tracking and Model Versioning",
    "section": "",
    "text": "This post is part of a series called The Missing Semester of Your DS Education."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-13-experiment-tracking/experiment-tracking.html#introduction",
    "href": "posts/series/doing-data-science/2023-05-13-experiment-tracking/experiment-tracking.html#introduction",
    "title": "Experiment Tracking and Model Versioning",
    "section": "Introduction",
    "text": "Introduction\nI’ve started the past few posts by saying things along the lines of “When I first started my current job…” or “It wasn’t until my second data science job…” and going on about how it took me too long to learn something important. But not today! Today, I’ll tell you a story. But first, some background context.\nLet’s imagine that you work for a company whose website provides college guidance to high school students. You might have a page where a student can input their profile – maybe their academic information like their GPA or their SAT scores, some demographic information, or a list of their extracurricular activities – and then a second page where they can view information about different schools they might be interested in. A killer feature of that second page is the student being able to see their chances of being accepted at different schools based on the information in their profile. At CollegeVine we call this “Chancing,” and it’s a core part of our student experience.\nWhen we first started using machine learning for Chancing, we had a number of architectural questions to answer. Of course, there were questions about how the model would be deployed, how it would serve predictions to users, if we could cache predictions, how fast the code had to be, and so on. But this post will focus on the problem that, at first glance, I thought was the simplest: What version of the model is running in production? Over time, we’ve taken a few approaches to solving this problem. And so begins the story."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-13-experiment-tracking/experiment-tracking.html#model-versioning",
    "href": "posts/series/doing-data-science/2023-05-13-experiment-tracking/experiment-tracking.html#model-versioning",
    "title": "Experiment Tracking and Model Versioning",
    "section": "Model Versioning",
    "text": "Model Versioning\nAt first, we used an incrementing counter as the version of our chancing model, so we would identify different versions as v1, v2, and so on. Our assumption initially was that what was most important was being able to update a model (i.e. increment the version) and roll back to a previous model (i.e. decrement), and this versioning scheme allowed us to do that easily. But we quickly ran into some headaches. Notably, we’d find ourselves asking “Okay, Chancing v5 is in production. But when was it trained? And when did we ship it?” And so we soon outgrew our initial approach.\nAfter our first approach failed, our next idea was to use a timestamp like 20230514123456 (12:34 and 56 seconds on May 14th, 2023) as the model version. The timestamp would correspond to when the model was trained – we were assuming we’d ship right after training – and would still have all of the same properties as the incrementing counter from before (letting us upgrade and roll back easily) while also encoding extra information. We viewed this change as a Pareto Improvement.\n\n\n\n\n\n\nIt’s important to mention at this point that all of our modeling code is checked into Git. This means that we can, roughly speaking, see what the state of the world was at the time the model was trained.\n\n\n\nSo now that we’re able to not only upgrade and downgrade model versions quickly, but also know when the model was trained, it’s problem solved. Right?\nWrong. As it turns out, there was another pressing issue. Finding the state of the code when the model was trained was actually non-trivial, because, as I wrote in my previous post on internal libraries, our modeling code was living in a library. But our retraining job was loading a specific version of that library, which meant that we’d need to look through our Git history to find the state of the retraining job when the model was trained, and then find the version of the library the code was using, and then dig through our Git history again to uncover the logic in the library (ultimately the logic training the model) at the time. This is certainly possible, but it’s not trivial.\nAnd so with that, we tried our next idea: Appending the library version to the timestamp, to get a model version like 20230514123456-1-2-3, which would correspond to the model being trained at 12:34 and 56 seconds on May 14th, 2023, using library version 1.2.3. This was another Pareto Improvement: Now we could upgrade and downgrade, we knew when the model was trained, and we knew which library version the model was trained on. Amazing!\nBut at this point, I presume you’re starting to realize that this approach didn’t work either. And so were we. This was when we began to look for an off-the-shelf tool for this deceivingly challenging problem."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-13-experiment-tracking/experiment-tracking.html#enter-mlflow",
    "href": "posts/series/doing-data-science/2023-05-13-experiment-tracking/experiment-tracking.html#enter-mlflow",
    "title": "Experiment Tracking and Model Versioning",
    "section": "Enter MLFlow",
    "text": "Enter MLFlow\nAs it turns out, we’re not the only ones to have this problem. At the highest level, we needed to figure out a way to version our models such that it was easy to determine what actually went into them – the data, the hyperparameters we evaluated and which we selected, the features, and the rest of the code itself. We also needed a way to differentiate one model version from another. In particular, what made two different training runs of the same model different? Did the data change? Did the features change? Lastly, we wanted to be able to track a few other important aspects of the ML lifecycle:\n\nWhen the model was in production from and to.\nAny other parameters to the model, in addition to the hyperparameters, that were used on that training run.\nModel diagnostics, like AUC or the Brier Skill Score, calibration plots, and miscellaneous sanity checks.\nThe data the model was trained on.\n\nTo accomplish all of these goals and more, we turned to MLFlow. MLFlow lets us catalog whatever we want about our model training runs. We save artifacts (a fancy word for files) to MLFlow containing our training data, the models we fit, the results of hyperparameter tuning, a bunch of diagnostic plots, test cases we dynamically generate, and so on. In addition, we log parameters like what high school graduation years we included in our data, or what version of our R package – mlchancing – was used in training the model. We log lots of metrics like AUC, the Brier Skill Score, and so on to keep track of the performance of the model at training time. We also log metrics like the mean and standard deviation of each feature in our model at training time, so we can evaluate data drift over time.\nIn addition to tracking models, metrics, artifacts, and so on, MLFlow also lets us create a new model version for a training run. When we create a model version, we can mark that version as either having no status, being in staging, being in production, or being archived. These statuses let us track what model is in production at any given time, and the model versions link back to the run that the model was trained on, so we can see all of the metrics, artifacts, etc. listed above by just clicking into the training run for any model version in the MLFlow UI.\nLastly, MLFlow lets us log a Git commit as an attribute of the training run, which means we can click directly from the MLFlow UI to the state of our modeling code in GitHub at the time that our model was trained, which lets us more easily track down exactly what the state of the code was at the time.\nAnd here concludes my story. Since adopting MLFlow, our model versioning headaches have more or less subsided. We’ve been running MLFlow in production for about a year and a half now, and it’s made running experiments on our models, tracking training runs, comparing metrics across different ideas we have, and keeping tabs on what’s in production at any given point simple. It’s not a perfect tool by any means, but it’s solved most of our most painful problems."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-13-experiment-tracking/experiment-tracking.html#experiment-tracking-model-registries-and-mlops",
    "href": "posts/series/doing-data-science/2023-05-13-experiment-tracking/experiment-tracking.html#experiment-tracking-model-registries-and-mlops",
    "title": "Experiment Tracking and Model Versioning",
    "section": "Experiment Tracking, Model Registries, and MLOps",
    "text": "Experiment Tracking, Model Registries, and MLOps\nOf course, MLFlow isn’t the only tool out there. Broadly speaking, tools like MLFlow are often referred to as “experiment tracking” tools. Others in this category include Weights & Biases, Neptune, and Comet. Experiment tracking tools let you version different experiments you run, and store metadata like the training data, parameters, etc. on those experiments and training runs. These tools generally also provide a “model registry,” which tends to be the component that handles the versioning of models.\n\n\n\n\n\n\nAs an important aside: There’s a whole world of tools like these out there that make up the field of MLOps. Over the past few years, MLOps has been exploding as more and more companies face pains like ours when it comes to putting models into production. These pain points include everything from versioning to experimenting to deployment, so it’s been very exciting to see all of the awesome new tooling that’s being introduced every week.\nThis also means that this post will likely become stale quickly."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-13-experiment-tracking/experiment-tracking.html#wrapping-up",
    "href": "posts/series/doing-data-science/2023-05-13-experiment-tracking/experiment-tracking.html#wrapping-up",
    "title": "Experiment Tracking and Model Versioning",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nI’d like to wrap this post up with an important lesson I learned from our model versioning saga: If you’re having a technical problem like model versioning that seems simple but is proving to be difficult, it’s often a good idea to see what others are doing to solve that problem. Nowadays, I’d realize that model versioning must be a common problem, and think to myself “Someone must have solved this problem. What do they do?” After all, every company providing any kind of machine learning product must have model versioning issues.\nSo in hindsight, we could’ve come across the world of MLOps far sooner had we taken a step back to consider the fact that we must not be the only ones solving this problem. But we didn’t do that, and versioning became a thorn in our side instead. Hopefully our mistakes will help you take the step back that I wish I had.\nHappy experimenting!"
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-01-library-code/library-code.html",
    "href": "posts/series/doing-data-science/2023-04-01-library-code/library-code.html",
    "title": "Writing Internal Libraries for Analytics Work",
    "section": "",
    "text": "This post is part of a series called The Missing Semester of Your DS Education."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-01-library-code/library-code.html#introduction",
    "href": "posts/series/doing-data-science/2023-04-01-library-code/library-code.html#introduction",
    "title": "Writing Internal Libraries for Analytics Work",
    "section": "Introduction",
    "text": "Introduction\nBy definition, library code is code that’s written to being reused by programs other than itself that are unrelated to each other. For instance, dplyr (R) and pandas (Python) are common examples of library code: Instead of writing code from scratch to work with tabular data, you might use one of those two fantastic libraries. And you get some additional benefits from using them:\n\nThose libraries are well-documented, so it’s easy to figure out how to use them.\nThey’re well-tested, so you (presumably) know that bugs are less likely than if you were to try to write the same functionality from scratch.\nThey’re (relatively) performant.\nThey’re widely used, so it’s easy to find answers to questions and get help from the communities using them."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-01-library-code/library-code.html#a-common-library",
    "href": "posts/series/doing-data-science/2023-04-01-library-code/library-code.html#a-common-library",
    "title": "Writing Internal Libraries for Analytics Work",
    "section": "A Common Library",
    "text": "A Common Library\nAt CollegeVine, we have collegeviner: An R package containing a lot of code that we use very often for all kinds of analytics projects. Some things that live in collegeviner include:\n\nPlot theming code, so that we can consistently theme graphics across all of our work.\nA custom implementation of item-based collaborative filtering, which is core to our school recommendation system.\nDBI and aws.s3 wrappers for connecting to and querying our databases and working with data in S3.\nHelper methods for common math we do, such as converting between odds, log-odds, and probabilities.\nMiscellaneous helper code for things that don’t exist natively in R, such as yeet for removing an item from a list and %notin%, the inverse of the %in% operator.\nAn implementation of the Brier Skill Score, which is a metric we often use for evaluating classification models.\nA lot more!\n\nYou might think of collegeviner as being a common library of things that our team does often, so we don’t need to repeat ourselves or reinvent the wheel.\n\nA Toy Example\nLet’s imagine that you’re setting up a common library (in this example, an R package) for your team. The first thing you might want to do is have some logic to help your team connect to your data warehouse. For this example, let’s just imagine that “warehouse” is your local Postgres instance. Then, you might write a method for your library called connect_to_dwh like this:\n\nlibrary(DBI)\nlibrary(rlang)\nlibrary(httr)\nlibrary(RPostgres)\n\nconnect_to_dwh &lt;- function(url = Sys.getenv(\"DATA_WAREHOUSE_URL\")) {\n  if (url == \"\") abort(\"You must specify a dwh URL.\")\n\n  check_installed(pkg = \"httr\")\n  check_installed(pkg = \"RPostgres\")\n\n  db_params &lt;- parse_url(url)\n\n  db_drv &lt;- Postgres()\n  db_user &lt;- db_params$username\n  db_password &lt;- db_params$password\n  db_host &lt;- db_params$hostname\n  db_port &lt;- db_params$port %||% 5432\n  db_name &lt;- db_params$path\n\n  dbConnect(\n    db_drv,\n    dbname = db_name,\n    host = db_host,\n    port = db_port,\n    user = db_user,\n    password = db_password\n  )\n}\n\nNow you have a single function that your whole team can share to connect to your data warehouse, assuming that they can provide the connection string. Let’s test out how the workflow might look for querying data now.\n\nConnecting\n\nSys.setenv(DATA_WAREHOUSE_URL = \"postgresql://localhost:5432/postgres\")\n\nconn &lt;- connect_to_dwh()\n\nAnd that’s it! You’re connected. You can now query away using dbGetQuery(), a custom wrapper, dbplyr, or any other method of choice.\n\n\nQuerying\n\n## Put some data into the warehouse for example purposes\ndbWriteTable(conn, Id(schema = \"blog\", table = \"iris\"), janitor::clean_names(iris))\n\nresult &lt;- dbGetQuery(\n  conn = conn,\n  \"\n  SELECT species, MAX(sepal_length) AS max_sepal_length\n  FROM blog.iris\n  GROUP BY species\n  ORDER BY 2 DESC\n  \"\n)\n\npretty_print(result)\n\n\n\n\nspecies\nmax_sepal_length\n\n\n\n\nvirginica\n7.9\n\n\nversicolor\n7.0\n\n\nsetosa\n5.8\n\n\n\n\n\n\n\n\n\nTesting\nIt’s also important to test your code. testthat makes writing unit tests for your new connect_to_dwh function very simple.\n\nlibrary(testthat)\n\ntest_that(\"Connecting works as expected\", {\n  ## This should error because the URL is empty\n  expect_error(\n    connect_to_dwh(\"\"),\n    \"You must specify a dwh URL\"\n  )\n  \n  conn &lt;- connect_to_dwh()\n\n  ## Should return a PqConnection object\n  expect_s4_class(conn, \"PqConnection\")\n  \n  ## Should be able to query an example table\n  expect_equal(\n    dbGetQuery(conn, \"SELECT COUNT(*) FROM blog.iris\")$count,\n    150\n  )\n})\n\nTest passed 😀\n\n\n\n\nVersioning\nLastly, it’s important to version your code. Semantic Versioning (SemVer) is a very common standard for versioning library code. In R specifically, you can read about versioning in Chapter 22 of R Packages.\nIn our toy example, this means that if you change how the logic of your connect_to_dwh function works, you should change the version of your package so that your users (your teammates) don’t get blindsided by your change. Incrementing your package’s version shows your teammates that something has changed in your library, and they can update their code to rely on the latest version (if they wish), or continue using the current version they’re on, or anything else.\n\n\n\n\n\n\nNote that being able to control which version of a library your code is using requires some manner of managing dependencies. In R, I would highly recommend renv. In Python, I like Poetry."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-01-library-code/library-code.html#one-library-per-model",
    "href": "posts/series/doing-data-science/2023-04-01-library-code/library-code.html#one-library-per-model",
    "title": "Writing Internal Libraries for Analytics Work",
    "section": "One Library Per Model",
    "text": "One Library Per Model\nIn addition to a common library for sharing code that’s very often used across the data org, our team has also gotten into the habit of having a library per ML model in production. This definition can be a bit flexible (both in terms of what “ML model” means, and also what “production” means), but the basic principle should be the same: ML in production requires at least some training logic and some monitoring logic. It’s a good idea to share code between those two things. Let’s consider another simple example.\n\nIris Species Prediction\nLet’s imagine that we work for a florist. On our website, we have a service where someone can provide some measurements about an iris (either a setosa or a virginica), as we’ll tell them which of the two we think it is. We know we’ll want to retrain the model periodically as we get more data, and we’ll also want to monitor how our model performs out-of-sample. Both training the model and monitoring will require some shared logic: loading raw data, doing feature engineering, and making predictions. So it would make sense to have those two jobs rely on a single library, as opposed to needing to repeat the logic. Let’s write that library here.\n\nFetching the Data\nFirst, let’s write a method to fetch the raw data from our data warehouse. In practice, it probably makes sense to factor out the SQL here into individual SQL scripts, but for this example I’ll just include the SQL directly as a string.\n\nfetch_raw_data &lt;- function(conn) {\n  dbGetQuery(\n    conn = conn,\n    \"\n    SELECT *\n    FROM blog.iris\n    WHERE species IN ('setosa', 'virginica')\n    \"\n  )\n}\n\n\n\nFeature Engineering\nNext, let’s write some methods to create features.\n\ncreate_sepal_length_feature &lt;- function(sepal_length) {\n  sepal_length + rnorm(n = length(sepal_length))\n}\n\ncreate_petal_width_feature &lt;- function(petal_width) {\n  petal_width + rgamma(n = length(petal_width), shape = 2)\n}\n\nAnd then we can write a function to take our raw data, run the feature engineering steps, and return the features.\n\ncreate_features &lt;- function(raw_data) {\n  sepal_length &lt;- create_sepal_length_feature(raw_data$sepal_length)\n  petal_width &lt;- create_petal_width_feature(raw_data$petal_width)\n  is_setosa &lt;- raw_data$species == \"setosa\"\n  \n  data.frame(\n    sepal_length,\n    petal_width,\n    is_setosa\n  )\n}\n\n\n\nModel Fitting and Prediction\nNext, let’s write methods to fit the model and make predictions.\n\nfit_model &lt;- function(features) {\n  formula &lt;- is_setosa ~ sepal_length + petal_width\n\n  ## Dynamically extract the variables in the formula\n  ## so we don't need to repeat ourselves\n  predictors &lt;- as.character(labels(terms(formula)))\n  target &lt;- as.character(formula[[2]])\n  \n  ## Very basic error handling\n  if (!all(c(predictors, target) %in% colnames(features))) {\n    abort(\"Some required columns were missing from `features`\")\n  }\n  \n  model &lt;- glm(\n    formula,\n    data = features,\n    family = binomial\n  )\n  \n  class(model) &lt;- c(\"irises\", class(model))\n  \n  model\n}\n\npredict.irises &lt;- function(object, newdata = NULL, ...) {\n  probs &lt;- predict.glm(\n    object,\n    newdata,\n    type = \"response\"\n  )\n  \n  ## If the predicted probability is &gt; 50%,\n  ## return `true`, else return `false`\n  probs &gt; 0.50\n}\n\n\n\nModel Evaluation\nAnd finally, let’s add a function to compute the model’s accuracy on some evaluation data.\n\ncompute_accuracy &lt;- function(prediction, is_setosa) {\n  100 * sum(prediction == is_setosa) / length(prediction)\n}\n\n\n\nTesting\nIt’s important to note that all of the methods above can and should be unit tested in the same way we tested our helper for connecting to the database. Testing is a great way to ensure the correctness of your code and make it more maintainable by making it easier to refactor in the future, and putting all of your modeling logic into a library like this makes it very easy to test. For instance, here’s how you might write a couple of unit tests for the petal width feature.\n\ntest_that(\"Petal width feature is created correctly\", {\n  ## The feature should be positive even when the\n  ## petal width is zero, since we're adding gamma\n  ## random noise.\n  expect_gt(create_petal_width_feature(0), 0)\n  \n  ## It should be extremely unlikely that a single \n  ## draw from a gamma(2) is &gt;10, which means this\n  ## feature should be &lt; 10 when the input is 0 in \n  ## the vast majority of cases.\n  ##\n  ## NOTE: This is by definition a brittle test, and\n  ## I wouldn't recommend writing tests that are\n  ## probabilistic like this in practice unless\n  ## you really need to. If you do, this will\n  ## fail _some_ of the time, at random, even\n  ## if \"some\" is a very small percentage.\n  purrr::walk(\n    rep(0, 100), \n    function(x) {\n      expect_lt(\n        create_petal_width_feature(x),\n        10\n      )\n    } \n  )\n})\n\nTest passed 🎊\n\n\n\n\n\nA Retraining Job\nGreat! Now that we have all of that library code written, we can package it up into a retraining job. A very simple training job might look like this:\nFirst, connect to the data warehouse\n\nconn &lt;- connect_to_dwh()\n\nNext, fetch the raw data from the warehouse that we need to train the model.\n\nraw &lt;- fetch_raw_data(conn)\n\npretty_print(head(raw))\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n\n\n\n\n\nNext, create the features from the raw data by running the feature engineering pipeline.\n\nfeatures &lt;- create_features(raw)\n\npretty_print(head(features))\n\n\n\n\nsepal_length\npetal_width\nis_setosa\n\n\n\n\n6.937\n1.570\nTRUE\n\n\n3.884\n2.792\nTRUE\n\n\n4.667\n2.152\nTRUE\n\n\n4.527\n3.242\nTRUE\n\n\n6.942\n1.555\nTRUE\n\n\n6.269\n3.101\nTRUE\n\n\n\n\n\n\n\nThen fit a model using the features.\n\nmodel &lt;- fit_model(features)\n\npretty_print(coef(model))\n\n\n\n\n\nx\n\n\n\n\n(Intercept)\n9.830\n\n\nsepal_length\n-1.064\n\n\npetal_width\n-1.240\n\n\n\n\n\n\n\nFinally, evaluate the performance of the model by making predictions and computing the accuracy of those predictions.\n\npreds &lt;- predict(model)\naccuracy &lt;- compute_accuracy(preds, features$is_setosa)\n\ncat(paste0(\"Model accuracy is \", accuracy, \"%\"))\n\nModel accuracy is 86%\n\n\nAnd that’s it – you have a simple retraining job. This is a very minimal example, but this general framework is very flexible and modular, and it makes up the foundation for how we write our retraining jobs at CollegeVine. You can plug and play all different kinds of feature engineering logic, logic to fetch raw data, metrics, etc. We also use MLFlow for versioning models and tracking experiments, so our retraining jobs have lots of logging of artifacts, parameters, and metrics to our MLFlow instance.\n\n\nA Monitoring Job\nNext, let’s imagine we want to monitor out-of-sample performance of the model. Let’s modify the table with our raw data in the database for this, just for the sake of an example.\n\ntransaction_result &lt;- dbWithTransaction(\n  conn = conn,\n  {\n    dbExecute(\n      conn = conn,\n      \"\n      ALTER TABLE blog.iris\n      ADD COLUMN created_at TIMESTAMP\n      \"\n    )\n    \n    dbExecute(\n      conn = conn,\n      \"\n      UPDATE blog.iris\n      SET created_at = NOW() - random() * INTERVAL '2 weeks'\n      \"\n    )\n  }\n)\n\nGreat, and now let’s make one or two small modifications to our code from above that pulled the raw data from the data warehouse.\n\nfetch_raw_data &lt;- function(conn, created_after = '1970-01-01 00:00:00') {\n  dbGetQuery(\n    conn = conn,\n    sprintf(\n      \"\n      SELECT *\n      FROM blog.iris\n      WHERE \n        species IN ('setosa', 'virginica')\n        AND created_at &gt; '%s'\n      \",\n      created_after\n    )\n  )\n}\n\nAll we’ve done here is added the ability to specify a created_at date to use as the cutoff point, where we’d only include records in our raw data that were created after said point. In practice, this lets us filter our raw data down to only records that were created after the model was trained (the out-of-sample data).\n\n## In practice, this would be set at training time and \"frozen\"\n## possibly by logging the value as a parameter in the MLFlow run\nmodel_trained_at &lt;- median(fetch_raw_data(conn)$created_at)\n\nAnd now that we’ve artificially created a trained_at date for the model, we can run our monitoring job. It’s quite simple, and very similar to the retraining job. All we do here is pull raw data that has been created since the model was trained, run the feature engineering pipeline, make predictions, and compute the accuracy of the model out-of-sample.\n\nraw &lt;- fetch_raw_data(conn, created_after = model_trained_at)\n\nfeatures &lt;- create_features(raw)\npredictions &lt;- predict(model, features)\n\naccuracy &lt;- compute_accuracy(predictions, features$is_setosa)\n\ncat(paste0(\"Out-of-sample accuracy is \", accuracy, \"%\"))\n\nOut-of-sample accuracy is 92%"
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-01-library-code/library-code.html#tying-it-together",
    "href": "posts/series/doing-data-science/2023-04-01-library-code/library-code.html#tying-it-together",
    "title": "Writing Internal Libraries for Analytics Work",
    "section": "Tying it Together",
    "text": "Tying it Together\nThe key piece to notice is how much we’re leveraging our library code in both the retraining and monitoring job. In both cases, we’re doing some very similar things – pulling raw data, creating features, making predictions, computing accuracy – so it makes a lot of sense that we’d want to reuse the code for those two jobs.\nThere might also be more use cases for the code: More retraining or monitoring jobs, REST APIs, ETL jobs, etc. The more times you need to rely on the same logic, the more benefit you’ll derive from having a single source of truth for all of the logic for your modeling process.\nIt also might be useful to separate this library from the common library proposed at the start. There are important tradeoffs to consider: On one hand, a single library might be convenient for having all of your logic in a single place. But on the other hand, as your library grows in scope, it’ll necessarily have a bigger footprint, rely on more dependencies, etc. which will make its use and maintenance more difficult. A happy middle ground for us has been having a single library per “model” or use case. For instance, at CollegeVine we have a package called mlchancing for our chancing model and a separate package called schoolrecommendr for school recommendations and affinity scoring. Keeping these separate has made it easier to iterate on each model individually while also not being a maintenance or ramp-up headache.\nIt’s my view that models and other analytics work that is in production is software, and should be treated as such. If a model is going to be shipped to production, it at the very least needs to be tested, documented, versioned, and put through some kind of CI/CD process. It’d be even better if it’s monitored automatically so that the data scientists working on it can be notified quickly if things start going wrong. Ultimately, writing library code for your modeling work is very well-suited to meeting all of these expectations. And it also just makes everyone’s lives easier by not needing to reinvent the wheel all the time."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html",
    "href": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html",
    "title": "Unit Testing Analytics Code",
    "section": "",
    "text": "This post is part of a series called The Missing Semester of Your DS Education."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#introduction",
    "href": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#introduction",
    "title": "Unit Testing Analytics Code",
    "section": "Introduction",
    "text": "Introduction\nUnit testing was a concept I had never even heard of before I started my second data science job. It never came up in any of my college statistics or computer science courses. It never came up in any of my data science internships. It never came up in my first data science job.\nIn conversations I have with friends – and, broadly, conversations with basically anyone doing data science or analytics – I face lots of pushback when it comes to unit testing. Usually the objections come in the form of either not knowing why you might test, since the code is just so simple and straightforward that nothing could go wrong, or not understanding the value added. In my opinion, both of these objections come from the same place. At first glance, it seems like some combination of blissful ignorance about what could go wrong and overconfidence in one’s own ability or in their code’s correctness, but I think that the objections actually come from something deeper. In my opinion, it’s the unfamiliarity of testing. It’s not something that’s commonly taught to people involved in analytics, and so it feels new. That can be scary."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#whats-a-unit-test",
    "href": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#whats-a-unit-test",
    "title": "Unit Testing Analytics Code",
    "section": "What’s A Unit Test?",
    "text": "What’s A Unit Test?\nFirst thing’s first: What’s a unit test? It’s actually really simple! Unit testing tests the smallest possible components of your code for correctness and what I would define as “good behavior.” In analytics, you might test things like feature engineering steps, metric definitions, or data wrangling code. The basic idea of a unit test is that you take a function you’ve written, and you’d make up some inputs to the function and then check if your code produces the outputs you’d expect. In the simplest case, you might test the identity function as follows:\n\nidentity &lt;- function(x) {\n  x\n}\n\ntest_that(\"The identity returns the input\", {\n  expect_identical(1, identity(1))\n  expect_identical(\"foo\", identity(\"foo\"))\n  expect_identical(1:100, identity(1:100))\n  expect_identical(\n    lm(iris$Sepal.Length ~ iris$Sepal.Width), \n    identity(lm(iris$Sepal.Length ~ iris$Sepal.Width))\n  )\n})\n\nTest passed 😸\n\n\nAll we’re doing here is checking that for some given input, our function returns the expected (correct) output. You can also test that your function returns an error or warning, returns nothing, returns some output, and much more. I’d highly recommend looking at the documentation for testthat or pytest to get a sense for what and how to test."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#case-study-feature-engineering",
    "href": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#case-study-feature-engineering",
    "title": "Unit Testing Analytics Code",
    "section": "Case Study: Feature Engineering",
    "text": "Case Study: Feature Engineering\nData scientists very often write feature engineering code. And as it turns out, feature engineering is a very common place where bugs can pop up unbeknownst to the code’s author.\n\n\n\n\n\n\nIt’s important to note that data scientists are usually writing Python or R, which are both dynamically typed, interpreted languages. Unit testing is doubly valuable in these types of languages, since you don’t get the benefits of a compiler and static type checks to catch issues in your code. In languages like Python and R, anything can go wrong in your code. And you often won’t find out about issues until runtime, or, depending on the nature of the bug, even later (if ever).\n\n\n\n\nAn Example\nLet’s write some example code to create a simple feature to use in a hypothetical machine learning model downstream.\n\ncreate_feature &lt;- function(x) {\n  (x - mean(x)) / sd(x)\n}\n\nThis might look familiar: It’s a function that takes a variable x and standardizes it. Now I’ll generate some data to show how it works.\n\n## 1000 draws from a Normal(25, 5)\nraw &lt;- rnorm(1000, mean = 25, sd = 5)\n\nWe can plot a histogram of our data to show what it looks like:\n\nhist(raw, breaks = 20)\n\n\n\n\n\n\n\n\nNow, let’s run raw through our feature engineering function.\n\nstd &lt;- create_feature(raw)\n\nhist(std, breaks = 20)\n\n\n\n\n\n\n\n\nGreat! Now std, the new variable that we created from standardizing raw, looks like it follows a standard normal distribution.\n\n\nTrivial Examples, Unexpected Results\nNow that we’ve written a function to create a feature, let’s use that feature engineering step on a couple of real-world examples. Keep in mind that as data scientists, we’re often working with messy data – it could include missing values, outliers, data of an incorrect type, etc. and there are often very few guarantees about what our data will look like in practice. These next few examples show how things could go wrong (often very quietly) in our seemingly correct feature engineering step we wrote above.\n\nMissings\nLet’s do the simplest thing first: What happens when we have missing values in our data?\n\ncreate_feature(c(1, 2, 3, NA_real_))\n\n[1] NA NA NA NA\n\n\nIf you’re familiar with R, this should be expected. And you’re probably thinking to yourself that we just need to set na.rm = TRUE, and you’d be right! But this brings me to the first major point I’d like to make on how things can go wrong.\n\n\n\n\n\n\nWhen you’re writing your code, it’s easy to forget things like adding na.rm = TRUE when the data you’re working with doesn’t appear to need it. It’s probably not your default behavior to remember to always set the flag to tell R to remove NA values, since if you’re not working with any of them, why would you remember to do that? Expecting yourself to remember to do something like this is a recipe for very brittle, error-prone code.\n\n\n\nLet’s fix this bug the naïve way.\n\ncreate_feature &lt;- function(x) {\n  (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)\n}\n\nGreat, and now let’s test it.\n\ncreate_feature(c(1, 2, 3, NA_real_))\n\n[1] -1  0  1 NA\n\n\nLooks good. Now all of our non-missing values have been correctly transformed.\n\n\nZero Standard Deviation\nHow about another non-trivial bug: A standard deviation of zero. Consider a toy example: Predicting whether or not someone plays basketball.\nIn this example, let’s say we have both men and women in our training data, and we want to use the height of each person to predict whether that person plays basketball. It probably doesn’t make sense to standardize the heights without grouping, since we don’t want to end up with a distribution of heights where most of the women are below average and most of the men are above average. It’d be much smarter to standardize within group, which would then produce a measure of height relative to the “competition” in some sense. Let’s give that a shot on a trivial data set where every man is the same height.\n\nlibrary(tibble)\nlibrary(dplyr)\n\ndata &lt;- tibble(\n  player_id = 1:6,\n  sex = c(\"M\", \"M\", \"M\", \"F\", \"F\", \"F\"),\n  height = c(77, 77, 77, 70, 71, 72)\n)\n\ndata %&gt;%\n  group_by(sex) %&gt;%\n  mutate(\n    height_std = create_feature(height)\n  ) %&gt;%\n  ungroup() %&gt;%\n  pretty_print()\n\n\n\n\nplayer_id\nsex\nheight\nheight_std\n\n\n\n\n1\nM\n77\nNaN\n\n\n2\nM\n77\nNaN\n\n\n3\nM\n77\nNaN\n\n\n4\nF\n70\n-1\n\n\n5\nF\n71\n0\n\n\n6\nF\n72\n1\n\n\n\n\n\n\n\nSo what happened here? In the male group, all of the heights were the same. This resulted in a standard deviation of zero when we went to standardize our height variable, which meant dividing by zero. So we get NaN back – Not a Number.\nThis example feels trivial, but in practice it’s not. If your data had hundreds or thousands of groups, it’s not all that unlikely to have a situation like this. And note that R doesn’t throw any kind of warning or error, it just quietly returns you NaN when you’re expecting a real number.\n\n\n\nInfinity\nR also has a built in value for infinity: Inf. What happens if we try to build our feature when we had an Inf in our data?\n\ncreate_feature(c(1, 2, 3, Inf))\n\n[1] NaN NaN NaN NaN\n\n\nA bunch of NaN. This seems trivial too, but imagine the following example, which you very well might run into in the real world: Computing GDP per capita.\n\ndata &lt;- tibble(\n  gdp = c(1000, 2000, 3000),\n  population = c(0, 50, 100)\n)\n\ndata %&gt;%\n  mutate(\n    gdp_per_capita = gdp / population\n  ) %&gt;%\n  mutate(\n    gdp_per_capita_std = create_feature(gdp_per_capita)\n  ) %&gt;%\n  pretty_print()\n\n\n\n\ngdp\npopulation\ngdp_per_capita\ngdp_per_capita_std\n\n\n\n\n1000\n0\nInf\nNaN\n\n\n2000\n50\n40\nNaN\n\n\n3000\n100\n30\nNaN\n\n\n\n\n\n\n\nR doesn’t yell at you for dividing by zero unlike Python for instance, or virtually any other self-respecting programming language. This means that if you accidentally divide by zero somewhere in your data pipeline, you could very well end up with a bunch of NaN if you standardize. And working with real-world data means that dividing by zero happens fairly often, generally because of small issues in the data we’re working with.\n\nOne Row Per Group\nBack to grouping: It’s also pretty likely when working with real-world data that you might have a group with only one row. Let’s see what happens in that case.\n\ndata &lt;- tibble(\n  group = c(1, 2, 2, 3, 3),\n  value = 1:5\n)\n\ndata %&gt;%\n  group_by(group) %&gt;%\n  mutate(\n    value_std = create_feature(value)\n  ) %&gt;%\n  ungroup() %&gt;%\n  pretty_print()\n\n\n\n\ngroup\nvalue\nvalue_std\n\n\n\n\n1\n1\nNA\n\n\n2\n2\n-0.7071068\n\n\n2\n3\n0.7071068\n\n\n3\n4\n-0.7071068\n\n\n3\n5\n0.7071068\n\n\n\n\n\n\n\nOnce again, same issue. A single-row group returns NA, since the standard deviation of a single number isn’t defined.\n\n\n\nLessons\nWhat have we learned here?\nIn short, there are many ways for things to quietly go wrong in your data pipelines, especially in a language like R. Even with a function as simple as standardization, it’s easy to cook up all kinds of possible corner cases or other issues that would cause your function to return any number of unpredictable results. And when working with real-world data, some of these quirks are inevitable. It’s virtually impossible that you’d never run into any of the issues enumerated above when working with messy data for any amount of time. And these examples were about as simple as they could be. In the real world, analytical code is often far more complicated and data far messier than this, which compounds the likelihood of issues like these."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#enter-testing",
    "href": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#enter-testing",
    "title": "Unit Testing Analytics Code",
    "section": "Enter Testing",
    "text": "Enter Testing\nAnd with all of that: Back to testing. Testing is the only way to know that your code is actually doing what you think it’s doing, and writing tests is a great way to make guarantees about the ability of your code to handle some of these issues that we’ve discussed. Writing tests also lets you ensure that your implementation is correct, and it lets you refactor your code more easily by loudly alerting you if you’ve broken something, which should make you much more confident in the correctness of your implementation.\n\n\n\n\n\n\nIf you’re not familiar with it, this is a good point to introduce Test-Driven Development. It’s not something I would always recommend, but at least being familiar with it might make for a good starting point when it comes to learning how to think about writing tests for your code\n\n\n\n\nTesting Our Standardization Function\nAnd with that, let’s write some tests. Imagine that we knew that we wanted to write our feature engineering function to do standardization. We also know that we want to avoid some of the bugs that we introduced in the examples above. One possible option for handling those cases where our function will do something unexpected would be to throw an error if we get unexpected inputs. That’s what I’ll illustrate here – it’d let the user learn about the issue quickly and debug. There are many options for how to handle these issues though. You might also fall back on a default value when your code will return a NA or NaN, for instance. Now, on to the tests.\n\nlibrary(testthat)\n\ntest &lt;- function() {\n  test_that(\"Standardized variable is transformed correctly\", {\n    random_normal &lt;- rnorm(1000, 10, 5)\n    \n    ## Expect that the mean of the transformed data is within\n    ## 0.005 of zero\n    expect_lt(\n      abs(mean(create_feature(random_normal))),\n      0.005\n    )\n    \n    ## Expect that the stddev of the transformed data is within\n    ## 0.005 of 1\n    expect_lt(\n      abs(1 - sd(create_feature(random_normal))),\n      0.005\n    )\n    \n    ## Expect that the math is done correctly\n    expect_identical(\n      create_feature(c(1,2,3)),\n      c(-1, 0, 1)\n    )\n  })\n  \n  test_that(\"Inifinity causes an error\", {\n    expect_error(\n      create_feature(c(1, 2, 3 / 0)),\n      \"`x` must not contain any infinite values\"\n    )\n  })\n  \n  test_that(\"Zero stddev causes an error\", {\n    expect_error(\n      create_feature(c(1, 1, 1)),\n      \"`x` must have a non-zero standard deviation\"\n    )\n  })\n  \n  test_that(\"Length one causes an error\", {\n    expect_error(\n      create_feature(c(1)),\n      \"`x` must have more than one unique element\"\n    )\n  })\n}\n\nAnd now let’s run our test suite.\n\ntest()\n\nTest passed 🥳\n── Failure (&lt;text&gt;:29:5): Inifinity causes an error ────────────────────────────\n`create_feature(c(1, 2, 3/0))` did not throw an error.\n\n\nError in `reporter$stop_if_needed()`:\n! Test failed\n\n\nUnsurprisingly, we get some failures. So now let’s refactor our feature engineering function to pass our tests.\n\nlibrary(rlang)\n\ncreate_feature &lt;- function(x) {\n  \n  mu &lt;- mean(x, na.rm = TRUE)\n  sigma &lt;- sd(x, na.rm = TRUE)\n\n  if (isTRUE(sigma == 0)) abort(\"`x` must have a non-zero standard deviation.\")\n  if (isTRUE(length(unique(x)) == 1L)) abort(\"`x` must have more than one unique element.\")\n  if (isTRUE(any(is.nan(x)))) abort(\"`x` must not contain any `NaN` values.\")\n  if (isTRUE(any(is.infinite(x)))) abort(\"`x` must not contain any infinite values.\")\n\n  (x - mu) / sigma\n}\n\ntest()\n\nTest passed 🥳\nTest passed 🥳\nTest passed 🥳\nTest passed 🥳\n\n\nWoohoo! Our tests all passed. Now, if the user of our function tries to do something like only providing a single value for x, they get an informative error back.\n\ncreate_feature(c(1))\n\nError in `create_feature()`:\n! `x` must have more than one unique element.\n\n\nIn a less trivial example, consider our grouped computation from before:\n\ndata &lt;- tibble(\n  group = c(1, 2, 2, 3, 3),\n  value = 1:5\n)\n\ndata %&gt;%\n  group_by(group) %&gt;%\n  mutate(\n    value_std = create_feature(value)\n  ) %&gt;%\n  ungroup() %&gt;%\n  pretty_print()\n\nError in `mutate()`:\nℹ In argument: `value_std = create_feature(value)`.\nℹ In group 1: `group = 1`.\nCaused by error in `create_feature()`:\n! `x` must have more than one unique element.\n\n\nNot only do we get a helpful error, but in this particular case dplyr also adds helpful context: It tells us which step failed and which group it failed on so that we can effectively debug."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#refactoring",
    "href": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#refactoring",
    "title": "Unit Testing Analytics Code",
    "section": "Refactoring",
    "text": "Refactoring\nNow that we have a test suite, we can also refactor our function and be much more confident that we haven’t broken anything. Let’s do an overly complicated refactor.\n\ncreate_feature &lt;- function(x) {\n  \n  ## Compute the mean as the sum of the non-null elements \n  ## divided by the number of non-null elements\n  mu &lt;- sum(x, na.rm = TRUE) / length(na.omit(x))\n  sigma &lt;- sd(x, na.rm = TRUE)\n\n  if (isTRUE(sigma == 0)) abort(\"`x` must have a non-zero standard deviation.\")\n  if (isTRUE(length(unique(x)) == 1L)) abort(\"`x` must have more than one unique element.\")\n  if (isTRUE(any(is.nan(x)))) abort(\"`x` must not contain any `NaN` values.\")\n  if (isTRUE(any(is.infinite(x)))) abort(\"`x` must not contain any infinite values.\")\n\n  (x - mu) / sigma\n}\n\nAnd now let’s run our tests again.\n\ntest()\n\nTest passed 🥳\nTest passed 🥳\nTest passed 🥳\nTest passed 🥇\n\n\nAwesome! We just did a simple refactor and our tests passed, so I feel good about the correctness of that refactor. This was a very simple example, but you could imagine arbitrarily complex refactors. The value of a test suite increases exponentially as the complexity of the code increases."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#wrapping-up",
    "href": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#wrapping-up",
    "title": "Unit Testing Analytics Code",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nThere are a few important takeaways from this post.\nFirst and most importantly, it’s important to test your code. There are so many ways that things can go wrong, and writing tests is the only way to really be confident that nothing is. This is especially true in dynamically typed, interpreted languages like R or Python, where data scientists don’t get the benefits of a compiler or a static type system to catch bugs in their code.\nSecond, analytical code that’s well-written should be easy to test. Ultimately, lots of the code we write for analytics work takes some data, does something to it, and returns some new data. That type of logic makes for a great use case for unit testing: If your code doesn’t produce the values you expect, it’s incorrect.\nLastly, there are many tools in Python (pytest, unittest, etc.) and R (testthat) to make unit testing as simple as writing a few lines of logic and an assertion or two. And then you get all the benefits of having well-tested code, such as the newly-found ease of refactoring said code without being concerned about breaking it.\nHappy testing, and enjoy the peace of mind!"
  },
  {
    "objectID": "posts/series/doing-data-science/2023-06-20-how-can-others-use-my-model/how-can-others-use-my-model.html",
    "href": "posts/series/doing-data-science/2023-06-20-how-can-others-use-my-model/how-can-others-use-my-model.html",
    "title": "How Can Someone Else Use My Model?",
    "section": "",
    "text": "This post is part of a series called The Missing Semester of Your DS Education."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-06-20-how-can-others-use-my-model/how-can-others-use-my-model.html#introduction",
    "href": "posts/series/doing-data-science/2023-06-20-how-can-others-use-my-model/how-can-others-use-my-model.html#introduction",
    "title": "How Can Someone Else Use My Model?",
    "section": "Introduction",
    "text": "Introduction\nAt this point in this series, I’ve discussed a lot of aspects of putting machine learning into production. I’ve gone over workflow orchestration for retraining and monitoring models, unit testing for ensuring that your code is correct, experiment tracking and model versioning for keeping track of what’s actually running in production at any given time, and dependency management and Docker for packaging up your analytical code.\nThe last of the nuts and bolts that I’ve yet to go through is how other people - teammates, clients, users, etc. - will actually use your model."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-06-20-how-can-others-use-my-model/how-can-others-use-my-model.html#types-of-inference",
    "href": "posts/series/doing-data-science/2023-06-20-how-can-others-use-my-model/how-can-others-use-my-model.html#types-of-inference",
    "title": "How Can Someone Else Use My Model?",
    "section": "Types of Inference",
    "text": "Types of Inference\nIn very broad strokes, there are two types of “inference” (read: ways of making predictions) that you’ll encounter: Online and batch. In short, “online” means that your model is making predictions in real-time and serving those predictions to whoever the user of your model is. For instance, if you have a widget on your website that lets a student increase or decrease their GPA and see how their chances of admission to Carleton College change, then the process of them sliding the slider, the new GPA being sent to your model, and your model returning a new prediction of that student’s chances is an example of online inference.\nBatch inference is what you might think of as “offline” inference: You make predictions for many people, items, etc. at one time and store them somewhere to be accessed by users. For instance, if you had individual pages with information on any given school and wanted to show a few recommended schools in a section entitled “Students who like X school also like” you might be okay with only recomputing those most similar schools once per day, or once per week, or so on. So in that case, you’d have some job - orchestrated by your workflow orchestration tool of choice - that would create the lists of similar schools for each school, and then would store those lists somewhere for them to be ingested and eventually used on your school pages. Then, once the lists are ingested, the recommendations are static until the job next runs to update them again.\nThis post will cover online inference, and a common way that a consumer of your model’s predictions might communicate with that model: A REST API."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-06-20-how-can-others-use-my-model/how-can-others-use-my-model.html#apis-and-rest",
    "href": "posts/series/doing-data-science/2023-06-20-how-can-others-use-my-model/how-can-others-use-my-model.html#apis-and-rest",
    "title": "How Can Someone Else Use My Model?",
    "section": "APIs and REST",
    "text": "APIs and REST\nAn Application Programming Interface (API) is a fancy acronym for something that I might best describe as an agreement between you and somebody else about what they provide and how you will use it. For instance, you might have used a public API like that of the Census for programmatically pulling population statistics, income statistics, and so on, or Twitter for doing the same for tweets.\nThe basic principle is that an API lets you interact with something, such as data (in the case of Twitter and the Census) or a machine learning model, as long as you comply with the API’s “contract.” If a model lives behind an API, you can make a “request” to that API, and assuming that you’ve correctly provided all the necessary fields, added the correct request headers, etc., the API will hold up its end of the bargain and return the prediction you asked for.\nVery often, you’ll hear talk of REST or “RESTful” APIs. At their core, a REST API is just an API that follows REST standards, including having a uniform interface, being stateless, and so on. I’m no expert on REST, so I’ll defer to a great explainer by the AWS team for more details."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-06-20-how-can-others-use-my-model/how-can-others-use-my-model.html#plumber",
    "href": "posts/series/doing-data-science/2023-06-20-how-can-others-use-my-model/how-can-others-use-my-model.html#plumber",
    "title": "How Can Someone Else Use My Model?",
    "section": "Plumber",
    "text": "Plumber\n\n\n\n\n\n\nThe examples in the remainder of this post will use Plumber to demonstrate a simple REST API.\nUnfortunately, I cannot in good faith recommend using Plumber or R itself in any sort of high-load or high-throughput production system for reasons that are beyond the scope of this post, and would strongly encourage considering a Python framework like FastAPI instead.\n\n\n\nIn R, the Plumber package provides a friendly set of tools for building and deploying APIs of your own. All you need in the most simple case is an R script like this:\nlibrary(plumber)\n\npr() %&gt;%\n  pr_get(\n    \"/hello\",\n    function() \"Hello world!\",\n    serializer = serializer_unboxed_json()\n  ) %&gt;%\n  pr_run(port = 9807)\nRunning that R code will open up a Swagger UI that shows the endpoint you defined at /hello that will look like this:\n\n\n\n\n\nYou can use the Swagger UI to make a request to your API and see the result, which is the Hello world! that we expected:\n\n\n\n\n\nYou can also use curl (or httr or similar) to make requests to your API from the command line, from R, or from any other language of your choosing as follows:\ncurl http://127.0.0.1:8080/hello\nThis request will return:\n\"Hello world!\""
  },
  {
    "objectID": "posts/series/doing-data-science/2023-06-20-how-can-others-use-my-model/how-can-others-use-my-model.html#a-more-involved-example",
    "href": "posts/series/doing-data-science/2023-06-20-how-can-others-use-my-model/how-can-others-use-my-model.html#a-more-involved-example",
    "title": "How Can Someone Else Use My Model?",
    "section": "A More Involved Example",
    "text": "A More Involved Example\nNow that we can successfully talk to our API, let’s imagine we wanted to make predictions from a model that predicts someone’s weight given their height. For the purposes of this example, our “model” is just going to take the height and multiply it by pi, which will return a terrible prediction, but a prediction nonetheless. This is actually an important note about APIs: This trivial example - multiplying by pi - is actually the whole point of having an API. From the consumer of an API’s point of view, whatever is happening behind the scenes is just that: Behind the scenes. The client needs no knowledge of how your API is actually going about figuring out what it should be returning to you. All the client knows is that there’s a contract: If they give you something, you process that thing and give them back what they asked for.\nWith this in mind, let’s add a /predict endpoint to our service above to predict someone’s weight based on their height.\nlibrary(plumber)\n\npr() %&gt;%\n  pr_get(\n    \"/hello\",\n    function() \"Hello world!\",\n    serializer = serializer_unboxed_json()\n  ) %&gt;%\n  pr_get(\n    \"/predict\",\n    function(height) as.numeric(height) * pi,\n    serializer = serializer_unboxed_json()\n  ) %&gt;%\n  pr_run(port = 9807)\nNow we can call our /predict endpoint with a query parameter height indicating the height of the person. Let’s call our prediction endpoint with a height of 5:\ncurl \"http://127.0.0.1:9807/predict?height=5\"\n\n15.708\nGreat! It gives back 15.708, which is indeed equal to 5 * pi.\nAgain, the actual computation here isn’t really the point. Multiplying the height by pi is indeed a terrible way to predict someone’s weight, but the key takeaway is that from the client’s point of view, the computation is a black box. As long as you don’t make any changes to how the client is supposed to interact with your API - these types of changes are called “breaking” changes, and you should be very, very careful making them - you can arbitrarily change the guts of your API and have the client be none the wiser. This means that you could multiply the height by e instead of by pi, or you could swap in a machine learning model and call its predict method from inside of the endpoint. For instance, you could do something like this:\nlibrary(plumber)\n\n## NOTE: \n## This assumes you have a model whose\n## predict method takes a single-column\n## dataframe (with `height` as the column).\n## living at `path/to/model.rds`\nmodel &lt;- readRDS(\"path/to/model.rds\")\n\npr() %&gt;%\n  pr_get(\n    \"/hello\",\n    function() \"Hello world!\",\n    serializer = serializer_unboxed_json()\n  ) %&gt;%\n  pr_get(\n    \"/predict\",\n    function(height) predict(model, newdata = data.frame(height = as.numeric(height))),\n    serializer = serializer_unboxed_json()\n  ) %&gt;%\n  pr_run(port = 9807)\nIn this example, we first load our model into memory at API boot time, and then it’s accessible to our /predict endpoint whenever it’s called by clients. You can extend this basic setup to be arbitrarily complex: Many models, many endpoints, helper code, tests, etc. but the basic premise is always the same: Declare a way for an outside client to use your model by providing an API contract, and then respond to requests for predictions from your model based on whatever inputs you need by taking those inputs, doing any data munging or feature engineering you need to do, running the prediction, doing post-processing or anything else, and returning the response to the client."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-06-20-how-can-others-use-my-model/how-can-others-use-my-model.html#deployment",
    "href": "posts/series/doing-data-science/2023-06-20-how-can-others-use-my-model/how-can-others-use-my-model.html#deployment",
    "title": "How Can Someone Else Use My Model?",
    "section": "Deployment",
    "text": "Deployment\n“But wait!” you think. “pr_run on my local machine is only accessible via localhost. What about if someone outside of my machine wants to access my model?”\nGreat question! You need to deploy your API somewhere outside of your local machine and accessible over the internet so that clients can use your model. There are a number of ways to achieve this, and discussing individual hosting platforms and how to get your code running on them is a bit of a rabbit hole beyond the scope of this post. There are a number of ways of deploying your code into production systems. One thing you can do is wrap up your API in Docker, and then ship it basically anywhere that runs Docker containers (read: basically anywhere). But how you go about shipping your code to production is ultimately a decision that needs to be made based on your team and the problem you’re solving, among other factors. There’s certainly no single right way to go about it.\n\n\n\n\n\n\nFor a bit more on this, I wrote about Docker in my last post, and Plumber has great documentation on how to use Docker to deploy a Plumber API.\n\n\n\nIf you do ultimately decide to use Docker, you’ll have a solution that works roughly the same whether you’re working in Plumber, Flask, FastAPI, or another framework. You’d build your API, package it up in a Docker image, and ship that image to some place out in the world that can run said image for you. Then, once it’s there, you can call it just as you would any public-facing API!"
  },
  {
    "objectID": "posts/series/doing-data-science/2023-06-20-how-can-others-use-my-model/how-can-others-use-my-model.html#wrapping-up",
    "href": "posts/series/doing-data-science/2023-06-20-how-can-others-use-my-model/how-can-others-use-my-model.html#wrapping-up",
    "title": "How Can Someone Else Use My Model?",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nThis post was the last nuts and bolts, mechanical post in this series about doing data science. And it felt like a good place to wrap up: Going over how models actually get used by consumers in the real world at a very high level.\nThe deployment, monitoring, and performance of APIs in production systems has been written about and done over and over again by lots of people far more qualified than I am to educate others on it. Notably, virtually every software developer working on the back end of any website or similar service has worked in production systems that are far more complicated than anything I’ve worked in, so I’ll defer lots of the technical details and best practices to them in favor of giving a hopefully helpful, minimal example here about how to achieve a similar goal, but for serving predictions from machine learning models.\nAt this point, I’ve discussed most of the end-to-end process that we follow for shipping ML products: Starting with the basics of how we structure, test, and review code, to how we run that code in Airflow to train and monitor models, to how we use Docker to deploy APIs in production. There’s lots I’ve missed and didn’t cover, but my hope was that by this point, I’d have achieved one of my main goals in writing this series: To give aspiring data scientists a brief, gentle introduction to some of the tools and processes that we lean on to operationalize our data science work."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-14-code-review/code-review.html",
    "href": "posts/series/doing-data-science/2023-04-14-code-review/code-review.html",
    "title": "Pull Requests, Code Review, and The Art of Requesting Changes",
    "section": "",
    "text": "This post is part of a series called The Missing Semester of Your DS Education."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-14-code-review/code-review.html#introduction",
    "href": "posts/series/doing-data-science/2023-04-14-code-review/code-review.html#introduction",
    "title": "Pull Requests, Code Review, and The Art of Requesting Changes",
    "section": "Introduction",
    "text": "Introduction\nIn my last post on unit testing analytics code, I started off by saying that I had never heard of unit testing until my second data science job. Not in college, not in internships, and not in my first data science job. The same is true for pull requests and code review – they were completely new to me.\nThis, in my view, is a very unfortunate commonality in analytics teams. All too often, code will go unreviewed or kept out of any kind of version control system entirely. Ultimately, this practice, albeit common, is detrimental to the teams practicing it.\nSince then, code review – both doing code review for others and getting feedback from others on my code – has grown to become one of my favorite parts of my job. It’s also a mission-critical process for any team that strives to maintain a high-quality codebase. But more importantly, it’s an valuable social exercise, and an fun way for contributors to come together as a team to collaborate, learn from each other, and improve their skills.\n\n\n\n\n\n\nI’d also like to note that the Tidyverse team just published a great series on code review, which I’d highly recommend reading."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-14-code-review/code-review.html#but-first-git",
    "href": "posts/series/doing-data-science/2023-04-14-code-review/code-review.html#but-first-git",
    "title": "Pull Requests, Code Review, and The Art of Requesting Changes",
    "section": "But First: Git",
    "text": "But First: Git\nThere are a ton of introductions to Git online, all of which are far better than anything I could write here. If you’re not already at least somewhat familiar with Git, I’d recommend starting with a post like one of these:\n\nFrom GitHub\nFrom Atlassian\nFrom Git itself\n\nThat said, what I’ll discuss here doesn’t require any particular Git knowledge."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-14-code-review/code-review.html#on-code-review",
    "href": "posts/series/doing-data-science/2023-04-14-code-review/code-review.html#on-code-review",
    "title": "Pull Requests, Code Review, and The Art of Requesting Changes",
    "section": "On Code Review",
    "text": "On Code Review\nI wasn’t sure where to start this post. I considered discussing Git itself, but I decided it would be better to leave it to the experts. Then I thought about discussing the function of pull requests, but that didn’t seem particularly valuable either. So I figured I might just jump into the meat of it: Why code review is valuable, and why analytics teams should be doing more of it than they are.\n\nWhy Do We Review Code?\nPeople and teams have wildly differing opinions about what the main objective of code review is. The following is the opinion of the Tidyverse team:\n\nThe primary purpose of code review is to ensure that the overall “code health” of our packages is improving over time.\n\nAnother common opinion on the main purpose of code review is to catch bugs in code before they hit production.\nIn my opinion, these are both important objectives of code review to be sure, but are not the main objective. My team’s view is that the main objective of code review is knowledge transfer between teammates. On our team – with “team” encompassing both data and engineering – we talk often about the bus factor of our work. A toned-down explanation of the bus factor is that it represents the number of people who need to go on vacation, or be out sick, or leave for a new job for a project to come to a grinding halt because else nobody on the team knows how a particular system works. We’ve been bitten multiple times by projects with a bus factor of one, meaning that if one person were to leave, that project would be left in limbo.\nAnalytics work is no different than engineering work in this respect: If one data scientist lives in a dark room for two years building a novel model and ships it to production without anyone else knowing how it works, how to maintain it, and how to monitor it, then that project is risky. If that data scientist were to leave, there would be a model in production that nobody else had experience working with, which could result in any number of issues. In steps code review to help us solve this problem.\n\n\nOther Reasons To Review Code\nAside from knowledge transfer, code review is also a valuable opportunity to do quality control. In no particular order, all of the following (and more) can be suggested in code review:\n\nBug fixes\nMaintainability improvements, such as improving the readability of the code, adding comments, adding tests, etc.\nPerformance improvements\nSecurity improvements\n\nAll of these are important aspects of maintaining a codebase over the long-term, and, in my experience, are far too often overlooked by analytics teams. In particular, a past team of mine – as well as many others, from what I’ve read and heard in talking to others in analytical roles – had a bad habit of data scientists being siloed and not having any eyes on their work until things were “ready” to be shipped to production. In my opinion, data products are software and should be treated as such. This means that if our models will be shipped to production, then it’s imperative that the code underlying them is written in a way that’s understandable by others, safe, correct, and, to whatever extent necessary, performant. Code review helps achieve all of these goals.\nLastly, code review is a social exercise. It’s a great way for a team to work together, especially in the world of remote work. In getting feedback from teammates, data scientists get to flex their collaboration muscle and, ideally, this type of collaboration allows the whole to be more than the sum of its parts.\n\n\nAn Example Review\nSince code review is unfamiliar to a lot of us in the analytics world, I’ll give an example of how I might do a review. Our team often works with data from external sources like the Census for doing all kinds of analysis. Let’s imagine reviewing the following code that a hypothetical teammate has put in to determine the five counties in Arizona with the highest percentage of Native American residents:\n\nlibrary(httr)\nlibrary(purrr)\nlibrary(dplyr)\n\nvariables &lt;- c(\n  \"NAME\",\n  \"B01001A_001E\",\n  \"B01001B_001E\",\n  \"B01001C_001E\",\n  \"B01001D_001E\",\n  \"B01001E_001E\",\n  \"B01001F_001E\",\n  \"B01001G_001E\"\n)\n\nresponse &lt;- GET(\n  \"https://api.census.gov/data/2019/acs/acs5\",\n  query = list(\n    get = paste(variables, collapse = \",\"),\n    \"for\" = \"tract:*\",\n    \"in\" = \"state:04\",\n    key = secret::get_secret(\"CENSUS_API_KEY\", vault = sprintf(\"%s/posts/config\", Sys.getenv(\"SITE_ROOT_DIR\")))\n  )\n) %&gt;%\n  content()\n\ncols &lt;- response[[1]]\nresponse &lt;- response[-1]\n\nresponse %&gt;%\n  map_dfr(\n    ~ set_names(.x, cols)\n  ) %&gt;%\n  janitor::clean_names() %&gt;%\n  mutate(\n    across(\n      -c(name, tract, county, state),\n      as.numeric\n    )\n  ) %&gt;%\n  group_by(county) %&gt;%\n  summarize(\n    na_pct = sum(b01001c_001e) / sum(rowSums(across(where(is.numeric))))\n  ) %&gt;%\n  slice_max(\n    order_by = na_pct,\n    n = 5\n  ) %&gt;%\n  pretty_print()\n\n\n\n\ncounty\nna_pct\n\n\n\n\n001\n0.7377187\n\n\n017\n0.4465544\n\n\n005\n0.2654699\n\n\n007\n0.1630934\n\n\n012\n0.1495215\n\n\n\n\n\n\n\nAt first glance, this code looks pretty involved and poorly written to me. In the real world, we often write complicated code that relies on hyper-specific domain knowledge about the problem being solved in order to really understand it. As I mentioned above, our team often finds ourselves working with external data sources, such as the Census data shown here. And these sources usually have their own internal codes or identifiers that denote what the data you’re actually looking at is about. For instance, the series B01001C_001E used above represents the total Native American population in a region. In the Census, it’s corresponding “concept” is defined as the following:\n\nB01001C_001E: SEX BY AGE (AMERICAN INDIAN AND ALASKA NATIVE ALONE)\n\nObviously, these codes are not comprehensible for the layman. There’s method to the madness, but we so often work with gibberish codes that don’t correspond to anything intuitive and make code review and maintenance dramatically harder. The challenge here is that in order to effectively review this code, the reviewer not only needs to review what the code is trying to accomplish in some high-level sense, but also needs to understand the data itself and its schema in order to understand how the author is trying to accomplish their goal. Unfortunately, the code’s author has not done a good job of helping the reviewer (me) understand the code.\nNot only do I not have any context on the Census codes, but there’s no documentation linked (or written out) to help me find what the codes mean, there’s no documentation of the API response to explain why they’re seemingly removing the first item in the response’s body and setting it as the names of the columns in my final data frame, and there’s no explanation of what na_pct means, which in the context of this work means “percentage Native American” but could also reasonably mean something like “percentage of the data that’s null”.\nAnd this is exactly where the value of code review comes in. We often like to say that you’re not writing code for yourself right now. Instead, you should be writing it for a future you in six months or a year who won’t remember all of the intricacies of this problem anymore. In short, you want your code to be understandable to future you, and, by extension, your teammates. But code authors, just like anyone else working on a project of any sort, are often so deep in the weeds of their work and so familiar with all of the ins and outs that it’s easy to forget just how confusing their work can be at times. Code review is a great opportunity for others to ask for clarification, which is the knowledge transfer I mentioned before.\nIf a teammate put in these changes, I’d request a few improvements:\n\nWhat do the Census codes mean?\nWhat does na_pct mean?\nWhat does the response from the Census API look like?\nWhat does state:04 mean?\n\nAfter requesting clarification on all of those points, I’d hope the code would ultimately end up looking something like this:\n\n## Data from the Census ACS (American Community Survey)\n##\n## ACS 2019 5-year data\n## B01001A_001E: Total White alone population\n## B01001B_001E: Total Black or African American alone population\n## B01001C_001E: Total American Indian and Alaska Native alone population\n## B01001D_001E: Total Asian alone population\n## B01001E_001E: Total Native Hawaiian and Other Pacific Islander alone population\n## B01001F_001E: Total Some other race alone population\n## B01001G_001E: Total Two or more races population\n##\n## Codes of the form B01001X_001E\n## correspond to \"Total X population\" where X\n## is a race as defined by the Census. For instance,\n## B01001A_001E: Total White alone population\nvariables &lt;- c(\n  white_alone = \"B01001A_001E\",\n  black = \"B01001B_001E\",\n  native_american = \"B01001C_001E\",\n  asian = \"B01001D_001E\",\n  hawaiian_pacific_islander = \"B01001E_001E\",\n  other_alone = \"B01001F_001E\",\n  multi_racial = \"B01001G_001E\"\n)\n\nresponse &lt;- GET(\n  \"https://api.census.gov/data/2019/acs/acs5\",\n  query = list(\n    get = paste(variables, collapse = \",\"),\n    \"for\" = \"tract:*\",\n    \"in\" = \"state:04\",\n    key = secret::get_secret(\"CENSUS_API_KEY\", vault = sprintf(\"%s/posts/config\", Sys.getenv(\"SITE_ROOT_DIR\")))\n  )\n) %&gt;%\n  content()\n\n## The first item in the JSON response body is a vector\n## of the names of the fields returned.\n## For this query, we get: \n##\n##   * B01001A_001E\n##   * B01001B_001E\n##   * B01001C_001E\n##   * B01001D_001E\n##   * B01001E_001E\n##   * B01001F_001E\n##   * B01001G_001E\n##   * state\n##   * county\n##   * tract\n##\n## Every other record contains data corresponding to these fields.\n## The values in the population fields (e.g. B01001A_001E) are integers\n## representing the total population for that racial group in that tract.\n## The state, county, and tract are all FIPS codes.\ncols &lt;- response[[1]]\nresponse &lt;- response[-1]\n\nresponse %&gt;%\n  map_dfr(\n    ~ set_names(.x, cols)\n  ) %&gt;%\n  janitor::clean_names() %&gt;%\n  mutate(\n    across(\n      all_of(tolower(unname(variables))),\n      as.numeric\n    )\n  ) %&gt;%\n  group_by(county) %&gt;%\n  summarize(\n    percent_native_american = sum(b01001c_001e) / sum(rowSums(across(all_of(tolower(unname(variables))))))\n  ) %&gt;%\n  slice_max(\n    order_by = percent_native_american,\n    n = 5\n  ) %&gt;%\n  pretty_print()\n\n\n\n\ncounty\npercent_native_american\n\n\n\n\n001\n0.7377187\n\n\n017\n0.4465544\n\n\n005\n0.2654699\n\n\n007\n0.1630934\n\n\n012\n0.1495215\n\n\n\n\n\n\n\nNow that the code has been rewritten, future readers of the code have some much needed context. The author has explained what the data looks like when we get it back from the API, has explained what each of the codes corresponds to, and has renamed a variable to be more helpful. Ultimately, these changes will make it easier for readers to understand what this code is trying to accomplish and work with it."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-14-code-review/code-review.html#wrapping-up",
    "href": "posts/series/doing-data-science/2023-04-14-code-review/code-review.html#wrapping-up",
    "title": "Pull Requests, Code Review, and The Art of Requesting Changes",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nThe key takeaway here should be that in doing this review, we’ve established some shared sense of how this code works between multiple people instead of just one (the author) understanding it. In addition, the reviewer helped the author rework their code to make it easier for anyone else on the team to understand. This means that when we need to use this code in the future, it’ll be written in a way that makes it easier for that future person to work with, which will ultimately lead to them being able to ship more quickly and confidently, and needing to reinvent the wheel less often.\nA popular saying is that code is read ten times more than it’s written since we often need to understand existing code to author new code. Reviews are a great way to make sure that code is written in a maintainable, understandable way so that when it’s read all of those many times in the future, it’s able to be understood effortlessly."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-06-workflow-orchestration/workflow-orchestration.html",
    "href": "posts/series/doing-data-science/2023-05-06-workflow-orchestration/workflow-orchestration.html",
    "title": "Workflow Orchestration",
    "section": "",
    "text": "This post is part of a series called The Missing Semester of Your DS Education."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-06-workflow-orchestration/workflow-orchestration.html#introduction",
    "href": "posts/series/doing-data-science/2023-05-06-workflow-orchestration/workflow-orchestration.html#introduction",
    "title": "Workflow Orchestration",
    "section": "Introduction",
    "text": "Introduction\nWhen I first started my current job, I installed an app called Amphetamine on my machine. As an editorial sidebar: I’d highly recommend Amphetamine. It’s a great app.\nBut anyways, the point is that the reason I had installed Amphetamine was to keep my machine alive at night as I was running some code to train a model or do some analysis that took seven or eight hours locally. My strategy – which I thought was the norm at the time, and was a habit I had brought over from my previous data science role – was to kick off a job manually, tell Amphetamine to keep my machine awake, plug it in, turn the brightness down, and go to sleep. In the morning, I could wake up and see my results."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-06-workflow-orchestration/workflow-orchestration.html#headaches",
    "href": "posts/series/doing-data-science/2023-05-06-workflow-orchestration/workflow-orchestration.html#headaches",
    "title": "Workflow Orchestration",
    "section": "Headaches",
    "text": "Headaches\nI imagine that this pattern is fairly common for data scientists. I thought it was totally normal, and it worked well enough (aside from the fact that my machine’s battery was crying out for help, but I digress). Over the past few years, I’ve learned just how much of an anti-pattern this was in reality. There were a number of painful aspects to this strategy that I had cooked up, all of which took me far too long to recognize as being legitimate:\n\nRunning a job on local at night means that there’s no audit trail of past job runs, and no schedule for future ones. Everything is ad-hoc and “memoryless” in some sense. Without the help of other tooling, it’s hard to remember what happened before and what jobs have run and which are yet to be run.\nIf something went wrong, I was the only one to know about it. Since the problem – error or otherwise – would only show up on my machine, none of my teammates would have any idea that anything had broken.\nIf a product (a model, a dashboard requiring some data wrangling to happen, etc.) needed to be updated for a production setting, the only way to go about that was for me to put down my work, open my code to do the updates, and kick off the job. Unfortunately, these jobs would often eat lots of my machine’s compute resources, leaving me unproductive for the remainder of the day while something ran.\nWhat I could build and run was severely constrained by the compute resources of my machine, which is pretty beefy but not invincible."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-06-workflow-orchestration/workflow-orchestration.html#workflow-orchestration-and-mlops",
    "href": "posts/series/doing-data-science/2023-05-06-workflow-orchestration/workflow-orchestration.html#workflow-orchestration-and-mlops",
    "title": "Workflow Orchestration",
    "section": "Workflow Orchestration and MLOps",
    "text": "Workflow Orchestration and MLOps\nAs it turns out, there are many, many tools that provide the proverbial ibuprofen for these headaches, if you will. Broadly speaking, they fall into two often-overlapping categories: Workflow orchestration tools and MLOps tools. This post will cover workflow orchestration, since workflow orchestration tools are a core part of most every data stack and are more common than MLOps tools, as they’re used for data science, data engineering, and more.\nThere are lots of workflow orchestrators on the market, and there’s a wide range of options in terms of capabilities they provide, whether they’re self-hosted, open-source tools or managed services, and much more. A few popular names in the space are Airflow, Luigi, Dagster, Argo, and Prefect. These are all fantastic tools that come with their own pros and cons, but at their core they all seek to achieve a similar goal: Letting you and your team run jobs in a sane way.\nThe long and short of workflow orchestration tools is that they provide tooling to help you run code – often in different languages and performing disparate tasks – in any way you want, by providing high-level APIs for triggering your jobs. For instance, we often run R code using Docker containers running in individual pods on our Kubernetes cluster, while we might trigger a bash task to just run on the leader node of our cluster without spinning up a new container. Even if that was a lot of technical jargon, the main takeaway is simple: Workflow orchestration tools let you run your code in many ways and in many places, which is incredibly powerful. You can run jobs locally (on your tool of choice’s compute) or you can farm them off to an external source of compute (like Batch or Lambda, and many, many more) to have them run in a galaxy far, far away. The workflow orchestrator will handle the triggering of the job, the monitoring of the job and listening for it to complete successfully or with errors, and will handle the alerting, deciding whether or not to continue running the next steps in the job based on what happened in the previous ones, and so on. All of these tools are highly configurable to your needs."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-06-workflow-orchestration/workflow-orchestration.html#airflow",
    "href": "posts/series/doing-data-science/2023-05-06-workflow-orchestration/workflow-orchestration.html#airflow",
    "title": "Workflow Orchestration",
    "section": "Airflow",
    "text": "Airflow\nOur team uses Airflow (Astronomer in particular, which has a very helpful team and does a great job of managing complicated things like Kubernetes and authentication for us), so that’s what I’ll discuss here. And this time, it’ll be concise.\nAirflow solves a few key problems for us:\n\nIt lets us run jobs on schedules that we define in the code. We can define any schedule we want, and our jobs will magically be run on time, in the cloud. In addition, if we want to manually trigger a job, all we have to do is press the play button.\nIt lets us modularize our jobs, so that if a step fails, we can retry or restart from the failure as opposed to from the start, which often saves lots of time.\nIt provides great visibility into the jobs themselves. In particular, we know when they fail, we know how long steps are taking to complete, and so on. Airflow makes it easy to track and audit what’s happening inside of our jobs.\nIt lets us trigger jobs in many places, such as running retraining jobs for our ML models on AWS Batch. Batch lets us spin up a machine that matches some compute requirements we have, run whatever code we need to on it, and shut down once it finishes. This is hugely valuable, since some of our jobs are memory heavy, and others require lots of cores to parallelize across, and so on. Airflow’s BatchOperator lets us configure the compute resources we need for each individual step (task) in our job, which is extremely flexible.\nAnd much, much more.\n\nNote that these points are solutions to the headaches listed above. In particular:\n\nAirflow lets us track past runs of jobs, so it’s easy to audit the history of any particular job and schedule future jobs as we please.\nAirflow can send notifications of job failures to (e.g.) Slack, which lets our team know that something is broken and starts our triage process.\nAirflow lets us very easily run our jobs on third-party (AWS, in our case) compute, which results in something of a “set it and forget it” process for running jobs: We press play, and wait for the job to finish. And in the meantime, we continue doing whatever else we were working on with minimal disruption.\nSince Airflow lets us easily run jobs on AWS compute and we can dynamically set compute requirements, we can spin up a specific machine – an EC2 box in our case – that’s well-suited to the needs of our jobs. We have some memory intensive jobs that we run on big R4 instances, which provide lots of RAM. You might also need a GPU for a job that trains a deep learning model, in which case you’d configure your compute requirements to include a GPU, and Batch could spin up a P3 instance (with a GPU). Instead of being limited by the compute resources of our local machines, we now have easy access to the entire AWS fleet of EC2 instance types."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-06-workflow-orchestration/workflow-orchestration.html#wrapping-up",
    "href": "posts/series/doing-data-science/2023-05-06-workflow-orchestration/workflow-orchestration.html#wrapping-up",
    "title": "Workflow Orchestration",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nThere was admittedly a lot of technical jargon in this post. But the main takeaway is this: Having a true workflow orchestration tool like Airflow makes it simple for you to run your analytical code (or any other code!) in a sane way. Workflow orchestrators help you run your code on certain schedules, provide lots of visibility into individual runs of your code, help you farm off your code to third-party compute, alert you when things go wrong, and so much more. So please, please shut off your machine at night instead of training your models on it. If you can, you should use a workflow orchestrator. It’s good for your machine’s health, your team’s productivity, and your sanity."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-27-renv-dependency-management/renv-dependency-management.html",
    "href": "posts/series/doing-data-science/2023-05-27-renv-dependency-management/renv-dependency-management.html",
    "title": "Dependency Management",
    "section": "",
    "text": "This post is part of a series called The Missing Semester of Your DS Education."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-27-renv-dependency-management/renv-dependency-management.html#introduction",
    "href": "posts/series/doing-data-science/2023-05-27-renv-dependency-management/renv-dependency-management.html#introduction",
    "title": "Dependency Management",
    "section": "Introduction",
    "text": "Introduction\nWhen I was first learning to program, I’d face problems that would require (or at least were just made easier by using) library code. For instance, I learned about the Tidyverse the hard way my first summer in college when I was interning at a FinTech firm doing lots of data wrangling, because I had just finished spending the summer very hackily implementing group_by and summarize by hand using lots and lots of layers of nested for loops. That experience taught me an important lesson: I realized that thinking “Someone else must have solved this problem before” and then seeing what they’ve done is a very practical way to solve problems.\nAnd with that realization (that I could use libraries for data wrangling), I started writing lots of lines of code like this one:\ninstall.packages(\"tidyverse\")\nor the not-quite-equivalent Python:\npip install pandas\nAnd as soon as I began doing that, I started running into one of the most common, if not the most common, source of headaches for new programmers: Dependency conflicts. I was getting errors like these:\n&gt; Error: package or namespace load failed for ‘foo’ in loadNamespace(j &lt;- i[[1L]], \n&gt; c(lib.loc, .libPaths()), versionCheck = vI[[j]]):\n    namespace ‘bar’ 0.6-1 is being loaded, but &gt;= 0.8 is required"
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-27-renv-dependency-management/renv-dependency-management.html#whats-the-problem",
    "href": "posts/series/doing-data-science/2023-05-27-renv-dependency-management/renv-dependency-management.html#whats-the-problem",
    "title": "Dependency Management",
    "section": "What’s the Problem?",
    "text": "What’s the Problem?\nAt the time, I saw these types of errors and was frustrated. I didn’t understand them – why is a package requiring a specific version of another package? And I also thought there was a quick fix: I’d just install the required version of the dependency and everything would work again.\nUntil I inevitably ran into the same problem again soon after, while trying to install some other dependency.\nAt its core, I was experiencing my first foray into what might be considered a semi-pro version of dependency hell. I thought I could “solve” my problem of incompatible dependencies with duct tape – installing the correct version in order to fix the immediate error – but that doesn’t actually work in practice, since the same issue is bound to come up again with another version of a different library, or in a different project, or on another day."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-27-renv-dependency-management/renv-dependency-management.html#dependency-resolution",
    "href": "posts/series/doing-data-science/2023-05-27-renv-dependency-management/renv-dependency-management.html#dependency-resolution",
    "title": "Dependency Management",
    "section": "Dependency Resolution",
    "text": "Dependency Resolution\nIn practice, this is a dependency management problem. And the basic story goes as follows.\nYou start on your programming journey, and you want to use some great library – take dplyr, for example – for some of your work. So you install.packages(\"dplyr\") and it installs the most recent version. Then you decide you’re interested in working with spatial data, so you install sf with install.packages(\"sf\"). Next, you realize you need to work with Census data (what else are you going to be making maps of, after all?), so you install.packages(\"tidycensus\") and with those three packages, you do your project.\nBut then next week you have a homework assignment, which, entirely hypothetically, requires you to install some niche package like rethinking, so, of course, you install.packages(\"rethinking\") and you do your homework.\nAnd then later that night, you get curious about seeing what the Elon Jet Tracker has been up to on Twitter, and so you install rtweet: install.packages(\"rtweet\"), but this time, you get one of the errors above about a namespace clash.\nAnd so now, you’re at a block in the road. You can either upgrade or downgrade the dependency that’s clashing and risk one of your other projects breaking, or you can try to get around the problem another way, such as using an old version of rtweet or not using rtweet at all.\nThis is the point at which former you – and former me – should have been thinking “someone must have solved this problem.”"
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-27-renv-dependency-management/renv-dependency-management.html#declaring-your-dependencies",
    "href": "posts/series/doing-data-science/2023-05-27-renv-dependency-management/renv-dependency-management.html#declaring-your-dependencies",
    "title": "Dependency Management",
    "section": "Declaring Your Dependencies",
    "text": "Declaring Your Dependencies\nThe root cause of the errors in situations like these is that we have a dependency leakage problem. In the example I gave above, there were three separate “projects”: Your Census data analysis, your homework, and your keeping tabs on Elon’s jet. Those three projects don’t need to know about each other, and it’s actually a problem that they do. The issue is that in using install.packages(\"...\") (or pip install ...) everywhere, you’re installing all of these packages globally on your machine. This means that every project you’re working on needs to use the same dependencies, even when the projects are separate.\nAbove, I proposed the solution to this problem that I used to use, which was to just install the version of the package that was needed by the package I was trying to install, and continue on until I was inevitably frustrated by the issue once again, generally sooner rather than later. But there’s a battle-hardened way of solving this problem: Declaring your dependencies, and using a dependency isolation tool.\nIn R, there’s the great renv for this. In Python, I personally like Poetry, but Conda, pipenv, or just a plain old virtualenv would work just fine too. The key is that you want to be declaring the dependencies that your project needs in some kind of file (such as an DESCRIPTION file in R, or a pyproject.toml or a requirements.txt in Python, or a Gemfile in Ruby, and so on), and then preferably having a dependency manager like renv or poetry resolve those dependencies and save the result into a lockfile, like a poetry.lock or an renv.lock. Then, when you want to work on your project, or homework, or whatever, you restore the dependencies as they’re recorded in that lockfile. This means that whenever you want to run a project, you know exactly what versions of every dependency need to be installed. And at the same time, if you want to add a new dependency, your dependency manager can do its best to resolve conflicts between that new dependency and all of the other libraries in your lockfile."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-27-renv-dependency-management/renv-dependency-management.html#dependency-isolation",
    "href": "posts/series/doing-data-science/2023-05-27-renv-dependency-management/renv-dependency-management.html#dependency-isolation",
    "title": "Dependency Management",
    "section": "Dependency Isolation",
    "text": "Dependency Isolation\nThe other key piece to the puzzle is dependency isolation, which is the leakage problem from before. In an ideal world, the dependencies for your project should be “isolated”, meaning that they’re only installed in your project environment (the R project, the virtualenv, etc.) and not globally on your machine.\nLet’s take an example in R to see how dependency isolation works. Start by initializing a very simple R project by running the command below in a terminal.\nmkdir renv-test && \\\n  cd renv-test && \\\n  echo \"library(example)\" &gt;&gt; test.R && \\\n  Rscript -e \"install.packages('renv', repos='http://cran.us.r-project.org'); renv::init()\" && \\\n  Rscript -e \"renv::install('mrkaye97/mrkaye97.github.io:posts/series/doing-data-science/2023-05-27-renv-dependency-management/example'); renv::snapshot()\" && \\\n  cat renv.lock\nYou should see some R output about installing packages, and then you should see something like this:\n{\n  \"R\": {\n    \"Version\": \"4.1.2\",\n    \"Repositories\": [\n      {\n        \"Name\": \"CRAN\",\n        \"URL\": \"https://cloud.r-project.org\"\n      }\n    ]\n  },\n  \"Packages\": {\n    \"example\": {\n      \"Package\": \"example\",\n      \"Version\": \"0.1.0\",\n      \"Source\": \"GitHub\",\n      \"RemoteType\": \"github\",\n      \"RemoteHost\": \"api.github.com\",\n      \"RemoteUsername\": \"mrkaye97\",\n      \"RemoteRepo\": \"mrkaye97.github.io\",\n      \"RemoteSubdir\": \"posts/series/doing-data-science/2023-05-27-renv-dependency-management/example\",\n      \"RemoteRef\": \"master\",\n      \"RemoteSha\": \"a72bb805e175c77e7bb8a2f4fb11780b76807d4d\",\n      \"Hash\": \"22d8e981dd94e2fab693636781631008\"\n    },\n    \"renv\": {\n      \"Package\": \"renv\",\n      \"Version\": \"0.17.3\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"CRAN\",\n      \"Requirements\": [\n        \"utils\"\n      ],\n      \"Hash\": \"4543b8cd233ae25c6aba8548be9e747e\"\n    }\n  }\n}\nThis last bit is the renv lockfile. It’s where all of the dependencies of your project are enumerated. Here, the project is trivial. We’re just installing example, which has no dependencies, so it’s the only thing (in addition to renv itself) that’s recorded in the lockfile. But this also generalizes to arbitrarily complicated projects and sets of dependencies. As you add more packages, the lockfile grows, but you can simply run renv::restore() to restore all of the dependencies for your project.\nNext, if you run the following, you should be able to use the package just fine:\nRscript -e \"library(example); hello()\"\nAnd you’ll see:\n[1] \"Hello, world!\"\nBut then, try this:\ncd .. && \\\n  Rscript -e \"library(example); hello()\"\nUnless you have another package installed globally called example that does the same thing as my example package, you’ll see:\nError in library(example) : there is no package called ‘example’\nExecution halted\nBut this time, this error is a feature, not a bug! This is an example of dependency isolation. When you’re inside of your renv-test project, you install the example package, record it in the lockfile, and can use it just fine. But as soon as you’re no longer inside of that project, this package is no longer available. This means that if you, for instance, start a new project that needs a different version of example, you don’t need to worry about this renv-test project being corrupted. The two projects are isolated from each other, so they can rely on separate sets of dependencies with no leakage from one to the other.\n\n\n\n\n\n\nAs an aside, tools like renv and pip are also usually good at dependency resolution, meaning they can figure out which versions of the dependencies of the packages you want to use you should install in order to make sure that everything is compatible. If you’re interested, you can read about how pip does dependency resolution on their site."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-27-renv-dependency-management/renv-dependency-management.html#other-resources",
    "href": "posts/series/doing-data-science/2023-05-27-renv-dependency-management/renv-dependency-management.html#other-resources",
    "title": "Dependency Management",
    "section": "Other Resources",
    "text": "Other Resources\nThere are tons of amazing resources on managing dependencies. This is a problem that every software team needs to manage, so virtually every programming language will have at least one, if not many widely-used dependency management tools. Since this is such a common problem, there’s also a lot written (and spoken!) about it. A few resources I particularly like are David Aja’s RStudio::conf 2022 talk about renv (which I was enthusiastically in the audience for!) and something he mentions, which is The 12-Factor App, which our team’s Head of Engineering recommended to me very soon after starting at CollegeVine."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-27-renv-dependency-management/renv-dependency-management.html#wrapping-up",
    "href": "posts/series/doing-data-science/2023-05-27-renv-dependency-management/renv-dependency-management.html#wrapping-up",
    "title": "Dependency Management",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nDavid Aja’s talk on this topic was called “You Should Be Using renv,” and you should be. In the short run, it might feel like setting up the scaffolding for renv or poetry or whatever your tool of choice is adds too much overhead. But if you’re feeling that way, it’s important to keep in mind that it’s inevitable that you end up running into dependency issues, and it’ll be much harder to untangle them once you’re already in deep than it will be to build good habits from the get-go.\nYou should be using renv."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-06-03-docker/docker.html",
    "href": "posts/series/doing-data-science/2023-06-03-docker/docker.html",
    "title": "A Gentle Introduction to Docker",
    "section": "",
    "text": "This post is part of a series called The Missing Semester of Your DS Education."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-06-03-docker/docker.html#introduction",
    "href": "posts/series/doing-data-science/2023-06-03-docker/docker.html#introduction",
    "title": "A Gentle Introduction to Docker",
    "section": "Introduction",
    "text": "Introduction\nIf you’re doing data science work, it’s likely you’ll eventually come across a situation where you need to run your code somewhere else. Whether that “somewhere” is the machine of a teammate, an EC2 box, a pod in a Kubernetes cluster, a runner in your team’s CI/CD rig, on a Spark cluster, and so on depends greatly on the problem you’re solving. But the ultimate point is the same: Eventually, you’ll need to be able to package your code up, put it somewhere in the world other than your local machine, and have it run just like it has been for you."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-06-03-docker/docker.html#enter-docker",
    "href": "posts/series/doing-data-science/2023-06-03-docker/docker.html#enter-docker",
    "title": "A Gentle Introduction to Docker",
    "section": "Enter: Docker",
    "text": "Enter: Docker\nDocker seems very complicated at first glance. And there’s a lot of jargon: Images, containers, volumes, and more, and that doesn’t even begin to cover the world of container orchestration: Docker-Compose, Kubernetes, and so on. But at its core, you can think of Docker as a little environment – not too unlike your local machine – that has a file system, configuration, etc. that’s packaged up into a magical box that you can run on any computer, anywhere. Or at least on computers that have Docker installed.\nIt might be simplest to consider a small example.\n\n\n\n\n\n\nNote that to run the example that will follow, you’ll need to have Docker installed on your machine. All of the files necessary to run this example can be found in my blog’s Github repo\nI’ll use R for the example I provide in this post, but note that the same principles apply if you’re doing your work in Python, or in any other programming language.\n\n\n\nLet’s imagine we want to print “Hello from Docker!” from R. First, make a new directory called docker-example (or whatever you want to call it):\nmkdir docker-example && cd docker-example\nAnd then we might do something like the following:\n## Dockerfile\n\nFROM rocker/r-ver:4.2.0\n\nCMD [\"Rscript\", \"-e\", \"'Hello from Docker!'\"]\nIf you paste that into a file called Dockerfile, you can then run:\ndocker build --tag example .\nWhich will build the Docker image by running each command you’ve specified. Going line by line, those commands are:\n\nUse the rocker/r-ver:4.2.0 image as the base image. In Docker, base images are useful because they come with things (such as the R language) pre-installed, so you don’t need to install them yourself. rocker/r-ver:4.2.0 ships with R version 4.2.0 pre-installed, which means you can run R as you would on your local.\nAfter declaring the base image, we specify a command to run when docker run is invoked. This command is simple – it just prints Hello from Docker!.\n\nOnce the build has completed, you can:\ndocker run example\nand you should see:\n[1] \"Hello from Docker!\"\nTada 🎉! You just ran R in a Docker container. And since you have your code running in Docker, you could now run the same code on any other machine that supports Docker."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-06-03-docker/docker.html#more-complicated-builds",
    "href": "posts/series/doing-data-science/2023-06-03-docker/docker.html#more-complicated-builds",
    "title": "A Gentle Introduction to Docker",
    "section": "More Complicated Builds",
    "text": "More Complicated Builds\nOf course, this example was trivial. In the real world, our projects are much more complex. They have dependencies, they rely on environment variables, they have scripts that need to be run, and so on.\n\nCopying Files\nLet’s start with running a script instead of running R from the command line as we have been.\nCreate an R script called example.R that looks like this:\n## example.R\n\nprint(\"Hello from Docker!\")\nAnd then you can update the Dockerfile by adding a COPY command to copy the script into your image, as follows.\n## Dockerfile\n\nFROM rocker/r-ver:4.2.0\n\nCOPY example.R example.R\n\nCMD [\"Rscript\", \"example.R\"]\nThe COPY command tells Docker that you want to take example.R and put it into your image at /example.R. You can also specify a file path in the image, but I’m just putting the files I copy in at the root.\nFinally, let’s build and run our Docker image again:\ndocker build --tag example .\n\ndocker run example\nAmazing! You can see in the build logs that the example.R script was copied into the image:\n =&gt; [2/3] COPY example.R example.R\nand then running the image gives the same result as before:\n[1] \"Hello from Docker!\"\n\n\nInstalling Dependencies\nYou’ll generally also need to install dependencies, which you can do using the RUN command. Let’s update the Dockerfile to install glue.\n## Dockerfile\n\nFROM rocker/r-ver:4.2.0\n\nCOPY example.R example.R\n\nRUN Rscript -e \"install.packages('glue')\"\n\nCMD [\"Rscript\", \"example.R\"]\nNow, the third step in the build installs glue. And to show it works, we’ll use glue to do a bit of string interpolation, printing the R version that’s running from the R_VERSION environment variable. Update example.R as follows:\n## example.R\n\nlibrary(glue)\n\nprint(glue('Hello from Docker! I am running R version {Sys.getenv(\"R_VERSION\")}'))\nBuilding and running again should give you some new output. First, you should see glue installing in the build logs:\n =&gt; [3/3] RUN Rscript -e \"install.packages('glue')\"\nAnd once you run the image, you should see:\nHello from Docker! I am running R version 4.2.0                                                                  \nWoohoo! 🥳🥳\n\n\nUsing renv\nBut as I wrote about in my last post, having global dependency installs is usually a bad idea. So we probably don’t want to have an install.packages() as a RUN step in the Dockerfile. Instead, let’s use renv to manage our dependencies.\nFrom the command line, run:\nRscript -e \"renv::init()\"\nSince you already are using glue in your project, this will generate a lockfile that looks something like this:\n{\n  \"R\": {\n    \"Version\": \"4.2.0\",\n    \"Repositories\": [\n      {\n        \"Name\": \"CRAN\",\n        \"URL\": \"https://cloud.r-project.org\"\n      }\n    ]\n  },\n  \"Packages\": {\n    \"glue\": {\n      \"Package\": \"glue\",\n      \"Version\": \"1.6.2\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"CRAN\",\n      \"Requirements\": [\n        \"R\",\n        \"methods\"\n      ],\n      \"Hash\": \"4f2596dfb05dac67b9dc558e5c6fba2e\"\n    },\n    \"renv\": {\n      \"Package\": \"renv\",\n      \"Version\": \"0.17.3\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"CRAN\",\n      \"Requirements\": [\n        \"utils\"\n      ],\n      \"Hash\": \"4543b8cd233ae25c6aba8548be9e747e\"\n    }\n  }\n}\n\n\n\n\n\n\nIt’s important to keep the version of R you’re running in your Docker containers in sync with what you have on local. I’m using 4.2.0 in my Docker image, which I defined with FROM rocker/r-ver:4.2.0, and that version is the same version that’s recorded in my renv.lock file. In Python, you might use a tool like pyenv for managing Python versions.\n\n\n\nNow that we have renv set up, we can update the Dockerfile a bit more:\n## Dockerfile\n\nFROM rocker/r-ver:4.2.0\n\nCOPY example.R example.R\nCOPY renv /renv\nCOPY .Rprofile .Rprofile\nCOPY renv.lock renv.lock\n\nRUN Rscript -e \"renv::restore()\"\n\nCMD [\"Rscript\", \"example.R\"]\nNow, we’re copying all of the renv scaffolding into the image. And instead of running install.packages(...), we’ve replaced that line with renv::restore() which will look at the lockfile and install packages as they’re defined. Rebuilding and running the image again will give you the same result as before.\nNow that we have a script running in Docker and are using renv to declare and install dependencies, let’s move on to…\n\n\nEnvironment Variables\nSometimes we need environment variables like a Github token or a database URL, either to install our dependencies or to run our code. Depending on when the variable will be used, we can either specify it at build time (as a build arg) or a run time. Generally, it’s a good idea to only specify build args that you really need at build time.\n\nBuild Time Config\nFor instance, if your build requires downloading a package from a private Github repository (for which you need to have a GITHUB_PAT set), then you would specify your GITHUB_PAT as a build arg). Let’s try that:\n## Dockerfile\n\nFROM rocker/r-ver:4.2.0\n\nARG GITHUB_PAT\n\n## Note: don't actually do this\n## It's just for the sake of example\nRUN echo \"$GITHUB_PAT\"\n\nCOPY example.R example.R\nCOPY renv /renv\nCOPY .Rprofile .Rprofile\nCOPY renv.lock renv.lock\n\nRUN Rscript -e \"renv::restore()\"\n\nCMD [\"Rscript\", \"example.R\"]\nNow, the second line adds a build arg using ARG. Next, run the build:\ndocker build --tag example --build-arg GITHUB_PAT=foobar .\nYou should see the following in the logs:\n =&gt; [2/7] RUN echo \"foobar\" \nThis means that your variable GITHUB_PAT has been successfully set, and can be used at build time for whatever it’s needed for.\n\n\n\n\n\n\nThis is just an example, but it’s important that you don’t expose secrets in your build like I’ve done here. If you’re using (e.g.) a token as a build arg, make sure it’s not printed in plain text to your logs.\n\n\n\n\n\nRuntime Config\nOther times, you want config to be available at container runtime. For instance, if you’re running a web app, you might not need to be able to connect to your production database when you’re building the image that houses your app. But you need to be able to connect when the container boots up. To achieve this, use --env (or --env-file). We’ll update our example.R a bit to show how this works.\n## example.R\n\nlibrary(glue)\n\nprint(glue('Github PAT: {Sys.getenv(\"GITHUB_PAT\")}'))\n\nprint(glue('Database URL: {Sys.getenv(\"DATABASE_URL\")}'))\n\nprint(glue('Hello from Docker! I am running R version {Sys.getenv(\"R_VERSION\")}.'))\nAnd then, let’s rebuild:\ndocker build --tag example --build-arg GITHUB_PAT=foobar .\nand now we’ll run our image, but this time with the --env flag:\ndocker run --env DATABASE_URL=loremipsum example\nThis tells Docker that you want to pass the environment variable DATABASE_URL=loremipsum into the container running your example image when the container boots up.\nAnd after running, you’ll see something like this:\nGithub PAT: \nDatabase URL: loremipsum\nHello from Docker! I am running R version 4.2.0.\nThere are a few things to note here.\n\nThe GITHUB_PAT that you set as a build arg is no longer accessible at runtime. It’s only accessible at build time.\nThe DATABASE_URL we provided with the --env flag is now accessible as an environment variable named DATABASE_URL\n\n\n\n\n\n\n\nVery often, container orchestration platforms like Heroku, Digital Ocean, AWS Batch, etc. will allow you to specify environment variables via their CLI or UI, which they will then inject into your container for you when it boots up."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-06-03-docker/docker.html#advanced-topics",
    "href": "posts/series/doing-data-science/2023-06-03-docker/docker.html#advanced-topics",
    "title": "A Gentle Introduction to Docker",
    "section": "Advanced Topics",
    "text": "Advanced Topics\nThis post is intended to be a gentle introduction to Docker, but there’s a lot that’s missing. I’d like to quickly address a couple more topics that have been helpful to me and our team as we’ve relied more and more heavily on Docker.\n\nCustom Base Images\nYou might have noticed that your builds take longer as they do more. For instance, the glue install, even though glue is extremely lightweight, takes a few seconds. If you have many dependencies (R dependencies, C dependencies, etc.), building an image for your project can get prohibitively slow. For us in the past, builds have taken over an hour just restoring the dependencies recorded in the lockfile.\nA convenient way around this is to install some “base” dependencies that you’ll update rarely and use often into a base image, which you then push to a repository like Docker Hub and then use as your base image in the FROM ... line of your Dockerfile for any particular project. This prevents you from needing to install the same, unchanged dependencies over and over again.\nWe’ve had a lot of success using this strategy on a few particular fronts:\n\nMaking sure we’re using the same version of R everywhere is simple if we define the R version in one place with a FROM rocker/r-ver:4.2.0 in our base image (which is called collegevine/r-prod-base), and then we use FROM collegevine/r-prod-base as the base image for all of our other Docker builds.\nInstalling Linux dependencies, such as curl, unzip, etc. which we’re happy keeping on a single version can happen once in the base image, and then every downstream image can rely on those same dependencies.\nInstalling CLIs like the AWS CLI, which again, really doesn’t need to happen on every build.\n\n\n\nCI / CD\nThe other time-saving strategy we’ve greatly benefited from is aggressive caching R packages in our CI / CD process. renv has great docs on using it within a CI / CD rig which I would highly recommend.\nAt a high level, what we do is renv::restore() in the CI itself (before running the docker build ...), which installs all of the packages our project needs. Then we COPY the cache of packages into our image, so that they’re available inside of the image. This means we don’t need to reinstall every dependency on every build, and has probably sped up our image build times by 100x."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-06-03-docker/docker.html#wrapping-up",
    "href": "posts/series/doing-data-science/2023-06-03-docker/docker.html#wrapping-up",
    "title": "A Gentle Introduction to Docker",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nI hope this post has demystified Docker a bit and helped clarify some of the basics of Docker and how it’s used. At the highest level, Docker lets you package up your code so that it can be run anywhere, whether that’s on your machine, on the machine of a coworker, in a CI/CD tool, on a cloud server like an EC2 box, or anywhere else. All you need to do is build and push your image!"
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-10-running-a-b-tests/index.html",
    "href": "posts/series/a-b-testing/2022-04-10-running-a-b-tests/index.html",
    "title": "Running A/B Tests",
    "section": "",
    "text": "This is the second post in a series on A/B testing. In the last post, I gave a high-level overview of what A/B testing is and why we might do it. This post will go a few steps farther. I’ll discuss how an A/B test is run, what we’re looking for along the way, and what happens when we call it one way or the other. This will set us up for the next post, which will discuss the mechanics of A/B testing."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-10-running-a-b-tests/index.html#how-can-i-run-an-ab-test",
    "href": "posts/series/a-b-testing/2022-04-10-running-a-b-tests/index.html#how-can-i-run-an-ab-test",
    "title": "Running A/B Tests",
    "section": "How Can I Run An A/B Test?",
    "text": "How Can I Run An A/B Test?\nIn the last post, I laid out a hypothetical A/B test where I was considering changing the underline color for links on my blog from red to blue. As a refresher: Blue was the variant (the new proposed color) and red was the control (the current state of the world). We were testing to see if a blue underline would cause significantly more users to click the links to my blog posts. “But,” you ask, “how does the test actually happen?” That’s a great question! But first, a disclaimer: I’m not an engineer, so I can only give a bird’s eye view of how we do it at CollegeVine. I’m sure there are many other solutions used by other companies.\nAt CV, we use a tool called LaunchDarkly for running A/B tests. Essentially, LaunchDarkly lets us set up “feature flags” and show the version of the code that’s “behind” them to only certain users. For example, you might imagine you were rolling out a risky new change, and wanted to QA it first. One way we’ve done this kind of thing at CV is to put the risky change behind a feature flag, and then roll it out to our own team. Then, our team can QA and if anything looks off we can either fix the issues or revert the changes before rolling out to external users.\nA/B testing with LD works similarly. Instead of only showing a new version of the code to internal users, we use a feature flag that shows each version of the code to a certain proportion of users, at random. The idea is to use the feature flag in LD to randomly sample users of our site into either the control group or the variant group. Then we track metrics over time to see if the variant is outperforming the control group."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-10-running-a-b-tests/index.html#my-experiment-is-running.-now-what",
    "href": "posts/series/a-b-testing/2022-04-10-running-a-b-tests/index.html#my-experiment-is-running.-now-what",
    "title": "Running A/B Tests",
    "section": "My Experiment Is Running. Now What?",
    "text": "My Experiment Is Running. Now What?\nBack to our hypothetical experiment on my blog. Now, half of users are seeing red-underlined links, and half are seeing blue underlines, at random. So now, we need a way to track the conversion rate of those links. In step a whole bunch of business intelligence (BI) tools, and other tools that brand themselves as being tools for all different flavors of analytics. At CV, we use a tool called Heap for user analytics (including A/B testing).\nLet’s imagine that my blog were wired up to Heap, and tracking page views on my landing page and clicks of the links on that page to individual posts behind the scenes. In Heap, we could visualize the conversion rate from the landing page to any post in a funnel or in a table, where the conversion rate between the two is the proportion of users who hit the landing page that end up clicking one of the links (“converting”) to a post. We could also view these numbers in a table, where we’d have one cell that has the total number of sessions on the landing page and another cell with the number of sessions on the posts, and the conversion rate is the latter divided by the former (roughly).\nSince we have our feature flag set up to track which users are being placed in each group, we can also group by that “property” in Heap, which lets us separate our analysis into the control and the variant. This means that we can compare the conversion rates for the red underline and the blue underline, which is exactly what we’re trying to do! Generally, we’ll set up a Heap dashboard with the funnel we’re interested in so we can track out metrics over time."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-10-running-a-b-tests/index.html#interpreting-the-metrics",
    "href": "posts/series/a-b-testing/2022-04-10-running-a-b-tests/index.html#interpreting-the-metrics",
    "title": "Running A/B Tests",
    "section": "Interpreting the Metrics",
    "text": "Interpreting the Metrics\nNow that the funnel is set up, you’re watching results stream in. Let’s imagine that at some point in time, each group has 1000 users (i.e. 1000 users have seen the variant and another 1000 have seen the control), and 250 users in the variant group converted while only 200 in the control group did. From there, we can calculate our conversion rates as 25% (variant) and 20% (control). And for the purposes of keeping this post simple, let’s assume that lift is big enough for us (by some definition of “big enough”, which we’ll get to in a later post). In that case, we call our test for the variant. In practice, this means we route all traffic to the variant instead of splitting it 50/50, and then we can remove the feature flag from our code and boom! We now have some cool blue underlines for the links on the blog.\nBut back to the lift being big enough: In practice, is knowing that the variant is performing 25% better than the control enough to call the test for the variant? Making this call in a rigorous, informed way is what the rest of the posts in this series will discuss."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-10-calling-a-b-tests/index.html",
    "href": "posts/series/a-b-testing/2022-04-10-calling-a-b-tests/index.html",
    "title": "Calling A/B Tests",
    "section": "",
    "text": "In the last post, I gave a bird’s eye level overview of the mechanics of running an A/B test. But at the end, we reached a problem: We had two conversion rates – 20% and 25% – but we didn’t know if the difference between those was really big enough to make a strong claim that the blue underlines were actually performing better than the red ones in some real world sense. If you’re asking yourself whether the five percentage point difference between the two conversion rates is statistically significant, then your head’s in the right place.\nIn this post, we’ll discuss how we can determine whether our test results are statistically significant. But since statistical significance is an often confusing and nebulous topic, we’ll also explore what statistical significance even is (including what p-values are), when it’s important, and when it might not be."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-10-calling-a-b-tests/index.html#statistical-significance",
    "href": "posts/series/a-b-testing/2022-04-10-calling-a-b-tests/index.html#statistical-significance",
    "title": "Calling A/B Tests",
    "section": "Statistical Significance",
    "text": "Statistical Significance\nMisunderstandings about statistical significance run rampant. It’s not a reach for me to say that the majority of the time I hear someone mention that something is “statistically significant” I end up rolling my eyes. But before we get into common mistakes and misunderstandings, we need to first establish what statistical significance actually is.\nIntuitively, if something is statistically significant, it’s unlikely to have happened due to random chance. Not that scary, after all! How unlikely, though, varies wildly depending on the setting. For instance, if we’re running clinical trials to determine if a new drug is capable of curing cancer, then we want it to be very unlikely that we make a consequential mistake and claim that the drug works when it actually doesn’t.\nWe use p-values as the indicator of the likelihood of our result being due to random chance. In this instance, we would run our test using the number of page views and the number of conversions for each group, and depending on how we ran our test we might get a p-value of 0.43% back. What this p-value actually means is that the probability of seeing the difference in conversion rates between groups that we do (five percentage points) due to purely random chance is 0.43%. A p-value threshold of 5% is very common, so in this case we’d call the test for the variant (since 0.43% is below 5%), and we’d assert that this difference in conversion rates is statistically significant."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-10-calling-a-b-tests/index.html#eye-rolling",
    "href": "posts/series/a-b-testing/2022-04-10-calling-a-b-tests/index.html#eye-rolling",
    "title": "Calling A/B Tests",
    "section": "Eye Rolling",
    "text": "Eye Rolling\nBack to my eye rolling: I often roll my eyes when someone claims that something is statistically significant for two reasons.\nFirst and foremost: Something being statistically significant does not mean that thing is significant. Often we get so hung up on things being statistically significant that we forget that lifting some metric by 0.0001% isn’t practically significant, since it won’t make any difference in the end. If 0.0001% more people read my blog posts, what do I care? That’s something like 1 extra person every hundred years (optimistically).\nSecondly, I often roll my eyes because of the number of choices and assumptions that need to be made along the way, many of which tend to be difficult to defend. One choice, as previously mentioned, is the p-value threshold (alpha) that you choose. In some instances, we want to be very confident that we’re not leaning into results that are the result of random chance, and so we might use a lower threshold. In other cases, we might be okay with taking on more risk of a false positive result in order to run our tests faster and mitigate the risk of a false negative (saying something does not help when it actually does).\nAnother thing that will affect the results we see is the type of test we’re running: one-tailed or two-tailed. Often, online calculators like this one will use two-tailed tests by default because they’re more conservative. But in my opinion, using a two-tailed test doesn’t actually make any sense. Here’s why: A two-tailed test checks if the conversion rates of the variant and the control are not equal, which means that we can get a statistically significant result if the variant is significantly worse than the control, in addition to if it’s significantly better. But in A/B testing, we’re only going to call a test for the variant when it’s significantly better, so why do we care about the case where it’s worse? We want to test the hypothesis that the variant is significantly better than the control, not that it’s not equal, and that’s what a one-tailed test does. If you use two-tailed tests, it’ll be harder to get significant results without any real benefits.\n\nYet another consideration is how the statistical test was actually conducted. For instance, if you use a Chi-square test with Yates’s continuity correction (the default in R, although a little controversial among statisticians), you’ll end up with higher (more conservative) p-values than if you don’t correct, which is why the p-value I just reported is higher than the one you’d get from most online calculators that don’t use the correction.\n\nFinally, and most importantly, is that the mechanics of running the test actually affect the chance that you are reporting a false positive result. For example, if you were to run the test described in the past few posts and calculate the p-values every time a new user visited the page and call the test for the variant the first time it were significant, you’d have just blown up the chances of a false positive result."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-10-calling-a-b-tests/index.html#a-common-mistake",
    "href": "posts/series/a-b-testing/2022-04-10-calling-a-b-tests/index.html#a-common-mistake",
    "title": "Calling A/B Tests",
    "section": "A Common Mistake",
    "text": "A Common Mistake\nThe most common mistake I see that’s made by people running A/B tests is using the “call it when it’s significant” heuristic. As I mentioned before, checking in on your test often and calling it for the variant the first time you get a significant p-value is a huge problem because the false positive rate of your test compounds the more you check on it. The reason for this is a statistical concept called multiple testing, and there’s an XKCD comic about it!\nSo we want to avoid checking the test all the time, but this raises another problem: If we can’t check our test all the time, how do we know when to call it? And this is where test planning comes in. There are a number of online test planners (which generally make shoddy assumptions, like that you’re running a two-tailed test when you should be running a one-tailed one instead) like this one that take a few parameters and tell you how long to run your test for. And these planners are great! The idea is that if you can plan your test in advance, given that you know your baseline conversion rate and can specify how big of a lift you’re shooting for, then all you have to do is wait until you hit the sample size number that the calculator gives you back. Once you hit it, you check in on your test, run your p-value calculation, and call the test.\nSo, problem solved, right? Well, not quite. Because while we’ve solved the multiple testing problem where we blow up our false positive rate by checking the test all the time, now we have a new issue: We have to wait until we hit some (potentially big) sample size before we can call our test, and that’s problematic for teams that want to iterate quickly.\nThe next post in this series is the punch line. It’ll discuss sequential testing, which is the methodology that makes up the guts of how we run A/B tests at CollegeVine. Sequential testing solves the problem of needing to wait until you hit a final sample size to call your test without making any sacrifices on the rigor front, which means you can call your tests quickly and reliably."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-03-25-a-b-testing-a-primer/index.html",
    "href": "posts/series/a-b-testing/2022-03-25-a-b-testing-a-primer/index.html",
    "title": "A/B Testing: A Primer",
    "section": "",
    "text": "This is the first post in a series I’m planning on writing on A/B testing. In this post, I’ll lay out a top-level overview of what A/B testing is and why companies do it. In future posts, I plan on diving into some common pitfalls, bad habits, and anti-patterns I’ve seen, and the systems we’ve put in place to mitigate them and allow our team to run statistically rigorous, fast A/B tests to make informed product decisions as quickly as possible.\nAt work, we generally try to keep documents like this written at a high level: The objective is for them to be understandable and useful for general audience. That will be the case here too, for the most part."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-03-25-a-b-testing-a-primer/index.html#whats-an-ab-test",
    "href": "posts/series/a-b-testing/2022-03-25-a-b-testing-a-primer/index.html#whats-an-ab-test",
    "title": "A/B Testing: A Primer",
    "section": "What’s an A/B Test?",
    "text": "What’s an A/B Test?\nSo, what is an A/B test, anyways? It’s probably easiest to explain with an example:\nLet’s imagine that I had been tracking the click rate on my blog posts over time. It’s pretty terrible – let’s say that the rate that someone clicks into any particular post from the main menu page is 5%. This means that of all of the views of my blog’s main page, only 5% of those page views actually result in a click on one of my posts. Pretty miserable, right?\nBut today I’m feeling optimistic. Right now, when a user hovers over a post title, it gets underlined in red. “But wait!” I think. What would happen if I made the underline blue instead?\nAnd now, I have an A/B test. In this test, the “A” group (or the “control”) is the current state of the world: The red underline. The “B” group (or the “variant” or “treatment” group) is the proposed change: The blue underline.\nThe basic idea of an A/B test is to run these two versions of my blog side-by-side, measuring the click rate in each version, and seeing which version ends up performing better. If the blue underline version – the variant – ends up increasing the click rate to my blog posts, then the conclusion is that I’d be better off permanently changing the underline to blue."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-03-25-a-b-testing-a-primer/index.html#why-test",
    "href": "posts/series/a-b-testing/2022-03-25-a-b-testing-a-primer/index.html#why-test",
    "title": "A/B Testing: A Primer",
    "section": "Why Test?",
    "text": "Why Test?\nIn my trivial example above, the color of the underline doesn’t seem super consequential (and it’s not). But this isn’t always the case. For instance, Facebook changed their notification icon color from blue to red once upon a time, and the rest was history. Amazon might A/B test a new model for recommending products to users, or Netflix a new model for recommending shows. A company doing lots of email marketing might A/B test different types of ways of addressing their emails (e.g. “Dear Matt” vs. “Hey Matt”), and so, so much more. Changes like these can have enormous business implications, and, as such, A/B testing makes up the backbone of so much of the tech and products we interface with every day. Companies want to maximize their conversion rates, click rates, revenues, etc. and A/B testing is one tool in their tool box for optimizing all of the metrics they care about.\nIf there’s one takeaway here, it’s this: Someone wants to make their product “better” in some sense, and to figure out whether or not a new idea of theirs is actually better than the current state of the world, they test it.\n\nIn statistics world, generally A/B tests boil down to testing “conversion rates” against each other, which usually means that the tests being run are Chi-square tests of independence of the proportions of success across the two groups. If the variant is significantly better than the control, we call the test for the variant and roll it out to 100% of users. You might also use a t-test to (e.g.) test if the variant results in significantly more sessions than the control does, or you might use a time series technique like Bayesian structural time series to do pre/post testing to compare user behavior before and after a treatment was applied. For the curious, Google has published an awesome R package called CausalImpact (and an associated talk and some papers, I believe) on this."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-03-25-a-b-testing-a-primer/index.html#up-next",
    "href": "posts/series/a-b-testing/2022-03-25-a-b-testing-a-primer/index.html#up-next",
    "title": "A/B Testing: A Primer",
    "section": "Up Next…",
    "text": "Up Next…\nAs I mentioned before, the rest of this series of posts will focus, roughly, on the following topics: 1. Okay, so we know what an A/B test is, but how do we actually run one? 2. What are the most common anti-patterns, pitfalls, and bad habits that I’ve seen, and why are they problematic? 3. What are we doing to correct those issues to allow our team to run fast, statistically rigorous A/B tests?"
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-17-sequential-testing/index.html",
    "href": "posts/series/a-b-testing/2022-04-17-sequential-testing/index.html",
    "title": "Sequential Testing",
    "section": "",
    "text": "The last post proposed a solution to the multiple testing problem that often invalidates A/B test results test planning. The idea is to calculate the sample sizes you need for your test in advance, and then wait for your control and variant groups to hit those sample sizes in order to call the test. This approach is a significant methodological improvement from the “call it when it’s significant” heuristic: It prevents you from compounding the false positive rate of your test by checking on it all the time.\nBut there’s a different issue with planning the test in advance and running it until the end: It’s slow. I use “slow” to mean “slower than it needs to be,” in the sense that you will likely end up waiting too long to call a test for the variant when you could’ve made the call earlier. This waiting around is expensive – the difference between running a test for a day or two and a week or two matters a lot for the teams and businesses running the tests. Often, this big of a time difference can have massive effects on metrics, revenue, learnings, etc., so teams benefit from being able to call their tests faster without sacrificing any statistical rigor.\nBut how do we do that without checking in on the test all the time? Enter sequential testing."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-17-sequential-testing/index.html#introduction",
    "href": "posts/series/a-b-testing/2022-04-17-sequential-testing/index.html#introduction",
    "title": "Sequential Testing",
    "section": "",
    "text": "The last post proposed a solution to the multiple testing problem that often invalidates A/B test results test planning. The idea is to calculate the sample sizes you need for your test in advance, and then wait for your control and variant groups to hit those sample sizes in order to call the test. This approach is a significant methodological improvement from the “call it when it’s significant” heuristic: It prevents you from compounding the false positive rate of your test by checking on it all the time.\nBut there’s a different issue with planning the test in advance and running it until the end: It’s slow. I use “slow” to mean “slower than it needs to be,” in the sense that you will likely end up waiting too long to call a test for the variant when you could’ve made the call earlier. This waiting around is expensive – the difference between running a test for a day or two and a week or two matters a lot for the teams and businesses running the tests. Often, this big of a time difference can have massive effects on metrics, revenue, learnings, etc., so teams benefit from being able to call their tests faster without sacrificing any statistical rigor.\nBut how do we do that without checking in on the test all the time? Enter sequential testing."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-17-sequential-testing/index.html#sequential-testing",
    "href": "posts/series/a-b-testing/2022-04-17-sequential-testing/index.html#sequential-testing",
    "title": "Sequential Testing",
    "section": "Sequential Testing",
    "text": "Sequential Testing\nSequential testing is a method for running experiments that allows us to evaluate the results of the test we’re running along the way instead of waiting to hit a pre-determined sample size. Intuitively, you might think about sequential testing like this: If early on in my test I see a massive lift in my metric, I should be able to use a lower p-value than the one I set at the start of my test to call it. It’s earlier, hence the lower p-value, but the intuitive idea is that the metric lift is so big that the p-value we’d see would be smaller than some yet undetermined p-value threshold, such that we could call the test.\nIn A/B testing world, this boils down to building checkpoints into our tests. For instance, imagine you have a test that you’re expecting to take six days to hit the final sample size that you need. If you build in three checkpoints, then you can check in on your test on day two, day four, and day six (the end of the test). On day two, if the p-value for your test is lower than the pre-determined day two p-value needed, you call the test. If it’s not, you move on to day four and repeat. Once you get to day six, if the test is still insignificant you call it for the control and end the test.\nThis gives us the best of both worlds: We have a setup where we can call the test on day two if the lift is big enough, but we can do so without inflating the false positive rate of our test. In practice, this means often being able to call tests in half, a third, a quarter, etc. of the time it’d otherwise take, which is hugely valuable for the team running the test.\n\nStatistical note: There are a number of ways to determine what p-value to use at each checkpoint when planning the test. We use the R package rpact for planning tests, and we plan our tests using the O’Brien-Fleming method (with alpha spending). This results in p-value thresholds that increase over time and asymptote to a value slightly less than the initial alpha you specified, depending on the number of checkpoints you build into your test. Another popular method is Pocock’s approach."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-17-sequential-testing/index.html#in-practice",
    "href": "posts/series/a-b-testing/2022-04-17-sequential-testing/index.html#in-practice",
    "title": "Sequential Testing",
    "section": "In practice",
    "text": "In practice\nSo how does this work in practice? We build an internal tool that lets you plan a test given a few inputs:\n\nThe alpha level (we generally use 20%, since we’re not particularly afraid of false positives and want to be able to run tests quickly)\nThe power (we generally use 95%, since we don’t want to take on many false negatives)\nThe minimum detectable effect\nThe baseline conversion rate\nThe expected number of users entering the funnel per day\nThe number of checkpoints to build in\nThe split of the test (is it 50/50?)\nThe number of variants (is it a true A/B test? Are there multiple variants being tested?)\n\nWith those inputs, we generate a test plan which you can save, tie to a JIRA card, and send to Slack. Then all you need to do is turn on your test and wait for it to hit the first checkpoint. Once it does, you evaluate the test to get a p-value, compare it to the p-value threshold that the test plan provided at the first checkpoint, and call the test if it’s significant. If it’s not, you keep running the test up to the next checkpoint and do the same thing, and so on."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-17-sequential-testing/index.html#the-bottom-line",
    "href": "posts/series/a-b-testing/2022-04-17-sequential-testing/index.html#the-bottom-line",
    "title": "Sequential Testing",
    "section": "The Bottom Line",
    "text": "The Bottom Line\nThe main takeaway from this post is that sequential testing lets us solve two huge problems in A/B testing simultaneously: It lets us run our tests fast, and it lets us do it without sacrificing any statistical rigor. Too often, I see teams committing atrocities against statistics in the name of moving fast when they don’t need to be – using sequential designs for your A/B tests lets you control the false positive and false negative rates of your A/B tests while also allowing you to make calls on those tests as quickly as possible, which is hugely valuable.\nAnd with that, we’ve concluded a four-part series on A/B testing! Hopefully you found this interesting and useful, and have taken something away that will be beneficial for your own work. Or, if I’m lucky, maybe you’re even considering overhauling how you run A/B tests."
  },
  {
    "objectID": "posts/2021-12-22-notes-on-hiring-data-analysts-scientists/index.html",
    "href": "posts/2021-12-22-notes-on-hiring-data-analysts-scientists/index.html",
    "title": "Notes on Hiring Data Analysts + Scientists",
    "section": "",
    "text": "In the past four months, I’ve been involved in hiring for two new roles at CollegeVine: a second data scientist, and our first data analyst. I’ve learned a lot along the way: Things that work, things that don’t, and things to ask in order to maximize the amount of signal we’re getting from our interviews. This post will sketch out our hiring process (the processes are very similar for our DS and DA roles, with slightly different questions), and I’ll add some notes about things I’ve learned along the way. It’s been a long time since I’ve written anything! I’m excited, so here goes."
  },
  {
    "objectID": "posts/2021-12-22-notes-on-hiring-data-analysts-scientists/index.html#introduction",
    "href": "posts/2021-12-22-notes-on-hiring-data-analysts-scientists/index.html#introduction",
    "title": "Notes on Hiring Data Analysts + Scientists",
    "section": "",
    "text": "In the past four months, I’ve been involved in hiring for two new roles at CollegeVine: a second data scientist, and our first data analyst. I’ve learned a lot along the way: Things that work, things that don’t, and things to ask in order to maximize the amount of signal we’re getting from our interviews. This post will sketch out our hiring process (the processes are very similar for our DS and DA roles, with slightly different questions), and I’ll add some notes about things I’ve learned along the way. It’s been a long time since I’ve written anything! I’m excited, so here goes."
  },
  {
    "objectID": "posts/2021-12-22-notes-on-hiring-data-analysts-scientists/index.html#our-process",
    "href": "posts/2021-12-22-notes-on-hiring-data-analysts-scientists/index.html#our-process",
    "title": "Notes on Hiring Data Analysts + Scientists",
    "section": "Our Process",
    "text": "Our Process\nOur hiring process only has a few steps. We try to keep things simple and quick:\n\nA phone screen with the hiring manager (30-45 minutes)\nA technical interview with two data scientists (60-90 minutes)\nA technical interview with two software developers (60 minutes, data science only)\nTwo cultural / behavioral interviews\n\nSince my portion of the process is the data science part, I’ll leave the phone screens, behavioral rounds, and system design round out of this post."
  },
  {
    "objectID": "posts/2021-12-22-notes-on-hiring-data-analysts-scientists/index.html#the-data-science-analysis-interview",
    "href": "posts/2021-12-22-notes-on-hiring-data-analysts-scientists/index.html#the-data-science-analysis-interview",
    "title": "Notes on Hiring Data Analysts + Scientists",
    "section": "The Data Science / Analysis Interview",
    "text": "The Data Science / Analysis Interview\nI’ve done a whole bunch of data science interviews from both sides of the table. Some have been better than others. Often, while on the hot seat, I’ve gotten a smorgasborg of questions that felt like they came from the first page of the first Google result for “Data science interview questions.” A handful of examples:\n\nWhat’s the difference between supervised and unsupervised learning?\nWhat’s the difference between linear regression and logistic regression?\nWhat’s a p-value?\nHow do you know when something is an outlier?\n\nAnd many, many more. In my view, these questions are fine. They ask about a relevant concept to data science and put your skills to the test. But I have two issues with them. First, they’re neither challenging nor questions that are easy to build on in terms of difficulty. Second, they’re not going to be good questions to figure out someone’s ability level.\n\nCreating Challenging Questions\nWhen I say the questions above aren’t challenging, I mean that these are the kinds of questions that someone who’s taken a single stats or machine learning class would be able to answer. This is fine if you’re trying to figure out if someone has taken one of said classes, but that isn’t our goal. We’re trying to determine your ability to succeed as a data scientist or analyst. This means that we need to know more than just your academic background: We need to know how you reason about stats and ML, and how you think in general. How do you approach a statistics problem where you haven’t seen the solution before? Can you intuit an answer and defend it?\nAs a result, we’ve designed our questions to be challenging enough that you wouldn’t have seen them in a class you took, and to build off of one another. For example, we often ask the following question:\n\nImagine you’re looking at a distribution of the heights of men in Spain. How might you find which men are outliers?\n\nSure, easy question. There are a handful of fine answers here, but we’re generally looking for something along the lines of using a Z-Score or the IQR. Either of those shows that you’ve seen some rule of thumb in a statistics class and realize that you can apply it to this problem.\nBut then, we ask a follow-up:\n\nNow, let’s imagine you’re looking at the distribution of incomes of men in Spain, instead of their heights. Does your approach change at all?\n\nThis is a question that most candidates need to think about for a little bit. At first, it seems simple. But we give you a hint: We explicitly ask if your approach to the problem would change, which is a push to think about how it might change. Many candidates struggle with this question, seemingly for a few reasons:\n\nThey understand the hint, but don’t immediately realize why the Z-Score or the IQR approach breaks down, so they feel stuck.\nThey understand why those approaches don’t work, but it doesn’t jump out at them what they should do about it.\n\nThese types of responses aren’t surprising to us: In statistics classes, you normally work with normally distributed data where nice properties and rules of thumb hold up, but now we have a problem: Everyone knows that incomes are skewed, and so now what do we do? Some candidates stick to their guns and insist the IQR or Z-score would still be fine. Here’s how I’d answer the question:\n\nFirst, I’d make a point that incomes are right-skewed. Everyone probably has this image in their head already, but it’s important.\nNext, I’d note that the IQR / Z-score approach would break down, since the rules of thumb we use about 95% of the data (e.g.) lying within 2 standard deviations of the mean only works on a normal distribution, which incomes do not follow. This means we can’t just arbitrarly say “Oh, this person is more than 2 standard deviations from the mean, so he’s an outlier!” anymore.\nFinally, I’d think about other approaches. One might be something fancy like an isolation forest, but I think there’s a simpler approach that’d work: Since incomes are severely right skewed, we could try taking the log of the incomes to see if the logged income looks roughly normally distributed. If it does, we can fall back on our IQR or Z-score approach.\n\nThe point here is to ask the candidate a question that makes them think intuitively about having to solve a real-world problem (in this case, one we face all the time) that they probably haven’t seen before, which gives us a lot of signal about their statistical foundations and intuitions.\nWe follow up this follow-up with another:\n\nNow, let’s imagine we’re looking for outliers in the heights of men again, but this time they’re from Sweden and Spain. Does your approach change?\n\nSimilar question here, with no real clear, immediate answer that jumps out at most candidates. A reasonable response would be to just separate the distributions into Swedish and Spanish (since we know that Swedes are taller than Spaniards, on average), and then fall back on the Z-score or IQR approach again. Again, not a particularly challenging, in the weeds, or niche technical question by any stretch, but definitely one that lets us really get a sense for your intuitive ability.\n\nTL;DR: We build questions that aren’t tricky (in the trick question sense), but should’t have an immediately obvious canned answer. These types of questions should give us a window into how you reason intuitvely about statistics and machine learning concepts at all levels of complexity.\n\n\n\nLayering Questions\nThis point is a nice segue into the second issue I noted above: It’s important to build questions that have multiple layers of difficulty to them, so that if someone immediately answers your question, you don’t completely shift gears and move to a different topic. Instead, we want to keep digging, so that we can figure out just how much you know. The question I laid out above is a good example of a simple, relatively straightforward question with multiple layers.\nHere’s another example:\n\nImagine you’re asking people their symptoms and trying to figure out if they have COVID or not. Sometimes you’ll say someone has COVID when they don’t, and sometimes you’ll say they don’t when they do. What are these two types of mistakes called? And which one do you think is worse? Why?\n\nThis is a question about Type I and Type II error (also known as false positives and false negatives, respectively). Most candidates realize this right away, and then make an argument for why they think a Type II error (the false negative) is a worse mistake. Generally, the argument centers on someone who is infected unknowingly spreading COVID. That’s a great answer. It shows that they can reason about different types of mistakes and can make an argument for why we might want to minimize one or the other. But this isn’t a particularly challenging question.\nWe ask a follow-up:\n\nNow, let’s imagine you get some COVID test results back from the people whose symptoms you were asking about. What’s wrong with this statement: ‘If COVID tests have a 10% false negative rate and you got a COVID test and it’s negative, that means there’s a 10% chance it’s wrong and you’re actually positive.’?\n\nThis question is a little more challenging, and it builds of the types of error we discussed in the previous question. Here, you need to realize what a false negative is, and it’s easy to get the conditioning flipped. In this case, the false negative rate of the test being 10% means that the probability that you test negative given that you have COVID is 10%. This is not the same as saying that the probability that you have COVID given that you test negative is 10%. In the second case, the conditioning is flipped around backwards. Most candidates get hung up on this, and rightfully so. It’s a tricky question to work out without pen and paper.\nFor those that get through that question, we have another, more difficult follow-up:\n\nOkay, so you got a negative test result, but you know the false negative rate of COVID tests is 10%. Imagine you wanted to calculate the actual chance you were positive given that you just got your negative result. What other information would you need to do your calculation?\n\nFor the Bayesians in the back, this question should be a slam dunk. It’s an obvious Bayes’ Rule question: we’re asking you to calculate p( COVID | negative test ), so you can use Bayes’ Rule to find the answer. It turns out the other information you’d need to do this calculation (in addition to the false negative rate) are a true negative rate and a prior, and you’re golden.\nLastly, once a candidate successfully identified Bayes’ Rule and (hopefully) discussed priors, we’d ask them how they’d elicit this prior. There’s no “right” answer here, but there are a couple options that are better than others:\n\nAsk an expert\nUse something relatively uninformative\nTake an “empirical” approach and use something like the overall positive test rate\n\nAny of these answers would be totally reasonable, given that there’s a whole literature on prior elicitation.\nAnd that’s one example of a multi-layer question we might ask in a data science interview. The vast majority of candidates won’t get through all the parts, and that’s totally fine! Errr, maybe “fine” isn’t actually the right word: That’s the goal. The point is that we’re constructing a question that lets us learn a lot about you: We learn how much you know about Type I / II error and how you reason about them. We learn about if you understand conditional probability and Bayes’ Rule. And we learn how you reason about prior elicitation. We also learn how you argue for decisions you make, and how you communicate complicated statistical concepts (Bayes’ Rule and Type I / II error aren’t simple). And finally, we learn something about the depth of your knowledge. The first part of this question you’d probably know the answer to if you’d taken an introductory statistics or ML class. The second part you’d likely see in a probability course or an inferential statistics course. The third part would also probably come up in one of those two courses, and if not, then certainly in a Bayesian statistics course. And finally, the fourth part you’d likely only see in a very advanced inferential statistics course or a Bayesian course. So not only do we see how you reason about statistics, how much you know, and how you communicate difficult concepts, we also learn something about the types of problems you’ve seen or worked on in the past, whether they be in classes or in a former job or internship. This is a lot of useful information."
  },
  {
    "objectID": "posts/2021-12-22-notes-on-hiring-data-analysts-scientists/index.html#the-take-home",
    "href": "posts/2021-12-22-notes-on-hiring-data-analysts-scientists/index.html#the-take-home",
    "title": "Notes on Hiring Data Analysts + Scientists",
    "section": "The Take Home",
    "text": "The Take Home\nThe other piece to our interview process is a take home project. For data science, we have a take-home that you can find in our CollegeVine DS Hiring Github repo. For data analysts, we ask them to bring in any project they’ve done in the past: Anything from a homework assignment to another company’s take home.\nWe spend about 25-30 minutes going through the take home project, and we’re looking for a few main things. For the most part, candidates are good at describing what they’ve done, so we’re generally trying to dig to figure out why they made the decisions they did. Can they defend them? How did they think through the pros and cons of each decision? What’s their thought process like? The idea here is that everyone in a data-related role will need to make decisions where there’s no clear “right” answer, so we want to see why you chose to make a pie chart instead of a box plot, or chose to use XGBoost instead of logistic regression. Can you talk me through the pros and cons of each option?\nIn the data science take home we give, there are also some tricks we’re trying to test the candidates out on. In an effort to not give away our whole bag of tricks, I invite everyone to give the exercise a shot! I’d even be happy to take a look through your projects to see what you come up with. The short of it is that there are some issues in the data that, if you don’t notice them and do something to fix them, will end up ruining the predictions your model produces. We don’t expect anyone to catch all of these issues in the short amount of time we ask them to spend on the problem, but we hope that they’ll be thoughtful about how they’d solve them once we point them out in the interview.\nFinally, there’s one elephant in the room here that’s important to address: Many people feel negatively about take homes. So do we – they tend to be long and unnecessarily complicated, and often feel like a waste of the candidate’s time. In our view, though, the take home is necessary for one main reason: It lets you familiarize yourself with a data science problem beforehand, so that when you get to the interview we can hit the ground running. In a sense, we’re giving you the rules of the game in advance so we can start playing right away. This lets us avoid any confusion or annoying ambiguity with regards to the types of problems were asking about, and to entirely avoid requiring candidates to reason through questions about entirely hypothetical problems that they’ve never seen before. For these reasons, and also because there’s historically been lots of signal that we’ve gotten from how candidates respond to our questions about the project they bring in, we’ve decided to stick with the take home. In addition, we don’t screen out any candidates on the basis of their take home. We don’t ask them to be submitted in advance, so every candidate who does a take home gets an interview."
  },
  {
    "objectID": "posts/2021-12-22-notes-on-hiring-data-analysts-scientists/index.html#wrapping-up",
    "href": "posts/2021-12-22-notes-on-hiring-data-analysts-scientists/index.html#wrapping-up",
    "title": "Notes on Hiring Data Analysts + Scientists",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nIn a nutshell, that’s our whole data scientific technical interview! We discuss a take home you bring in, and then we talk through some miscellaneous statistics and machine learning questions like the ones we discussed above. In general, we’re not looking for any specific expertise or knowledge – we’re a start up, after all. Instead, we’re testing candidates to see how they reason about problems that are similar to the ones they’ll work on at CollegeVine, and how they explain the ins and outs of different possible solutions to those problems. We’re looking at their intuitions about statistics and machine learning, their ability to think on their feet when faced with questions they don’t immediately know the answer to, and their curiosity and creativity when it comes to solving challenging questions. At the end of the day, it’s these traits, not any hyper-specific technical knowledge, that’ll make for a great CV data team member."
  },
  {
    "objectID": "posts/2021-06-08-working-with-your-fitbit-data-in-r/index.html#introduction",
    "href": "posts/2021-06-08-working-with-your-fitbit-data-in-r/index.html#introduction",
    "title": "Working With Your Fitbit Data in R",
    "section": "Introduction",
    "text": "Introduction\nfitbitr 0.1.0 is now available on CRAN! You can install it with\ninstall.packages(\"fitbitr\")\nor you can get the latest dev version with\n## install.packages(\"devtools\")\ndevtools::install_github(\"mrkaye97/fitbitr\")\nfitbitr makes it easy to pull your Fitbit data into R and use it for whatever interests you: personal projects, visualization, medical purposes, etc.\nThis post shows how you might use fitbitr to pull and visualize some of your data."
  },
  {
    "objectID": "posts/2021-06-08-working-with-your-fitbit-data-in-r/index.html#sleep",
    "href": "posts/2021-06-08-working-with-your-fitbit-data-in-r/index.html#sleep",
    "title": "Working With Your Fitbit Data in R",
    "section": "Sleep",
    "text": "Sleep\nFirst, you should either generate a new token with generate_token() or load a cached token with load_cached_token().\n\nlibrary(fitbitr)\nlibrary(lubridate)\nlibrary(tidyverse)\n\n## Dates to use throughout post\nstart &lt;- as_date(\"2020-01-01\")\nend &lt;- as_date(\"2021-10-18\")\n\ngenerate_fitbitr_token()\n\nAnd then you can start pulling your data!\n\nsleep &lt;- get_sleep_summary(\n  start_date = end - months(3),\n  end_date = end\n)\n\nhead(sleep)\n\n\n\n\nlog_id\ndate\nstart_time\nend_time\nduration\nefficiency\nminutes_to_fall_asleep\nminutes_asleep\nminutes_awake\nminutes_after_wakeup\ntime_in_bed\n\n\n\n\n34207402675\n2021-10-18\n2021-10-17 23:01:00\n2021-10-18 06:30:00\n26940000\n91\n0\n391\n58\n0\n449\n\n\n34193579435\n2021-10-17\n2021-10-16 23:03:30\n2021-10-17 08:11:00\n32820000\n95\n0\n472\n75\n4\n547\n\n\n34183584553\n2021-10-16\n2021-10-15 22:46:30\n2021-10-16 06:57:30\n29460000\n94\n0\n424\n67\n0\n491\n\n\n34174304493\n2021-10-15\n2021-10-14 23:50:00\n2021-10-15 08:20:30\n30600000\n94\n0\n438\n72\n0\n510\n\n\n34159751655\n2021-10-14\n2021-10-13 23:34:00\n2021-10-14 09:18:00\n35040000\n98\n0\n524\n60\n0\n584\n\n\n34146865838\n2021-10-13\n2021-10-12 23:50:00\n2021-10-13 08:32:30\n31320000\n94\n0\n461\n61\n1\n522\n\n\n\n\n\n\n\nOnce you’ve loaded some data, you can visualize it!\n\nlibrary(zoo)\nlibrary(scales)\nlibrary(ggthemes)\n\nsleep &lt;- sleep %&gt;%\n  mutate(\n   date = as_date(date),\n   start_time = as_datetime(start_time),\n   end_time = as_datetime(end_time),\n   sh = ifelse(hour(start_time) &lt; 8, hour(start_time) + 24, hour(start_time)), #create numeric times\n   sm = minute(start_time),\n   st = sh + sm/60,\n   eh = hour(end_time),\n   em = minute(end_time),\n   et = eh + em/60,\n   mst = rollmean(st, 7, fill = NA), #create moving averages\n   met = rollmean(et, 7, fill = NA),\n   year = year(start_time)\n)\n\nsleep %&gt;%\n    ggplot(aes(x = date)) +\n    geom_line(aes(y = et), color = 'coral', alpha = .3, na.rm = T) +\n    geom_line(aes(y = st), color = 'dodgerblue', alpha = .3, na.rm = T) +\n    geom_line(aes(y = met), color = 'coral', na.rm = T) +\n    geom_line(aes(y = mst), color = 'dodgerblue', na.rm = T) +\n    scale_y_continuous(\n      breaks = seq(0, 30, 2),\n      labels = trans_format(\n        function(x) ifelse(x &gt; 23, x - 24, x), \n        format = scales::comma_format(suffix = \":00\", accuracy = 1)\n      )\n    ) +\n    labs(x = \"Date\", y = 'Time') +\n    theme_fivethirtyeight() +\n    scale_x_date(date_breaks = '1 month', date_labels = '%b', expand = c(0, 0)) +\n    facet_grid(. ~ year, space = 'free', scales = 'free_x', switch = 'x') +\n    theme(panel.spacing.x = unit(0,\"line\"), strip.placement = \"outside\")\n\n\n\n\n\n\n\n\nThis bit of code makes a nicely formatted plot of the times you went to sleep and woke up over the past three months. You can also use fitbitr to expand the time window with a little help from purrr (the Fitbit API rate limits you, so you can’t request data for infinitely long windows in a single request).\n\n## Pull three months of data\nsleep &lt;- map_dfr(\n  3:0,\n  ~ sleep_summary(\n    end - months(.x), \n    end - months(.x) + months(1)\n  )\n)\n\nAfter pulling the data, we can use the same code again to visualize it.\n\nsleep &lt;- sleep %&gt;%\n  mutate(\n   date = as_date(date),\n   start_time = as_datetime(start_time),\n   end_time = as_datetime(end_time),\n   sh = ifelse(hour(start_time) &lt; 8, hour(start_time) + 24, hour(start_time)), #create numeric times\n   sm = minute(start_time),\n   st = sh + sm/60,\n   eh = hour(end_time),\n   em = minute(end_time),\n   et = eh + em/60,\n   mst = rollmean(st, 7, fill = NA), #create moving averages\n   met = rollmean(et, 7, fill = NA),\n   year = year(start_time)\n) %&gt;%\n  distinct()\n\nsleep %&gt;%\n    ggplot(aes(x = date)) +\n    geom_line(aes(y = et), color = 'coral', alpha = .3, na.rm = T) +\n    geom_line(aes(y = st), color = 'dodgerblue', alpha = .3, na.rm = T) +\n    geom_line(aes(y = met), color = 'coral', na.rm = T) +\n    geom_line(aes(y = mst), color = 'dodgerblue', na.rm = T) +\n    scale_y_continuous(\n      breaks = seq(0, 30, 2),\n      labels = trans_format(\n        function(x) ifelse(x &gt; 23, x - 24, x), \n        format = scales::comma_format(suffix = \":00\", accuracy = 1)\n      )\n    ) +\n    labs(x = \"Date\", y = 'Time') +\n    theme_fivethirtyeight() +\n  scale_x_date(date_breaks = '1 month', date_labels = '%b', expand = c(0, 0)) +\n  facet_grid(. ~ year, space = 'free', scales = 'free_x', switch = 'x') +\n  theme(panel.spacing.x = unit(0,\"line\"), strip.placement = \"outside\")"
  },
  {
    "objectID": "posts/2021-06-08-working-with-your-fitbit-data-in-r/index.html#heart-rate-and-steps",
    "href": "posts/2021-06-08-working-with-your-fitbit-data-in-r/index.html#heart-rate-and-steps",
    "title": "Working With Your Fitbit Data in R",
    "section": "Heart Rate and Steps",
    "text": "Heart Rate and Steps\nYou can also pull your heart rate data with fitbitr. Maybe we’re curious about seeing how the number of minutes spent in the “fat burn,” “cardio,” and “peak” zones correlates with the number of steps taken that day. Let’s find out!\n\nhr &lt;- map_dfr(\n  3:0,\n  ~ heart_rate_zones(\n    end - months(.x), \n    end - months(.x) + months(1)\n  )\n)\n\nsteps &lt;- map_dfr(\n  3:0,\n  ~ steps(\n    end - months(.x), \n    end - months(.x) + months(1)\n  )\n)\n\nFirst, we can examine the heart rate data:\n\nhead(hr)\n\n\n\n\ndate\nzone\nmin_hr\nmax_hr\nminutes_in_zone\ncalories_out\n\n\n\n\n2021-07-18\nOut of Range\n30\n113\n1440\n2530.16460\n\n\n2021-07-18\nFat Burn\n113\n141\n0\n0.00000\n\n\n2021-07-18\nCardio\n141\n176\n0\n0.00000\n\n\n2021-07-18\nPeak\n176\n220\n0\n0.00000\n\n\n2021-07-19\nOut of Range\n30\n113\n1408\n2689.45124\n\n\n2021-07-19\nFat Burn\n113\n141\n9\n86.59917\n\n\n\n\n\n\n\nand the steps data:\n\nhead(steps)\n\n\n\n\ndate\nsteps\n\n\n\n\n2021-07-18\n5620\n\n\n2021-07-19\n7537\n\n\n2021-07-20\n5513\n\n\n2021-07-21\n9014\n\n\n2021-07-22\n10883\n\n\n2021-07-23\n2975\n\n\n\n\n\n\n\nNow, let’s plot them against each other.\n\ndf &lt;- hr %&gt;%\n  filter(zone != \"Out of Range\") %&gt;%\n  group_by(date) %&gt;%\n  summarize(total_minutes = sum(minutes_in_zone), .groups = \"drop\") %&gt;%\n  inner_join(steps, by = \"date\")\n  \ndf %&gt;%\n  mutate(steps = as.numeric(steps)) %&gt;%\n  filter(log(total_minutes) &gt; 1) %&gt;%\n  ggplot(\n    aes(\n      steps,\n      total_minutes\n    )\n  ) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F) +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\n\n\n\n\nOr maybe it’d be interesting to predict your zone minutes from your steps:\n\npredictions &lt;- df %&gt;%\n  mutate(steps = as.numeric(steps)) %&gt;%\n  lm(total_minutes ~ steps, data = .) %&gt;%\n  broom::tidy() %&gt;%\n  mutate(across(where(is.numeric), round, 5))\n\nhead(predictions)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n23.09761\n5.77502\n3.99957\n0.00011\n\n\nsteps\n0.00252\n0.00056\n4.52922\n0.00001"
  },
  {
    "objectID": "posts/2021-06-08-working-with-your-fitbit-data-in-r/index.html#wrapping-up",
    "href": "posts/2021-06-08-working-with-your-fitbit-data-in-r/index.html#wrapping-up",
    "title": "Working With Your Fitbit Data in R",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nAnd that’s it! Hopefully this helped show how fitbitr makes pulling your data easy, and gets you curious about the insights you can glean from your own data. The Fitbit API gives you access to so much interesting information about yourself, your habits, your fitness, and so much more, and fitbitr is just meant to be a door into that gold mine."
  },
  {
    "objectID": "posts/2023-03-09-on-auc-roc/index.html",
    "href": "posts/2023-03-09-on-auc-roc/index.html",
    "title": "Interpreting AUC-ROC",
    "section": "",
    "text": "AUC goes by many names: AUC, AUC-ROC, ROC-AUC, the area under the curve, and so on. It’s an extremely important metric for evaluating machine learning models and it’s an uber-popular data science interview question. It’s also, at least in my experience, the single most commonly misunderstood metric in data science.\nI’ve heard several common misunderstandings or flat-out falsehoods from people in all kinds of roles discussing AUC. The biggest offenses tend to come from overcomplicating the topic. It’s easy to see the Wikipedia page for the ROC curve and be confused, intimidated, or some combination of the two. ROC builds off of other fundamental data science concepts – the true and false positives rates of a classifier – so it’s certainly not a good place to start learning about metrics for evaluating the performance of models.\nThe most common cause for confusion about AUC seems to come from the plot of the ROC curve, and nothing particularly special about AUC itself. Generally, I’ll hear AUC explained as being the area under the ROC curve, and that it’s all about testing how well your model balances false positives and false negatives. That’s all well and good, but it doesn’t give someone new to AUC any intuition about what AUC actually means in practice. For instance, let’s imagine we’re trying to predict the chance that a student is accepted at Carleton College – a quite common problem at CollegeVine! How does saying “AUC tells me about how my model is balancing false negatives and false positives” tell me anything about how well my model is doing at predicting that student’s chances?\nThe main issue I have with this factual-yet-unhelpful explanation of AUC is just that: While it may be true, it doesn’t get to the point. And even worse, it’s sometimes used as a crutch: A fallback answer when someone feels stuck when asked how to interpret AUC in real, practical terms.\nSo in this post, I’ll focus on just one thing, then: Answering the question above about how to interpret AUC."
  },
  {
    "objectID": "posts/2023-03-09-on-auc-roc/index.html#what-is-auc",
    "href": "posts/2023-03-09-on-auc-roc/index.html#what-is-auc",
    "title": "Interpreting AUC-ROC",
    "section": "What is AUC?",
    "text": "What is AUC?\nAs I mentioned, it’s usually not helpful to try to explain AUC to someone by telling them that it’s just the area under the ROC curve, or that it’s a metric you can use for predicting probabilities as opposed to predicting classes, or that it’s a metric trying to balance false positives and false negatives. None of those things get to the crux of the problem.\nSo what is AUC, then? It’s pretty simple: Let’s imagine a model \\(M\\) being evaluated on data \\(X\\) where \\(X\\) contains some instances of the true class and some instances of the false class. The AUC of \\(M\\) on \\(X\\) is the probability that given a random item from \\(X\\) belonging to the true class (\\(T\\)) and another random item from \\(X\\) belonging to the false class (\\(F\\)), that the model predicts that the probability of \\(T\\) being true (belonging to the true class) is higher than the probability of \\(F\\) being true (belonging to the true class).\nLet’s go back to the example about Carleton admissions, and let’s imagine that we have a model that gives a probability of admission to Carleton given some information about a student. If I give the model one random accepted student and one random rejected student, the AUC of the model is the probability that the accepted student had a higher chance of acceptance (as estimated by the model) than the rejected student did.\nFor more on this, I’d refer everyone to this fantastic blog post by the team at Google, which does a great job at explaining further and/or better."
  },
  {
    "objectID": "posts/2023-03-09-on-auc-roc/index.html#a-simple-implementation",
    "href": "posts/2023-03-09-on-auc-roc/index.html#a-simple-implementation",
    "title": "Interpreting AUC-ROC",
    "section": "A Simple Implementation",
    "text": "A Simple Implementation\nThe easiest way to convey this idea might be to show a simple implementation of AUC. Below is some R code.\nFirst, let’s start by writing a function to do exactly what’s described above. Again, here’s the algorithm given some evaluation data:\n\nChoose a random item from the true class.\nChoose a random item from the false class.\nMake a prediction on each of the two items.\nIf the predicted probability for the actually true item is greater than the predicted probability for the actually false item, return true. Otherwise, return false. If they’re equal, flip a coin.\nRepeat 1-4 many times, and calculate the proportion of the time your model guessed correctly. This is your AUC.\n\nNow, let’s write this in R with a little help from some vectorization.\n\nlibrary(rlang)\nlibrary(dplyr)\nlibrary(tibble)\n\n## Our AUC implementation\n## In this implementation, we take a data frame containing a \"truth\" (i.e. whether\n## the example is _actually_ in either the true class or the false class)\n## and an \"estimate\" (our predicted probability).\n## This implementation is in line with how {{yardstick}} implements all of its metrics\ninterpretable_auc &lt;- function(data, N, truth_col = \"truth\", estimate_col = \"estimate\") {\n  \n  ## First, subset the data down to just trues and just falses, separately\n  trues &lt;- filter(data, .data[[truth_col]] == 1)\n  falses &lt;- filter(data, .data[[truth_col]] == 0)\n  \n  ## Sample the predicted probabilities for N `true` examples, with replacement\n  random_trues &lt;- sample(trues[[estimate_col]], size = N, replace = TRUE)\n  \n  ## Do the same for N `false` examples\n  random_falses &lt;- sample(falses[[estimate_col]], size = N, replace = TRUE)\n\n  ## If the predicted probability for the actually true\n  ##  item is greater than that of the actually false item,\n  ##  return `true`. \n  ## If the two are equal, flip a coin.\n  ## Otherwise, return false.\n  true_wins &lt;- ifelse(\n    random_trues == random_falses,\n    runif(N) &gt; 0.50,\n    random_trues &gt; random_falses\n  )\n  \n  ## Compute the percentage of the time our model was \"right\"\n  mean(true_wins)\n}\n\nNext, we can test our simple implementation against yardstick on some real data. For the sake of demonstration, I just used the built-in mtcars data. Here’s how the data looks:\n\nlibrary(knitr)\nlibrary(kableExtra)\n\n## Doing a little data wrangling\ndata &lt;- mtcars %&gt;%\n  transmute(\n    vs = as.factor(vs),\n    mpg,\n    cyl\n  ) %&gt;%\n  as_tibble()\n\ndata %&gt;%\n  slice_sample(n = 6) %&gt;%\n  kable(\"html\", caption = 'Six rows of our training data') %&gt;%\n  kable_styling(position = \"center\", full_width = TRUE)\n\n\nSix rows of our training data\n\n\nvs\nmpg\ncyl\n\n\n\n\n1\n17.8\n6\n\n\n0\n15.0\n8\n\n\n0\n15.8\n8\n\n\n1\n21.4\n6\n\n\n1\n32.4\n4\n\n\n0\n15.2\n8\n\n\n\n\n\n\n\nNow, let’s fit a few logistic regression models to the data to see how our AUC implementation compares to the yardstick one.\n\nlibrary(purrr)\nlibrary(yardstick)\n\n## Simplest model -- Just an intercept. AUC should be 50%\nmodel1 &lt;- glm(vs ~ 1, data = data, family = binomial)\n\n## Adding another predictor\nmodel2 &lt;- glm(vs ~ mpg, data = data, family = binomial)\n\n## And another\nmodel3 &lt;- glm(vs ~ mpg + cyl, data = data, family = binomial)\n\n## Make predictions for all three models\npreds &lt;- tibble(\n  truth = data$vs,\n  m1 = predict(model1, type = \"response\"),\n  m2 = predict(model2, type = \"response\"),\n  m3 = predict(model3, type = \"response\")\n)\n\n## For each model, compute AUC with both methods: Yardstick (library) and \"homemade\"\nmap_dfr(\n  c(\"m1\", \"m2\", \"m3\"),\n  ~ {\n    yardstick &lt;- roc_auc(preds, truth = truth, estimate = !!.x, event_level = \"second\")$.estimate\n    homemade &lt;- interpretable_auc(preds, N = 100000, truth_col = \"truth\", estimate_col = .x)\n    tibble(\n      model = .x,\n      yardstick = round(yardstick, digits = 2),\n      homemade = round(homemade, digits = 2)\n    )\n  }\n) %&gt;%\n  kable(\"html\", caption = 'Yardstick vs. Our Implementation') %&gt;%\n  kable_styling(position = \"center\", full_width = TRUE)\n\n\nYardstick vs. Our Implementation\n\n\nmodel\nyardstick\nhomemade\n\n\n\n\nm1\n0.50\n0.50\n\n\nm2\n0.91\n0.91\n\n\nm3\n0.95\n0.95\n\n\n\n\n\n\n\nAs we’ve seen here, AUC actually shouldn’t be all that much of a cause for confusion! The way I like to frame it is this: The AUC of your model is how good your model is at making even-odds bets. If I give your model two options and ask it to pick which one it thinks is more likely, a “better” model (by AUC standards) will be better at identifying the true class more often.\nIn real terms, that’s a meaningful, good thing. If we’re trying to predict the probability of a cancer patient having cancer, it’s important that our model can distinguish between people with cancer and people without it when given one person from each class. If it couldn’t - meaning the model was either randomly guessing or doing worse than random - the AUC would be 50% (or below 50%, in the worse-than-random disaster scenario)."
  },
  {
    "objectID": "posts/2023-03-09-on-auc-roc/index.html#additional-thoughts",
    "href": "posts/2023-03-09-on-auc-roc/index.html#additional-thoughts",
    "title": "Interpreting AUC-ROC",
    "section": "Additional Thoughts",
    "text": "Additional Thoughts\nI also often hear the misconception that AUC is sensitive to things like class imbalance. This means that if the true class makes up a disproportionately large (or small) proportion of the evaluation data, that can skew the AUC. But based on the intuition we just built before, that’s of course not true. The key thing to remember is that the model is given one true and one false example. In choosing those, it doesn’t matter if the true class only makes up 0.005% of all of the examples in the evaluation data: AUC is only evaluating the model on its ability to determine which of the two is the true class.\nHowever, there is one thing related to class imbalance, and just sample size in general, that would affect AUC, which is the raw number of examples of each class in the evaluation data. If, for instance, you had only a single instance of the true class in the evaluation set, then the AUC of the model is entirely determined by how good the predictions of the model are on that single example. For instance, if we have a single true class and the model predicts a 100% probability of it being true, then, assuming the predictions for all of the other examples in the evaluation set are not 100%, the AUC of the model as evaluated on that data is 100%. This isn’t necessarily because the model is “good” in any sense, but just because the model is over-indexing to a single good prediction in the evaluation set. In practice though, this AUC estimate wouldn’t generalize. As we got more data, the predictions for all the true classes would certainly not all be 100%, so the AUC of the model would go down over time.\nFortunately, there’s an easy fix for this problem. AUCs are a point estimate, but we could also estimate a margin of error or a confidence interval for our AUC. For a situation where we only have a single instance of the true class in the evaluation set, the margin of error for our AUC would be very wide."
  },
  {
    "objectID": "posts/2023-03-09-on-auc-roc/index.html#wrapping-up",
    "href": "posts/2023-03-09-on-auc-roc/index.html#wrapping-up",
    "title": "Interpreting AUC-ROC",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nHopefully this post helped give a better intuition for what AUC actually is! A couple of major takeaways:\n\nAUC doesn’t need to be this super complicated thing about trading off between false positives and negatives and trying many different classification thresholds and such. In my opinion, it’s much simpler to just think about it as the likelihood of a guess that your model makes between two choices being correct.\nAUC isn’t affected by class imbalances."
  },
  {
    "objectID": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html",
    "href": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html",
    "title": "Deploying MLFlow on Heroku (with Heroku Postgres, S3, and nginx for basic auth)",
    "section": "",
    "text": "Disclaimer: I followed this guide to setting up MLFlow on Heroku initially. However, there were certain aspects of it that are either outdated or do not work, so this post remedies those issues."
  },
  {
    "objectID": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#mlflow",
    "href": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#mlflow",
    "title": "Deploying MLFlow on Heroku (with Heroku Postgres, S3, and nginx for basic auth)",
    "section": "MLFlow",
    "text": "MLFlow\nMLFlow is an open source tool for the entire machine learning lifecycle. It lets you create and experiment with models, write notes and descriptions, track parameters, metrics, and artifacts, and deploy to production all through an easy-to-use API and an intuitive UI. You can make calls to a running MLFlow service through the R API, the Python API, the Java API, or via the command line (cURL) or your favorite language by interacting with the REST API."
  },
  {
    "objectID": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#overview",
    "href": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#overview",
    "title": "Deploying MLFlow on Heroku (with Heroku Postgres, S3, and nginx for basic auth)",
    "section": "Overview",
    "text": "Overview\nMLFlow is surprisingly easy to set up and deploy to Heroku. There are a few steps to follow to get everything running: 1. Dockerize an MLFlow instance 2. Set up an artifact store 3. Set up a database 4. Secure the instance with basic auth"
  },
  {
    "objectID": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#prerequisites",
    "href": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#prerequisites",
    "title": "Deploying MLFlow on Heroku (with Heroku Postgres, S3, and nginx for basic auth)",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis guide assumes you already have AWS (specifically S3) set up. It also assumes some knowledge of shell scripting, Docker, and Heroku."
  },
  {
    "objectID": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#creating-a-heroku-app",
    "href": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#creating-a-heroku-app",
    "title": "Deploying MLFlow on Heroku (with Heroku Postgres, S3, and nginx for basic auth)",
    "section": "Creating a Heroku App",
    "text": "Creating a Heroku App\nFirst, let’s create a Heroku app from the CLI. For the purposes of this example, I’m going to call the app my-mlflow-example. You’ll need to choose your own app name.\nheroku create my-mlflow-example\nNext, let’s attach a Heroku Postgres instance.\nheroku addons:create heroku-postgresql:hobby-dev --app my-mlflow-example\nCreating this hobby-dev Heroku Postgres instance will also automatically set the DATABASE_URL environment variable in your app’s configuration.\nNext, we’ll need some other environment variables to be set. You can set your config like this:\nheroku config:set \\\n  S3_URI=s3://YOUR-S3-URI \\\n  AWS_SECRET_ACCESS_KEY=YOUR-SECRET \\\n  AWS_ACCESS_KEY_ID=YOUR-KEY \\\n  AWS_DEFAULT_REGION=YOUR-REGION \\\n  MLFLOW_TRACKING_USERNAME=YOUR-USERNAME \\\n  MLFLOW_TRACKING_PASSWORD=YOUR-PASSWORD \\\n  --app my-mlflow-example\nYou can repeat fewer lines of code by creating a .env file that looks like this:\nS3_URI=s3://YOUR-S3-URI AWS_SECRET_ACCESS_KEY=YOUR-SECRET ...\nand then using heroku config:set .env --app my-mlflow-example.\nGreat! At this point, your Heroku app should be all set up. Now all we need to do is create the Docker image that will run MLFlow."
  },
  {
    "objectID": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#setup",
    "href": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#setup",
    "title": "Deploying MLFlow on Heroku (with Heroku Postgres, S3, and nginx for basic auth)",
    "section": "Setup",
    "text": "Setup\nFirst thing’s first, let’s create a folder called my-mlflow-example where we’ll store all the files we’ll need. I’ll do that with:\nmkdir my-mlflow-example && cd my-mlflow-example\nNext, let’s create a few files we’ll need:\ntouch Dockerfile run.sh requirements.txt nginx.conf_template\nYou’ll also want to make your run.sh script executable.\nchmod +x run.sh\nGreat, now we’ve to all the files we’ll need to deploy our MLFlow instance!\n\nThe Dockerfile\nThe Dockerfile will contain all of the library installs and files you need to run your MLFlow instance. The Dockerfile you’ll need to write for MLFlow should look something like this:\nFROM continuumio/miniconda3\n\n## Copy files into the image\nCOPY run.sh run.sh\nCOPY requirements.txt requirements.txt\nCOPY nginx.conf_template /etc/nginx/sites-available/default/nginx.conf_template\n\n## Install Postgres\nRUN apt-get -y update && \\\n  apt-get -y upgrade && \\\n  apt-get install -y postgresql\n\n## Install nginx and dependencies\nRUN apt-get -y update && \\\n  apt-get install -y make vim \\\n  automake gcc g++ subversion \\\n  musl-dev nginx gettext apache2-utils\n\n## Install pip and dependencies\nRUN conda install -c anaconda pip && \\\n  pip install --upgrade pip && \\\n  pip install -r requirements.txt && \\\n  conda update -n base -c defaults conda && \\\n  conda env list && \\\n  pip freeze list\n\n## Run your `run.sh` script on container boot\nCMD ./run.sh\n\n\nThe requirements.txt File\nNext, copy the following lines into your requirements.txt:\nmlflow\npsycopg2-binary\nboto3\nThis file will tell pip which libraries to install in your docker image in the RUN pip install -r requirements.txt line above.\n\n\nThe nginx Template\nNext, we’ll create a template for the nginx.conf file that we’ll eventually use in the container for basic auth. One important issue here: This file is a template because Heroku randomly assigns a port on dyno start, which means we can’t hard code any ports for nginx (since we don’t know them ahead of time). Instead of hard-coding, we specify a couple of placeholders: $HEROKU_PORT and $MLFLOW_PORT. We’ll replace these with the proper ports on container startup.\nYour nginx.conf_template should look like this:\nevents {}\nhttp {\n  server {\n        listen $HEROKU_PORT;\n\n        access_log /var/log/nginx/reverse-access.log;\n        error_log /var/log/nginx/reverse-error.log;\n\n        location / {\n            auth_basic \"Restricted Content\";\n            auth_basic_user_file /etc/nginx/.htpasswd;\n\n            proxy_pass                          http://127.0.0.1:$MLFLOW_PORT/;\n            proxy_set_header Host               $host;\n            proxy_set_header X-Real-IP          $remote_addr;\n            proxy_set_header X-Forwarded-For    $proxy_add_x_forwarded_for;\n        }\n    }\n}\n\n\nRun Script\nFinally, let’s create a script, run.sh, which will run when the Heroku dyno starts.\nexport HEROKU_PORT=$(echo \"$PORT\")\nexport MLFLOW_PORT=5000\n\nenvsubst '$HEROKU_PORT,$MLFLOW_PORT' &lt; /etc/nginx/sites-available/default/nginx.conf_template &gt; /etc/nginx/sites-available/default/nginx.conf\n\nhtpasswd -bc /etc/nginx/.htpasswd $MLFLOW_TRACKING_USERNAME $MLFLOW_TRACKING_PASSWORD\n\nkillall nginx\n\nmlflow ui \\\n  --port $MLFLOW_PORT \\\n  --host 127.0.0.1 \\\n  --backend-store-uri $(echo \"$DATABASE_URL\" | sed \"s/postgres/postgresql/\") \\\n  --default-artifact-root $S3_URI &\n\nnginx -g 'daemon off;' -c /etc/nginx/sites-available/default/nginx.conf\nThere are a few things happening here: 1. We set the HEROKU_PORT environment variable from the randomly assigned PORT that Heroku creates on dyno startup 2. We assign MLFLOW_PORT to 5000. The chances that Heroku assigns exactly port 5000 are low. If you want, you can add a few more lines to first check if Heroku assigns 5000 as the port, and if it does, choose any other port instead (e.g. 1000). 3. We substitute the port placeholder variables in our nginx.conf_template file with the actual environment variable values for the ports, so that nginx knows where to listen and direct traffic. 4. We create a new .htpasswd file from the MLFLOW_TRACKING_USERNAME and MLFLOW_TRACKING_PASSWORD environment variables that we set in the Heroku config. This file will be used by nginx to check inputted usernames and passwords against. 5. We start the MLFlow UI in the background, telling it to run on the MLFLOW_PORT variable we created, and pointing it to our Heroku Postgres instance for the backend store and our S3 bucket for artifact storage. Note: By default, Heroku Postgres provides a URL that begins with postgres. This is not compatible with SQLAlchemy, so we substitute postgresql (which is compatible) for postgres. 6. We start nginx for basic auth."
  },
  {
    "objectID": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#deploying",
    "href": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#deploying",
    "title": "Deploying MLFlow on Heroku (with Heroku Postgres, S3, and nginx for basic auth)",
    "section": "Deploying",
    "text": "Deploying\nDeployment is simple, and can be done with a few lines of bash:\nheroku login\nheroku container:login\nheroku container:push web --app my-mlflow-example\nheroku container:release web --app my-mlflow-example\nAnd that’s it! Running those four lines should build your Docker image, push it to Heroku, and release it as your app. Once it releases, Heroku will boot up a dyno and you should be able to go to my-mlflow-example.herokuapp.com and see the MLFlow UI."
  },
  {
    "objectID": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#using-mlflow",
    "href": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#using-mlflow",
    "title": "Deploying MLFlow on Heroku (with Heroku Postgres, S3, and nginx for basic auth)",
    "section": "Using MLFlow",
    "text": "Using MLFlow\nNow that your instance is deployed, you should have no problem using MLFlow for your whole ML lifecycle. For example, in R you might want to create a new experiment. You could do something like this:\n## install.packages(\"mlflow\")\n\nlibrary(mlflow)\n\n## Set up some environment variables\n## This way, your R session will know where to\n##  look for your MLFlow instance and will have\n##  the proper credentials set up\nSys.setenv(\n  MLFLOW_TRACKING_URI = \"https://my-mlflow-example.herokuapp.com\"\n  MLFLOW_TRACKING_USERNAME = \"YOUR-USERNAME\",\n  MLFLOW_TRACKING_PASSWORD = \"YOUR-PASSWORD\"\n)\n\nmlflow_create_experiment(\"my-first-experiment\")\nAnd with that, you should be able to harness all of the awesome power of MLFlow for all of your ML lifecycle needs!"
  },
  {
    "objectID": "posts/2021-01-10-what-i-ve-been-drinking-in-quarantine/index.html",
    "href": "posts/2021-01-10-what-i-ve-been-drinking-in-quarantine/index.html",
    "title": "What I’ve Been Drinking in Quarantine (Plus a Cocktail Lesson)",
    "section": "",
    "text": "The past ten-or-so months of limited activity due to Covid-19 have been a slog for everyone, to say the least. I’ve been fortunate to have avoided the worst of it, having spent much of the past ten months living in rural northwestern Connecticut.\nMany of us have had lots of time to explore hobbies over this time, and something that has gained lots of popularity – editor’s note: unsurprisingly – is drinking! In particular, there seems to have been a surge in home bartending and cocktail lessons happening during Covid, since hobbyist bartending – which has been, and continues to be, a hobby of mine – is a great Covid activity! By bartending, you get to learn about cocktails and build your skills to impress your friends when social gatherings start happening again, and you get to drink during it! What could be better than that?"
  },
  {
    "objectID": "posts/2021-01-10-what-i-ve-been-drinking-in-quarantine/index.html#beers-ive-been-loving",
    "href": "posts/2021-01-10-what-i-ve-been-drinking-in-quarantine/index.html#beers-ive-been-loving",
    "title": "What I’ve Been Drinking in Quarantine (Plus a Cocktail Lesson)",
    "section": "Beers I’ve Been Loving",
    "text": "Beers I’ve Been Loving\n\nGreen, Very Green, Juice Machine, and Haze by Tree House Brewing Company in Charlton, MA. Bascially, you can’t go wrong with Tree House. It’s one of the best breweries in the country, is all over the top ratings on Untappd, and makes almost univerally awesome beer. The ones I’ve listed are all very hazy, juicy, fruity (although not as much as some others, like Saturated and Iridescent) New England Style IPAs. That means they’re a little less bitter, a little less hoppy, and a little more like drinking orange juice than some of the other IPAs you’ve probably had elsewhere (Goose Island, Dogfish Head, etc.).\nFocal Banger and Heady Topper from The Alchemist in Stowe, VT. Similar to Tree House, The Alchemist is a wildly popular New England brewery with some awesome beers. I’ve liked all of the ones I’ve tried, but Heady Topper and Focal Banger are especially awesome. I actually prefer the two of them to most of the Tree House beers (bar Very Green, probably). They’re both a little less fruity than the Tree Houses, which I prefer.\nYou Drive Us Wild, by Grimm in New York City. This is another beer with a similar profile to the Tree House / Alchemist groups (are you sensing a pattern?). Grimm is also an awesome brewery, and I’ve loved almost everything of theirs that I’ve had. Magnetic Compass and Tesseract are other highlights."
  },
  {
    "objectID": "posts/2021-01-10-what-i-ve-been-drinking-in-quarantine/index.html#liquors-and-amari-ive-been-drinking",
    "href": "posts/2021-01-10-what-i-ve-been-drinking-in-quarantine/index.html#liquors-and-amari-ive-been-drinking",
    "title": "What I’ve Been Drinking in Quarantine (Plus a Cocktail Lesson)",
    "section": "Liquors and Amari I’ve Been Drinking",
    "text": "Liquors and Amari I’ve Been Drinking\nThese are my go-to glasses to sip on. Generally, liquor before or during dinner, and amaro after dinner. “Amaro” is the Italian word for bitter, so reader beware: the amari (especially Fernet) are very bitter.\n\nLagavulin 8 Year\nDel Maguey Chichicapa\nTequila Ocho Plata\nCynar\nCampari\nFernet Branca"
  },
  {
    "objectID": "posts/2021-01-10-what-i-ve-been-drinking-in-quarantine/index.html#cocktails",
    "href": "posts/2021-01-10-what-i-ve-been-drinking-in-quarantine/index.html#cocktails",
    "title": "What I’ve Been Drinking in Quarantine (Plus a Cocktail Lesson)",
    "section": "Cocktails",
    "text": "Cocktails\nSo, now for the main event. As promised, some great cocktails to try! A preface: I like a weird profile of cocktail. I’m not huge on sweet drinks, so the overly sweet margaritas from your neighborhood cantina aren’t what you’ll find here. I like sour, bitter, smoky, spicy, and herbal with just a touch of sweet. I’ll start with a few simple drinks (just a few easy to find ingredients), and then go through a couple of favorites that are a little more niche. At the end, I’ll give some general pointers on general cocktail making things and ingredients.\n\nNegroni\nThe Negroni is my all time favorite drink. It’s bitter, sweet, complex, and easy to make! Traditionally, it’s equal parts gin, Campari, and sweet vermouth:\n\n1oz gin\n1oz Campari\n1oz sweet vermouth\nOrange or lemon twist garnish\n\nAdd as much ice as you can fit into a beaker or other glass that you can stir in, and add the ingredients. Stir about thirty seconds until the glass is well chilled. Strain into the glass of your choosing. Garnish.\nFor a Negroni, you probably want a neutral, London dry style gin. I’ve found that Beefeater works great, and isn’t particularly expensive. The vermouth is the most important piece here, as it gives all kinds of interesting flavors to the drink depending on the brand you use. You’ll want to spend the extra few dollars to get something great, like Carpano Antica.\nPersonally, as a mezcal lover, I often find myself swapping out the gin in my Negronis for Del Maguey Vida, which is a high-quality mezcal that’s great for making cocktails with. I’ve also found that using apple brandy is delicious as well, and using bourbon gets you close to a Boulevardier. Feel free to experiment!\n\n\nManhattan\nThe Manhattan is another classic. I use rye in mine, but you can get away with bourbon too (although you might get weird from bartenders are certain bars for doing so).\n\n2oz rye whiskey\n1oz sweet vermouth\n4-6 dashes of bitters\nMaraschino cherry garnish\n\nAdd as much ice as you can fit into a beaker or other glass that you can stir in, and add the ingredients (preferably bitters-first so they don’t just sit on top of the ice). Stir about thirty seconds until the glass is well chilled. Strain into the glass of your choosing. Garnish.\nSince the Manhattan is so simple, you really need to use high quality ingredients. I’ve found Rittenhouse Rye to be a fantastic rye, especially given the price. As before, you probably want Carpano Antica for the vermouth.\n\n\nMargarita\nYet another classic, everyone’s familiar with a margarita. It’s simple and delicious.\n\n2oz tequila\n1oz lime juice\n.75oz triple sec, Cointreau, or 1:1 simple syrup\n\nAdd ingredients to a shaker with as much ice as you can fit and shake vigorously for 30 seconds until the shaker is well chilled. Double strain into your glass of choice. Optionally, you can salt the rim of your glass.\nYou have some options for this one. Again, as a mezcal lover, I often swap the tequila for mezcal (or go 50/50). I also prefer less sweet drinks, so if .75oz of sugary stuff isn’t enough for you, just add some more!\n\n\nMoscow Mule\nThe copper cup drink, and a super easy, delicious vodka cocktail.\n\n2oz vodka\n3-5oz ginger beer, depending on how strong you like it\nsqueeze of lime juice\n\nFor this one, I normally just add the ingredients to a glass, stir, and sip away! Using more ginger beer will mellow out the drink a lot.\nNow, with some delicious, easy classics that won’t let you down out of the way, on to some more involved drinks! Some of these require harder-to-find ingredients (or just more ingredients), but I think they’re all delicious and worth a shot.\n\n\nLast Word\nThe Last Word is an interesting one. It’s sweet, sour, herbal, and definitely not everybody’s cup of tea. It’s another easy equal-parter:\n\n.75oz lime juice\n.75oz green Chartreuse\n.75oz Luxardo Maraschino liqueur\n.75oz gin\nMaraschino cherry garnish\n\nAdd the ingredients to a cocktail shaker with as much ice as you can fit, and shake vigorously for about 30 seconds until the shaker is well chilled. Double strain into your glass of choice.\nThis is a weird mix of ingredients, but it comes together to make a complex, delicious drink! For the adventurous, it’s definitely worth a shot.\n\n\nJungle Bird\nThe Jungle Bird is a bitter drink fan’s tiki drink. It’s a delicious mix of tiki and bitter, and is a real crowd-pleaser.\n\n1.5oz dark rum\n1.5oz pineapple juice\n.75oz Campari\n.5oz lime juice\n.5oz 1:1 simple syrup\n\nAdd the ingredients to a cocktail shaker with as much ice as you can fit, and shake vigorously for about 30 seconds until the shaker is well chilled. Double strain into your glass of choice.\nThe Jungle Bird is another favorite of mine. It’s not too bitter, not too sweet, and you also get some lime and pineapple in there, which I love both of. It’s a tough one to go wrong with, since it’s not as strong as something like a Manhattan, less bitter than a Negroni, and less sweet than a Margarita can be.\n\n\nCorpse Reviver #2\nAnother personal favorite of mine! The middle child of a trio of Corpse Revivers, I find this one to be totally delicious. Probably named for being the drink that’ll get you on your feet the next morning (need confirmation on that).\n\n1.5oz gin\n1.5oz Lillet Blanc\n1.5oz lemon juice\n.75oz triple sec or Cointreau\n&lt;.25 (small dash) absinthe, Pernod, or green Chartreuse\n\nAdd the ingredients to a cocktail shaker with as much ice as you can fit, and shake vigorously for about 30 seconds until the shaker is well chilled. Double strain into your glass of choice.\nThis is another personal favorite. It’s sour, a little sweet, and very complex. The Lillet adds a lot of character and plays really well with the lemon and the herbal notes from the absinthe.\n\n\nEnzoni\nThe Enzoni is a remix of the Negroni that’s a little sweeter, and uses fresh grapes! It’s equally delicious, and similar in profile to a Jungle Bird (a little sweet and less bitter than the Negroni).\n\n1oz gin\n1oz Campari\n.75oz lemon juice\n.5oz 1:1 simple syrup\n5 white grapes\n\nMuddle the grapes with the simple syrup in the bottom of a cocktail shaker. Then, add as much ice as you can with the rest of the ingredients, and shake vigorously for about 30 seconds until the shaker is well chilled. Double strain into your glass of choice.\nThe Enzoni is an awesome drink. The acidity from the lemon and the sweetness and flavor from the grapes make this a unique and delicious drink.\n\n\nBitter Giusseppe\nFinally, a bitters-forward drink! The Bitter Giusseppe is simple, and checks a lot of boxes for me. It’s sour and bitter, and it’s also a low alcohol content drink!\n\n2oz Cynar\n1oz sweet vermouth\n.25oz lemon juice\n4 dashes of bitters\n\nAdd as much ice as you can fit into a cocktail shaker with the ingredients and shake vigorously for about 30 seconds until the shaker is well chilled. Double strain into your glass of choice.\nAs usual, you probably want to be using Carpano Antica as your vermouth here. It adds a lot of interesting flavors to the drink. This one is awesome for an easy sipper, and the acidity from the lemon really takes it over the top"
  },
  {
    "objectID": "posts/2021-01-10-what-i-ve-been-drinking-in-quarantine/index.html#general-pointers",
    "href": "posts/2021-01-10-what-i-ve-been-drinking-in-quarantine/index.html#general-pointers",
    "title": "What I’ve Been Drinking in Quarantine (Plus a Cocktail Lesson)",
    "section": "General Pointers",
    "text": "General Pointers\n\nCocktail Making\n\nThe reason we shake or stir our drinks is not to mix the ingredients together. Well, it is, but that’s not the primary reason. We’re shaking or stirring to dilute the drink. If you want an example of the importance of dilution, try making a Negroni in a glass without any ice, and just stir it together and taste it. Unless you like an extremely alcohol-forward, bitter drink (which you might, I do), you’ll probably like it far better after stirring with ice. The dilution and chilling of the drink does a lot to enhance the flavor, make the drink more enjoyable, and take the edge off. Don’t skip it!\nYour ingredients matter. High quality ingredients will make better tasting drinks. You really don’t want to be trying to mask bad rye in a Manhattan, for example. You want to be showcasing a great rye!\nSpeaking of ingredients: squeeze your own juice, or at least your own lemon and lime juice. The stuff from the bottle with all of the preservatives is just not the same. If you don’t believe me, do a blind taste test of a margarita made with fresh lime juice vs. one with bottled juice, and see what you think. It’s worth it.\nYou can (and should) make your own simple syrup! It’s easy and takes two minutes, so there’s no reason to spend $7 on a small bottle from the store. All of my recipes call for 1:1 syrup, which means 1 part sugar, 1 part water. If you’re making it, that just means put a cup of sugar and a cup of water in a pot and heat it up gently until the sugar is dissolved. It will keep in the fridge for about a month. If you make 2:1 syrup, cut the amounts in the recipes in half. 2:1 syrup will keep in the fridge for far longer than 1:1 syrup.\n\n\n\nIngredients\n\nBuy lemons and limes. It makes all the difference in your drinks.\nThere are a lot of different types of liquor discussed here. These are the brands I like, but feel free to experiment with others! The key is to make drinks you like:\n\nGin I use Beefeater for everything. It’s relatively inexpensive, available everywhere, and a great, neutral-flavored London dry gin. For a more herbal gin, go for Hendricks.\nTequila This is a blog post in itself. At the very least, get a 100% agave tequila. Espolón and Olmeca Altos are good choices. If you want sipping tequila, you want to be buying something from a NOM (producer) who doesn’t use diffusers or autoclaves. This means NOT Clase Azul, Casamigos, or Patron. Good tequilas are fermented in barrels after the agave is roasted in ovens made of brick or stone, and NOMs using diffusers and autoclaves are cutting corners by using chemicals and high-pressure chambers to decrease their costs and speed up the process. The product suffers as a result, and they often mask bad product by adding sugar to their tequilas, which is why many people will tell you that Clase Azul is “smooth.” It is. That’s added sugar making it taste like that. There are a number of great (harder to find) tequilas that are both great for drinking and an opportunity to support distillers doing things the right way. A few that I love are Fortaleza, Tapatio, Tequila Ocho, and Siete Leguas.\nMezcal Everything by Del Maguey is great. Chichicapa is an incredible sipping mezcal (but more expensive), and I use Vida for all of my mezcal cocktails.\nVodka Expensive vodka is not necessarily good vodka. Personally, I strongly dislike Tito’s, Absolut, and Grey Goose. I’ve found that Tower (from Texas), Smirnoff, and Russian Standard all work fine, and are all far less expensive. A good vodka should taste and smell like nothing, and that’s basically what all three of those vodkas will give you. If you want to impress your friends, fill a Grey Goose bottle with Smirnoff.\nRye As I said before, I’ve had great success with Rittenhouse. It’s a great rye, and it’s not very expensive. Knob Creek is also great if you want to spend the extra money.\nBourbon If you want to use bourbon in a Manhattan (or just to sip), two that I like a lot are Buffalo Trace and Maker’s Mark.\nRum For white rum, I normally just use Bacardi Superior. Plantation is another good one, albeit slightly more expensive. Dark rum preferences will depend on your taste, as some are much sweeter than others. Again, Plantation makes good rum. Or, if you’re ever traveling abroad post-Covid and want to bring back Havana Club Especial duty free, I’d recommend that. We can’t buy it in the U.S. because of the trade embargo with Cuba, which is a shame for many reasons, including Havana Club being great rum.\n\nOther ingredients\n\nVermouth Dolin is fine, Carpano is great. Spend the extra money, it’s worth it.\nGinger Beer, Tonic, etc. My personal favorite brand for mixers is Fever Tree. Their stuff is a little more expensive, but it’s worth it. It’s all great, and will make your drinks even better.\n\nPro-tip: Lots of better bars and restaurants will have a secret “bartender’s choice” option that isn’t listed on the menu. This is a great way to broaden your cocktail horizons. Historically, I’ve just asked for things like “A mezcal drink that’s a little smoky, sour, or bitter, but not too sweet” and have discovered some great drinks! The key is letting a knowledgeable bartender know in broad strokes what you like, and letting them be creative!\nExperiment, and make drinks you like! At the end of the day, you’re drinking for you, so why not enjoy your drink while you’re at it?"
  },
  {
    "objectID": "posts/2021-02-07-what-s-new-in-slackr-2-1-0/index.html",
    "href": "posts/2021-02-07-what-s-new-in-slackr-2-1-0/index.html",
    "title": "What’s New in slackr 2.1.0",
    "section": "",
    "text": "slackr 2.1.0+ is live! There are a whole bunch of exciting changes that we (mostly Andrie de Vries and I) have made to improve the package a bunch."
  },
  {
    "objectID": "posts/2021-02-07-what-s-new-in-slackr-2-1-0/index.html#introduction",
    "href": "posts/2021-02-07-what-s-new-in-slackr-2-1-0/index.html#introduction",
    "title": "What’s New in slackr 2.1.0",
    "section": "",
    "text": "slackr 2.1.0+ is live! There are a whole bunch of exciting changes that we (mostly Andrie de Vries and I) have made to improve the package a bunch."
  },
  {
    "objectID": "posts/2021-02-07-what-s-new-in-slackr-2-1-0/index.html#changes",
    "href": "posts/2021-02-07-what-s-new-in-slackr-2-1-0/index.html#changes",
    "title": "What’s New in slackr 2.1.0",
    "section": "Changes",
    "text": "Changes\nHere are some of the things that are new in slackr 2.1.0+. For more info on the package, check out the Github repo and the pkgdown site.\n\nEase of Use Improvements\n\nWe’ve dramatically improved error messaging, so long gone are the days of errors like No 'id' column found in 'x'! Now, error messages should be far more helpful, with some hints about what might be going wrong.\nWe’ve updated the package documentation significantly, so now there’s a far more informative README, some vignettes, and a pkgdown site.\nWe’ve more clearly described the different use cases for slackr, in order to better help users set up slackr in a way that makes sense for them.\n\n\n\nNew Features\n\nWe’ve fixed a bunch of bugs that were preventing things like icon_emoji and username from working, so those are fixed now!\nWe’ve brought back some old functions that were removed in slackr 2.0.0: slackr_history() and slackr_delete(). See the docs for descriptions of what these functions can do.\n\n\n\nBack-End Changes\nWe’ve made a ton of changes for how slackr interacts with the Slack API:\n\nWe now allow paging, which is especially helpful when you have a workspace of more than 1000 channels.\nWe cache requests to get lists of channels and users so that we don’t need to repeat common API calls. This speeds up calls to slackr_***() and limits how often you need to actually hit the API.\nWe’ve gotten rid of a really nasty implementation of channel caching (writing a local cache to the disk) in favor of the method described above.\nWe’ve factored out API calls into a separate function, which makes the package easier to understand and test.\nSpeaking of testing, we’ve implemented a whole bunch of unit tests, and will be working on more.\n\n\n\nDeprecations\n\nWe’ve deprecated a bunch of camel case functions in favor of their snake case counterparts for simplicity. Don’t worry! These are soft-deprecated for now. They won’t go away fully until a future version of slackr\nWe’ve deprecated text_slackr in favor of slackr_msg, since they do basically the same thing."
  },
  {
    "objectID": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html",
    "href": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html",
    "title": "Exploring the Tail Behavior of ESPN’s Win Probability Model",
    "section": "",
    "text": "It’s College Football Playoff season, which means I’ve been watching a lot of games lately. And I find myself complaining pretty often about how badly calibrated I think ESPN’s win probability model is. In particular, I’ve noted a bunch of examples – or at least enough for it to feel like a bunch – of games where ESPN’s model gives a team a win probability that feels way too extreme in a situation where they’re clearly winning. I’m not talking about giving a team an 80% chance when they should have a 60% chance. The cases I’ve been curious about are something more like teams getting a 99.7% chance of winning when, at least in my opinion, they should be getting something more like a 98% chance."
  },
  {
    "objectID": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#introduction",
    "href": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#introduction",
    "title": "Exploring the Tail Behavior of ESPN’s Win Probability Model",
    "section": "Introduction",
    "text": "Introduction\n\nFor classification problems like predicting win probabilities, model calibration is, at the highest level, the answer to the question “When my model says that Michigan has a 62% chance to win, do they actually end up winning about 62% of the time?” A well-calibrated model will have relatively low error over the long run. As we get more and more data, we’d expect that the amount of error in our calibration numbers would go down, and hopefully the predicted probabilities start to converge to the actual win probabilities as the games play out. For more on calibration, check out this cool FiveThirtyEight post.\n\nYou might be reading this thinking that the difference (both in absolute terms and ratio terms) between 60 and 80 percent is way bigger than the difference between, say, 98 and 99.7. And you’d be right. But I’d encourage you to think about it like this: The team that’s the underdog in the latter case has either a 2% chance (the first case) or a 0.3% chance (the second case). If you’re applying the same ratio of win probabilities back of the napkin math, that increase feels a lot bigger.\n\nFor the statistically inclined folks in the back, the “right” way to do this is just to use odds ratios, which would show that going from 98% to 99.7% is a massive magnitude odds (or log-odds) increase.\n\nSo in a nutshell, what I’ve been curious about is the tail behavior of the ESPN model – I’m trying to answer the question of how ESPN’s model does at predicting events we know are unlikely. How often, for instance, does a team that ESPN gives a 0.5% mid-game chance of winning actually end up winning? My suspicion, based on my anecdotal evidence from watching games and complaining over the years, has been that the model is badly calibrated in the tails. I’ve been on the record arguing that ESPN’s model gives win probabilities greater than 99% way too often, and can usually be heard saying things like “Well, they’re almost definitely going to win. But 99%? I’m not sure…”\nSo then, to the question. I looked into a couple of things: 1. How well-calibrated is their model in general? 2. When ESPN gives a team a very high chance of winning (&gt;98%), how often do they actually win? 3. Does the model perform better or worse for ranked teams?"
  },
  {
    "objectID": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#calibration",
    "href": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#calibration",
    "title": "Exploring the Tail Behavior of ESPN’s Win Probability Model",
    "section": "Calibration",
    "text": "Calibration\nFirst, how well-calibrated is the model in general? I usually like to look at calibration plots to evaluate models, similar to the ones in the FiveThirtyEight post above.\nThis first plot is the overall calibration of the model at kickoff time. What we’re looking for are the points to roughly lie along the dashed line, which is the line y = x.\n\n\n\nTwo main things to notice in that plot:\n\nThe model, on aggregate, is quite well calibrated.\nThe model looks like it’s off by a bit in the lower tail, where it appears to be predicting win probabilities that are too low. That’s a sample size issue. For instance, there were 64 games where the model gave the home team a worse than 5% chance to win, and the home team ended up winning 6.25% in those games. But generating a confidence interval for that proportion gives us a range of 1.25%-12%, which is too wide to scold the model for that mistake\n\nWe can also look at the same plot, broken down by the teams playing. For instance, the following plot is broken down by whether neither team is ranked, one team is, or both teams are:\n\n\n\nIn this case, even with relatively wide error bars, we see that the model seems to perform worse for games where both teams are ranked. And it’s pretty clearly the best in games where neither team is ranked."
  },
  {
    "objectID": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#edge-cases",
    "href": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#edge-cases",
    "title": "Exploring the Tail Behavior of ESPN’s Win Probability Model",
    "section": "Edge Cases",
    "text": "Edge Cases\nNext, I was curious about how often teams given very high chances of winning ended up doing so. Anecdotally, I’ve found myself complaining the most about games like the Oregon - Oregon State game from 2022 where ESPN gives Oregon a 98.3% chance of winning when they’re up 18 with 6:53 left in the third. Of course, I’m leaning into confirmation bias. But it’s hard to not think to yourself that with more than 20 minutes of football to go, Oregon State only wins that game in a wild comeback less than two in one hundred times. I’m not sure what I’d view as a more “correct” estimate of their win probability, but seventeen in a thousand seems low to me. Maybe even thirty in a thousand (3%) would be better.\nOne note is that the 3% probability I’d probably lobby for doesn’t feel that different from the 1.7% that ESPN gave, but that’s an odds ratio of 1.79, which is a big difference in practice. For instance, that’s a similar odds ratio to what you’d get if you went from a 35% chance to a 50% chance, which is significant. In FiveThirtyEight world, that’s the difference between being right in the middle of the “toss-up” category vs. being solidly in the “lean Oregon” category.\nSo anyways, back to Oregon - Oregon State. I was curious about games like that: Games where, with more than, say, five minutes to go and one team leading by at most three scores (24 points), how often ESPN was right when they gave the leading team a better than 98% chance of winning the game.\nAs it turns out, ESPN’s model is doing pretty well in the tails on the whole. See the table below:\n\n\n\n\n\n\n\n\n\n\n\nRanked\nOverall Win %\nWin % CI Lower Bound\nWin % CI Upper Bound\nN\n\n\n\n\nBoth\n0.54%\n0%\n1.63%\n184\n\n\nOne\n1%\n0.4%\n1.7%\n1002\n\n\nNeither\n1.07%\n0.7%\n1.52%\n2427\n\n\nAll\n1.02%\n0.72%\n1.36%\n3613\n\n\n\n\nRanked corresponds to how many of the teams in the game were ranked (i.e. “both” means “both teams were ranked”). “all” is all of the data pooled together.\nThe main takeaway from the table above is that when ESPN gives a team a &lt;2% chance of winning a game, that tends to not be a severe underestimate as I was expecting. Across the crosstabs I checked, even the high end of a 95% confidence interval for the proportion of the time that the underdog would go on to win was below the 2% threshold."
  },
  {
    "objectID": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#wrapping-up",
    "href": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#wrapping-up",
    "title": "Exploring the Tail Behavior of ESPN’s Win Probability Model",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nAll told, I didn’t end up confirming my suspicions. At least from a cursory look through the data, ESPN’s model seems to be performing quite well in the tails. Or, at the very least, it’s not making predictions that are as ridiculous as I had thought they were. I still have my suspicions and will surely continue finding individual cases that don’t make sense to me intuitively, but after poking around a little I at least feel less concerned about the model making egregious predictions – as far as I can tell, it’s doing a pretty good job on average."
  },
  {
    "objectID": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#future-work",
    "href": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#future-work",
    "title": "Exploring the Tail Behavior of ESPN’s Win Probability Model",
    "section": "Future Work",
    "text": "Future Work\nA couple of other things jump out at me as being worth exploring:\n\nHow well did the model do vs. my intuitions? In games where I was on the record as thinking the win probabilities given were far too high (or low), how do I perform?\nHow does ESPN’s model perform by other common ML metric standards? For instance, does its AUC outperform (e.g.) Vegas? (Almost certainly not). Or how negative is the model’s Brier Skill Score when using Vegas as a baseline?\nDoes the model perform better or worse for certain teams? Maybe some teams are being consistently overrated or underrated by the model."
  },
  {
    "objectID": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#appendix",
    "href": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#appendix",
    "title": "Exploring the Tail Behavior of ESPN’s Win Probability Model",
    "section": "Appendix",
    "text": "Appendix\nYou can find the code to reproduce this analysis on my Github."
  },
  {
    "objectID": "posts/2023-03-25-balancing-classes/balancing-classes.html",
    "href": "posts/2023-03-25-balancing-classes/balancing-classes.html",
    "title": "Balancing Classes in Classification Problems",
    "section": "",
    "text": "In my last post I wrote about common classifications metrics and, especially, calibration.\nWith calibration in mind, this post will show why balancing your classes – which is an all-too-common practice when working on classification problems – is generally a bad idea and leads to poorly calibrated models."
  },
  {
    "objectID": "posts/2023-03-25-balancing-classes/balancing-classes.html#introduction",
    "href": "posts/2023-03-25-balancing-classes/balancing-classes.html#introduction",
    "title": "Balancing Classes in Classification Problems",
    "section": "",
    "text": "In my last post I wrote about common classifications metrics and, especially, calibration.\nWith calibration in mind, this post will show why balancing your classes – which is an all-too-common practice when working on classification problems – is generally a bad idea and leads to poorly calibrated models."
  },
  {
    "objectID": "posts/2023-03-25-balancing-classes/balancing-classes.html#some-example-data",
    "href": "posts/2023-03-25-balancing-classes/balancing-classes.html#some-example-data",
    "title": "Balancing Classes in Classification Problems",
    "section": "Some Example Data",
    "text": "Some Example Data\nFor the purposes of this example, I’ll use the Wisconsin breast cancer data. The data is built into the mlbench package in R and scikit-learn in python. You can also get it from the UCI Machine Learning Repository.\nI’ll only be using cl_thickness, which is the indicator for clump thickness.\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(janitor)\nlibrary(purrr)\nlibrary(mlbench)\n\ndata(BreastCancer)\n\ndata &lt;- BreastCancer %&gt;%\n  clean_names() %&gt;%\n  transmute(\n    cl_thickness = as.numeric(cl_thickness), \n    class\n  ) %&gt;%\n  as_tibble()\n\ndata %&gt;%\n  slice_sample(n = 5) %&gt;%\n  pretty_print()\n\n\n\n\ncl_thickness\nclass\n\n\n\n\n1\nbenign\n\n\n3\nbenign\n\n\n5\nmalignant\n\n\n4\nbenign\n\n\n1\nbenign\n\n\n\n\n\n\n\nThe data is imbalanced: There are far more (about 2x) benign tumors than malignant ones in the sample.\n\ndata %&gt;%\n  count(class) %&gt;%\n  mutate(prop = n / sum(n)) %&gt;%\n  pretty_print()\n\n\n\n\nclass\nn\nprop\n\n\n\n\nbenign\n458\n0.6552217\n\n\nmalignant\n241\n0.3447783"
  },
  {
    "objectID": "posts/2023-03-25-balancing-classes/balancing-classes.html#model-fitting",
    "href": "posts/2023-03-25-balancing-classes/balancing-classes.html#model-fitting",
    "title": "Balancing Classes in Classification Problems",
    "section": "Model Fitting",
    "text": "Model Fitting\nWith that class imbalance in mind, let’s get to model fitting. The first thing I’ll do is fit a simple logistic regression model to predict the class (either malignant or benign) from the clump thickness.\nFirst, I’ve written a bit of tidymodels helper code below for reuse later.\n\nlibrary(tidymodels)\n\nfit_model &lt;- function(data, spec) {\n  spec &lt;- set_mode(spec, \"classification\")\n  \n  rec &lt;- recipe(\n    class ~ cl_thickness,\n    data = data\n  )\n  \n  wf &lt;- workflow() %&gt;%\n    add_model(spec) %&gt;%\n    add_recipe(rec)\n  \n  fit(wf, data)\n}\n\npredict_prob &lt;- function(model, data) {\n  predict(model, data, type = \"prob\")$.pred_malignant\n}\n\nNow, I’ll fit a simple logistic regression model by specifying logistic_reg() as the model specification in fit_model().\n\nlibrary(probably)\n\nunbalanced_model &lt;- fit_model(\n  data,\n  logistic_reg()\n)\n\npreds &lt;- tibble(\n  truth = data$class,\n  truth_int = as.integer(data$class) - 1,\n  estimate = predict_prob(unbalanced_model, data)\n)\n\nAnd now we can make a calibration plot of our predictions. Remember, the goal is to have the points on the plot lie roughly along the line y = x. Lying below the line means that our predictions are too high, and above the line means our predictions are too low.\n\ncal_plot_breaks(preds, truth = truth_int, estimate = estimate)\n\n\n\n\nAwesome! Even with the class imbalance, our model’s probability predictions are well-calibrated. In other words, when we predict that there’s a 25% chance that a tumor is malignant, it’s actually malignant about 25% of the time."
  },
  {
    "objectID": "posts/2023-03-25-balancing-classes/balancing-classes.html#balancing-the-training-data",
    "href": "posts/2023-03-25-balancing-classes/balancing-classes.html#balancing-the-training-data",
    "title": "Balancing Classes in Classification Problems",
    "section": "Balancing the Training Data",
    "text": "Balancing the Training Data\nSo then, what happens if we balance the training data as we’re so often told to do? First, let’s balance by undersampling from the majority class.\n\nminority_class &lt;- data %&gt;%\n  count(class) %&gt;%\n  filter(n == min(n))\n\nbalanced &lt;- data %&gt;%\n  group_split(class, .keep = TRUE) %&gt;%\n  map_dfr(\n    ~ {\n      if (.x$class[1] == minority_class$class) {\n        .x\n      } else {\n        slice_sample(\n          .x,\n          n = minority_class$n,\n          replace = FALSE\n        )\n      }\n    }\n  )\n\nbalanced %&gt;%\n  count(class) %&gt;%\n  pretty_print()\n\n\n\n\nclass\nn\n\n\n\n\nbenign\n241\n\n\nmalignant\n241\n\n\n\n\n\n\n\nNow we have the same number of observations for each class. Let’s go ahead and fit another logistic regression model, but this time on the balanced data.\n\nbalanced_model &lt;- fit_model(balanced, logistic_reg())\n\npreds$balanced_preds &lt;- predict_prob(\n  balanced_model,\n  data\n)\n\ncal_plot_breaks(preds, truth = truth_int, estimate = balanced_preds)\n\n\n\n\nAll of a sudden, our model is very poorly calibrated. We’re consistently overpredicting the probability of a tumor being malignant. Why is that? Think back to what we just did: We removed a bunch of examples of benign tumors from our training data.\nLet’s think about that from first principles for a minute. If you had no information at all, a reasonable guess for whether or not a tumor is malignant would be the overall proportion of tumors that are malignant. In the unbalanced data, that number was about 34%. But after balancing, it’s now 50%. That means that we’ve just biased our “no-information” prediction upwards by about 16 percentage points (or 50%). And so it shouldn’t be surprising that in our calibration plot above, we see that we’re consistently over-predicting. Our probabilities are too high because the baseline rate at which the true class appears in our training data has just increased significantly.\n\nAn important note which I’ll circle back to later is that this intuition about a baseline guess is directly rated to the intercept term of the logistic regression model you fit."
  },
  {
    "objectID": "posts/2023-03-25-balancing-classes/balancing-classes.html#smote",
    "href": "posts/2023-03-25-balancing-classes/balancing-classes.html#smote",
    "title": "Balancing Classes in Classification Problems",
    "section": "SMOTE",
    "text": "SMOTE\n“But no!” you might be thinking. “Why would you just undersample directly? You’re supposed to use an algorithm like SMOTE to overcome your class imbalance problem.”\nGreat! Let’s see if using SMOTE fixes our calibration issues. I’ll first use SMOTE to intelligently oversample the minority class.\n\nlibrary(themis)\n\nsmote &lt;- recipe(class ~ cl_thickness, data = data) %&gt;%\n  step_smote(class) %&gt;%\n  prep() %&gt;%\n  bake(new_data = NULL)\n\nsmote %&gt;%\n  count(class) %&gt;%\n  pretty_print()\n\n\n\n\nclass\nn\n\n\n\n\nbenign\n458\n\n\nmalignant\n458\n\n\n\n\n\n\n\nNow that we have balanced classes thanks to SMOTE, let’s fit another logistic regresion model and see if it’s any better-calibrated.\n\nsmote_model &lt;- fit_model(smote, logistic_reg())\n\npreds$smote_preds &lt;- predict_prob(\n  smote_model,\n  data\n)\n\ncal_plot_breaks(preds, truth = truth_int, estimate = smote_preds)\n\n\n\n\nInteresting – same problem. With SMOTE, we still make very similar errors to the ones we made in the case where we naively undersampled from our majority class. But let’s think back to first principles again, because the exact same rationale applies. When we undersampled, we ended up artificially increasing the baseline rate of malignant tumors in our training data, which resulted in predictions that were too high. With SMOTE, we’re doing the exact same thing: We’ve stil rebalanced our data to 50/50, we’ve just done it a fancier way. So of course we’ll have the same problem with overprediction."
  },
  {
    "objectID": "posts/2023-03-25-balancing-classes/balancing-classes.html#a-random-forest",
    "href": "posts/2023-03-25-balancing-classes/balancing-classes.html#a-random-forest",
    "title": "Balancing Classes in Classification Problems",
    "section": "A Random Forest",
    "text": "A Random Forest\n“But no!” you might be thinking. “You need to use a more complicated model like a random forest, because logistic regression won’t pick up on complexities in your data well enough to be well-calibrated.”\nGreat! Let’s try a random forest:\n\nrf &lt;- fit_model(smote, rand_forest())\n\npreds$rf_preds &lt;- predict_prob(\n  rf,\n  data\n)\n\ncal_plot_breaks(preds, truth = truth_int, estimate = rf_preds)\n\n\n\n\nSame issue again, albeit not quite as severe. And the same logic holds. In fact, it’s even more straightforward with tree-based models. In a decision tree, you would determine a predicted probability by seeing what proportion of the labels in the leaf node that you end up in based on the features belong to the positive class. But then there’s the same logic as before: We’ve just artifically increased the number of instances of the positive class dramatically, so of course the proportion of labels belonging to the positive class in our leaf nodes will increase."
  },
  {
    "objectID": "posts/2023-03-25-balancing-classes/balancing-classes.html#coefficients",
    "href": "posts/2023-03-25-balancing-classes/balancing-classes.html#coefficients",
    "title": "Balancing Classes in Classification Problems",
    "section": "Coefficients",
    "text": "Coefficients\nLooking at the coefficients of our three models can help understand what’s going on here.\n\nlist(\n  \"Original\" = unbalanced_model, \n  \"Undersampled\" = balanced_model, \n  \"Smote\" = smote_model\n) %&gt;%\n  map(tidy) %&gt;%\n  imap(~ select(.x, \"Term\" = term, !!.y := \"estimate\")) %&gt;%\n  reduce(inner_join, by = \"Term\") %&gt;%\n  pretty_print()\n\n\n\n\nTerm\nOriginal\nUndersampled\nSmote\n\n\n\n\n(Intercept)\n-5.1601677\n-4.0441444\n-4.4363999\n\n\ncl_thickness\n0.9354593\n0.8190776\n0.9159715\n\n\n\n\n\n\n\nIn all three models, the coefficient associated with the clump thickness is very similar. This should make sense intuitively: Our sampling was at random, so the relationship between the clump thickness and whether or not the tumor was malignant shouldn’t change at all.\nThe thing that does change, though, is the intercept term. In both the model where we undersampled from the majority class and in the SMOTE model, the intercept term is significantly higher than it is in the original model on the unbalanced data. This should feel similar intuitively to the idea of the baseline guess from before. As its core, the intercept term in your logistic regression model is the guess you’d make with “no” information (in this particular case, no information means a clump thickness of 0).\nWe can illustrate this more clearly with three intercept-only models:\n\nunbalanced_intercept_only &lt;-  glm(class ~ 1, data = data, family = binomial)\nundersampled_intecept_only &lt;- glm(class ~ 1, data = balanced, family = binomial)\nsmote_intercept_only &lt;-       glm(class ~ 1, data = smote, family = binomial)\n\nNow, let’s compare the intercept coefficients of these three models on a probability (read: not a log-odds) scale.\n\nconvert_log_odds_to_probability &lt;- function(x) {\n  odds &lt;- exp(x)\n  odds / (1 + odds)\n}\n\nunbalanced_intercept &lt;-   coef(unbalanced_intercept_only)\nundersampled_intercept &lt;- coef(undersampled_intecept_only)\nsmote_intercept &lt;-        coef(smote_intercept_only)\n\nintercepts &lt;- tibble(\n  original =     convert_log_odds_to_probability(unbalanced_intercept),\n  undersampled = convert_log_odds_to_probability(undersampled_intercept),\n  smote =        convert_log_odds_to_probability(smote_intercept)\n)\n\npretty_print(intercepts)\n\n\n\n\noriginal\nundersampled\nsmote\n\n\n\n\n0.3447783\n0.5\n0.5\n\n\n\n\n\n\n\nAnd it’s just as we expected: The intercept coefficient in the SMOTE model and the undersampling model are exactly 1/2, which corresponds to the fact that we balanced the classes to be exactly 50/50. And the intercept in the original model with the unbalanced classes is exactly the percentage of the data made up by the true class (malignant)."
  },
  {
    "objectID": "posts/2023-03-25-balancing-classes/balancing-classes.html#when-to-rebalance",
    "href": "posts/2023-03-25-balancing-classes/balancing-classes.html#when-to-rebalance",
    "title": "Balancing Classes in Classification Problems",
    "section": "When To Rebalance",
    "text": "When To Rebalance\nThere are some times where re-balancing the classes in your training data might make sense. One application that comes to mind is if you have strong prior information that your training data is actually biased, and is over-representing one of the two classes.\nFor instance, let’s imagine we have data from 1000 breast cancer patients and we know a priori that about 20% of tumors are malignant, but in the training data, maybe 40% of the tumors we have are malignant. Depending on the long-term goal of the project, it might make sense to undersample from the malignant cases to get the overall rate of tumors being malignant down to around the 20% prior.\nThe rationale behind doing this would be that if you wanted your model to generalize well to future cases (outside of your training set) and you knew that in the broader population about 20% of cases are malignant, your biased training data could very well result in biased predictions out-of-sample even if your predictions look good in-sample.\nAnother case where rebalancing can make sense is if you plan to use a technique like Platt Scaling or Isotonic Regression to re-calibrate your predictions ex-post. These methods are a bit beyond the scope of this post, but they’re both fantastic ways to make sure a model is giving well-calibrated predictions while using classifiers that don’t guarantee calibration, such as tree-based models. For instance, a combination of upsampling with SMOTE, using an often poorly-calibrated classifier like a tree booster such as XGBoost, and then re-calibrating ex-post with a Platt Scaler can result in a rare win-win scenario: Well-calibrated predictions, but also improved performance on normal classification metrics like the F1 score or AUC."
  },
  {
    "objectID": "posts/2023-03-25-balancing-classes/balancing-classes.html#conclusion",
    "href": "posts/2023-03-25-balancing-classes/balancing-classes.html#conclusion",
    "title": "Balancing Classes in Classification Problems",
    "section": "Conclusion",
    "text": "Conclusion\nIn the previous post, I wrote about calibration. In short, I think calibration is the single most important metric in evaluating the performance of classification models.\nAnd so with that in mind, the main takeaway of this post is that you should be very careful about trying to “fix” the “problem” of class imbalances when you’re working on classification problems. If I could summarize the principle that I would follow in just a sentence, it would be that class imbalances often reflect important information about the prevalence of your classes in the real world, and it’s often risky to dismiss that information in the name of having data that’s split equally. In other words, class imbalances are usually not a problem at all. They’re a feature, not a bug."
  },
  {
    "objectID": "posts/2023-03-20-metrics-calibration/calibration.html",
    "href": "posts/2023-03-20-metrics-calibration/calibration.html",
    "title": "Calibration and Evaluating Classification Models",
    "section": "",
    "text": "There’s something of a complexity trajectory in evaluating classification models that I’ve observed over the past few years. It starts with accuracy. But soon after learning about accuracy, data scientists are taught that accuracy is problematic for two reasons:\n\nIt doesn’t work well with unbalanced classes. This is the “if 95% of people don’t have cancer and you always predict ‘no cancer’, your model isn’t actually good” argument.\nIt doesn’t make any distinction between types of errors. In particular, it weighs false positives and false negatives equally, which may not be appropriate for the problem being solved.\n\nThese are both perfectly valid drawbacks of using accuracy as a metric. So then we move on. Next stop: precision and recall."
  },
  {
    "objectID": "posts/2023-03-20-metrics-calibration/calibration.html#introduction",
    "href": "posts/2023-03-20-metrics-calibration/calibration.html#introduction",
    "title": "Calibration and Evaluating Classification Models",
    "section": "",
    "text": "There’s something of a complexity trajectory in evaluating classification models that I’ve observed over the past few years. It starts with accuracy. But soon after learning about accuracy, data scientists are taught that accuracy is problematic for two reasons:\n\nIt doesn’t work well with unbalanced classes. This is the “if 95% of people don’t have cancer and you always predict ‘no cancer’, your model isn’t actually good” argument.\nIt doesn’t make any distinction between types of errors. In particular, it weighs false positives and false negatives equally, which may not be appropriate for the problem being solved.\n\nThese are both perfectly valid drawbacks of using accuracy as a metric. So then we move on. Next stop: precision and recall."
  },
  {
    "objectID": "posts/2023-03-20-metrics-calibration/calibration.html#precision-and-recall",
    "href": "posts/2023-03-20-metrics-calibration/calibration.html#precision-and-recall",
    "title": "Calibration and Evaluating Classification Models",
    "section": "Precision and Recall",
    "text": "Precision and Recall\nPrecision and recall are the two next most common classification metrics I’ve seen. Precision is the percentage of the time that your model is correct when it labels something as true. Recall is the percentage of the actual true examples that your model labels as true. These metrics are important for different reasons.\nA very precise model doesn’t make very many Type I errors. For instance, if you’re predicting whether or not someone has cancer, a very precise model is “trustworthy” in the sense that if it tells you that they do have cancer, they most likely do. You might think about precision as a metric in hiring: You probably want your hiring process to be very good at evaluating good candidates. A high-precision hiring process would mean that when you think you’ve found a person who would be a great fit on your team, you’re very likely correct.\nRecall is a bit different: It’s the percentage of the true labels that your model finds. A high-recall model suffers few false negatives: When something actually belongs to the true class, your model very often predicts it as such. You might think about this in the context of our cancer example from before. A higher recall model would mean that your model catches more of the cancer cases.\nDepending on your use case, you might optimize for one of these or the other. Or you could use a blend of the two, the most common of which is the F1 score, which is the harmonic mean of precision and recall. The idea of the F1 score is to optimize for a balance of both precision and recall, as opposed to optimizing for one at the cost of the other."
  },
  {
    "objectID": "posts/2023-03-20-metrics-calibration/calibration.html#predicting-probabilities",
    "href": "posts/2023-03-20-metrics-calibration/calibration.html#predicting-probabilities",
    "title": "Calibration and Evaluating Classification Models",
    "section": "Predicting Probabilities",
    "text": "Predicting Probabilities\nYou might be reading this thinking about how this is all about predicting classes, but very often we care about predicting probabilities. For instance, at CollegeVine we make predictions about each student’s chances of getting into their favorite colleges and universities. It’s not useful for students if we predict an acceptance or a rejection. After all, they want to know their chance, and to make a class prediction would mean that we determine a cutoff point at which we decide that if someone’s chance is above that threshold, they’ll get in. And if not, they won’t.\nThe problem is that there is no such threshold. More likely, college admissions is a bit of a game of chance: If five students each have a 20% chance of getting in to Carleton, for instance, I’d expect that about one of the five would get in on average. But it’d be disingenuous to make a class prediction, and I’m not sure how we’d even do that. For the five previously mentioned students, we’d expect one to get in. But if we were predicting classes we’d either predict all five to be accepted or all five to be rejected depending on where we set our threshold, and neither of those is the most likely scenario.\nWith all of that in mind, what metrics do we use instead? There are three metrics that we look at when we’re evaluating our models: The Brier Skill Score, AUC, and calibration. I’ve already written about AUC, and for the purposes of this post, I’m going to focus on the one I view as the most important: Calibration."
  },
  {
    "objectID": "posts/2023-03-20-metrics-calibration/calibration.html#calibration",
    "href": "posts/2023-03-20-metrics-calibration/calibration.html#calibration",
    "title": "Calibration and Evaluating Classification Models",
    "section": "Calibration",
    "text": "Calibration\nThe most important metric we track is calibration, which we’ll often evaluate by looking at a calibration plot. Below is an example of such a plot from FiveThirtyEight\n\nThe idea behind calibration is to answer the basic question “When my model said that someone had a 31% chance of winning an election, did they actually win about 31% of the time?” As you get more data, you can group your predictions into buckets to answer this question. For instance, if we’re predicting college chances we might take all the times that we said someone had between a 30 and 35% chance of getting accepted, and we’d calculate the actual proportion of the time that they were accepted. If it’s about 32.5% (close to the mean prediction in our bucket), we’d say that our model is making well-calibrated predictions in that bucket. If our model makes well-calibrated predictions of admissions chances across all buckets, it’s fair to say that it’s well-calibrated in general.\nCalibration is important because it directly affects how we interpret our model’s predictions. If we’re making probability predictions and they’re not well-calibrated, then when we say something has a 25% chance of happening, or a 50% chance of happening, or a 90% chance of happening, those numbers aren’t actually meaningful. It might be the case that the 50% probablity event happens more than the 25% one and less that the 90% one, but that isn’t even a guarantee. It also is probably not the case with a badly-calibrated model that the 50% predicted probability event happens twice as often as the 25% one.\nFor instance, let’s imagine we’re working on a classic machine learning example (and real-world) problem: Email spam detection. Ultimately, we need to predict a class: Given an email, we need our model to tell us if it’s spam or not. But for a large proportion of classifiers, this requires setting a probability threshold. For instance, if our model says there’s a greater than 50% chance that some email is spam, we mark it as spam. If false positives (a real email being marked as spam) are more problematic than false negatives (a spam email being marked as not spam), then we might increase our probability threshold to 80%, for example, which would make it “harder” for an email to be marked as spam. But with the higher threshold, the emails that we do mark as spam we’re more confident about. Often times, we’ll use domain knowledge to determine this threshold. I’ve had many conversations where we set arbitrary thresholds based on our experience or our gut instincts about how the system we’re working in should work. Often, those conversations end with something like “80% feels about right” and we go with that.\nHopefully you’re starting to see the issue here: If our model’s predictions are poorly calibrated, then it’s not possible to make a decision like that. We can’t lean into our domain knowledge about any particular number being a threshold that makes sense, because the probabilities we’re predicting don’t actually mean anything in practice. In other words, the fact that we don’t know if when we say 80%, it’s actually 95% or 35% or some other number makes it impossible to make decisions based on our predictions. In short, if our predictions aren’t well-calibrated, it’s not possible to reason about them in any meaningful way. They can also very easily be misleading.\nAs I’ve mentioned before, this is especially important is the probabilities themselves are the prediction. If you’re telling a student that their chances of getting into Carleton are 27%, it goes without saying that when you say they have a 27% chance, that if they were to apply four times they’d get in about once on average. If they get in about one in eight times instead, the miscalibrated prediction could have a meaningful, negative effect on their college outcomes. For instance, if you severely overpredict a student’s chances and they end up applying to fewer schools as a result, there’s an increased likelihood of them getting in nowhere, which would be a particularly bad outcome. In this case, better-calibrated predictions lead directly to better decision-making.\nIn statistics world, it might be helpful to think of poor calibration as a bias issue: You might think of better-calibrated predictions as being less biased, in the sense that the expected value of the outcome (the actual long-run frequency of a student being accepted) is closer to what your prediction was."
  },
  {
    "objectID": "posts/2023-03-20-metrics-calibration/calibration.html#wrapping-up",
    "href": "posts/2023-03-20-metrics-calibration/calibration.html#wrapping-up",
    "title": "Calibration and Evaluating Classification Models",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nHopefully I’ve convinced you that the calibration of your classifier’s predictions is important and practically meaningful. If a model’s predictions are poorly calibrated, it’s difficult (or impossible) to reason about them in a practical sense. Miscalibrated predictions can also be misleading, because we often naturally interpret probabilities as long-run frequencies. For instance “If I flip this coin 100 times, it’ll come up heads about half of the time.” You might think about working with poorly calibrated predictions being similar to flipping a biased coin when you’re unaware of its biases.\nIn the next post, I’ll talk about class imbalances and why balancing your classes can be a particularly bad idea."
  },
  {
    "objectID": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html",
    "href": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html",
    "title": "Our 2021 Big Data Bowl Submission",
    "section": "",
    "text": "This post is my team’s 2021 NFL Big Data Bowl submission. My team was made up of me, Hugh McCreery (Baltimore Orioles), John Edwards (Seattle Mariners), and Owen McGrattan (DS student at Berkeley). I’m proud of what we’ve put forth here, and hopefully you find it interesting. At the end (after the Appendix), I’ve added some overall thoughts on things we were curious about or feel like we could have done better, as well as what we view as the biggest strengths and weaknesses of our submission. Enjoy!"
  },
  {
    "objectID": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#meta",
    "href": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#meta",
    "title": "Our 2021 Big Data Bowl Submission",
    "section": "",
    "text": "This post is my team’s 2021 NFL Big Data Bowl submission. My team was made up of me, Hugh McCreery (Baltimore Orioles), John Edwards (Seattle Mariners), and Owen McGrattan (DS student at Berkeley). I’m proud of what we’ve put forth here, and hopefully you find it interesting. At the end (after the Appendix), I’ve added some overall thoughts on things we were curious about or feel like we could have done better, as well as what we view as the biggest strengths and weaknesses of our submission. Enjoy!"
  },
  {
    "objectID": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#introduction",
    "href": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#introduction",
    "title": "Our 2021 Big Data Bowl Submission",
    "section": "Introduction",
    "text": "Introduction\nOur project aims to measure the ability of defensive backs at performing different aspects of their defensive duties: deterring targets (either by reputation or through good positioning), closing down receivers, and breaking up passes. We do this by fitting four models, two for predicting the probability that a receiver will be targeted on a given play and two for predicting the probability that a pass will be caught, which we then use to aggregate the contributions of defensive backs over the course of the season."
  },
  {
    "objectID": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#modeling-framework",
    "href": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#modeling-framework",
    "title": "Our 2021 Big Data Bowl Submission",
    "section": "Modeling Framework",
    "text": "Modeling Framework\nWe opted to use XGBoost for all of our models. At a high level, we chose a tree booster for its ability to find complex interactions between predictors, something we anticipated would be necessary for this project. Tree boosting also allows for null values to be present, which helped us divvy up credit in the catch probability models (more on this later). Finally, tree boosting is a relatively simple, easy-to-tune algorithm that generally performs extremely well, as was the case for us."
  },
  {
    "objectID": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#catch-probability",
    "href": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#catch-probability",
    "title": "Our 2021 Big Data Bowl Submission",
    "section": "Catch Probability",
    "text": "Catch Probability\nOur catch probability model has two distinct components: The catch probability at throw time – as in, the chance that the pass is caught at the time the quarterback releases the ball – and the catch probability at arrival time – the chance the pass is caught at the time the ball arrives. These probabilities are distinct since a lot can happen between throw release and throw arrival. First, we will walk through the features that are used in building each model.\nFor the throw time model (which we will refer to as the “throw” model) and the arrival time model (the “arrival” model), the most important predictors by variable importance were what we expected entering this project: the distance of the receiver to the closest defender, the position of the receiver in the \\(y\\) direction (i.e. distance from the sideline), the distance of the throw, the position of the football in the \\(y\\) direction at arrival time (this is mostly catching throws to the sideline and throw aways), the velocity of the throw, the velocity and acceleration of the targeted receiver, and a composite receiver skill metric. For the arrival model, we use many of the same features. However, we do not account for the distance of the defenders to the throw vector – which accounts for the ability to break up a pass mid-flight – because the throw has already arrived.\nThese two models both perform quite well, and far better than random chance. The throw model accurately predicts \\(74\\%\\) of all passes, with strong precision (\\(84\\%\\)) and recall (\\(76\\%\\)), and an AUC of \\(0.81\\). As can be expected, our arrival model outperforms the throw model in all measures - accurately predicting \\(78\\%\\) of all passes with a precision of \\(88\\%\\), a recall of \\(79\\%\\), and an AUC of \\(.87\\). All of these metrics were calculated on a held-out data set not used in model training. Below are plots of the calibration of the predictions of each of the models on the same held-out set.\n \nWe can do a few particularly interesting things with the predictions from these two models in tandem. Namely, we can use the two to calculate marginal effects of the play of the defensive backs. A simple example is as follows: For a given pass attempt, our throw model estimates that there is a \\(80\\%\\) chance of a catch. By the time that pass arrives, however, our arrival model estimates instead that there is a \\(50\\%\\) chance of a catch. Ultimately, the play results in a drop. In total, our defense can get credit for \\(+.8\\) drops, but we can break it down into \\(+.3\\) drops worth from closing on the receiver and \\(+.5\\) drops from breaking up the play, based on how the individual components differ. In other words, we subtract the probability of a catch at arrival time from the probability at throw time to get the credit for closing down the receiver, and we subtract the true outcome of the play from the probability of a catch at arrival time to get the credit for breaking up the pass.\nThe main challenge comes not from calculating the overall credit on the play, but from the distribution of credit among the defenders. In the previous example where we have to credit the defense with \\(+.8\\) drops added, who exactly on the defense do we give that credit to? There are a couple of heuristics that might make sense. One option would be to just split the credit up evenly among the defense, but this would be a bad heuristic because some defenders will have more of an impact on a pass being caught than others, and thus deserve more credit. We might also give all of the credit to the nearest defender, but that would be unfair to players who are within half a yard of the play and are also affecting its outcome but would get no credit under this heuristic. Ultimately, we opted to use the models to engineer the credit each player deserves by seeing how the catch probabilities would change if we magically removed them from the field, which we believe to be a better heuristic than the previous ones described. To implement this heuristic, we remove one defender from our data and re-run the predictions to see how big the magnitude of the change in catch probability is. The bigger the magnitude difference, the more credit that player gets. Then, we calculate the credit each defender gets with \\(credit_{i} = \\frac{min(d_{i}, 0)}{\\sum_{i} min(d_{i}, 0)}\\) where \\(d_{i}\\) is the catch probability without that player on the field minus the catch probability with him on the field. In other words, if one player gets \\(75\\%\\) of the credit for a play and the play is worth \\(+.8\\) drops added, then that player gets \\(.8 \\cdot .75 = +.6\\) drops of credit, and the remaining \\(+.2\\) drops are divvied up amongst the other defenders in the same fashion."
  },
  {
    "objectID": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#target-probability",
    "href": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#target-probability",
    "title": "Our 2021 Big Data Bowl Submission",
    "section": "Target Probability",
    "text": "Target Probability\nOur target model is based on comparing the probabilities that a receiver is targeted before the play begins and when the ball is thrown with the actual receiver targeted. We can use these probabilities to make estimates of how well the defender is covering (are receivers less likely to be thrown the ball because of the pre-throw work of a defensive back?) and how much respect they get from opposing offenses (do quarterbacks tend to make different decisions at throw time because of the defensive back?).\nTo determine the probability of a receiver being targeted before the play, we chose to take a naive approach. Each receiver on the field is assigned a “target rate” of \\(\\frac{targets}{\\sqrt{plays}}\\), which is then adjusted for the other receivers on the field and used as the only feature for a logit model. The idea of this rate was to construct a statistic that rewarded receivers for showing high target rates over a large sample but also gave receivers who play more often more credit.\nThe model for target probability at the time of throw was a tree booster similar to the two catch probability models. This model uses positional data, comparing the receiver position to the QB and the three closest defenders along with a variety of situational factors such as the distance to the first down line, time left, weather conditions, and how open that receiver is relative to others on the play to determine how likely that receiver is to be targeted.\nThe pre-snap model, which by design only considers the players on the field for the offense, performs relatively well for the lack of information with an AUC of \\(.59\\) and is well-calibrated on a held-out dataset. The pre-throw model performs much better given the extra information, with \\(89\\%\\) recall, \\(94\\%\\) precision, and \\(.94\\) AUC. Calibration plots of the models on held-out data are below.\n \nWe can estimate how a defensive back is impacting the decisions made by the quarterback through two effects. The first, comparing the target probability before the play to the target probability at the time of the throw is meant to estimate how well the receiver is covered on the play. For example, consider a receiver with a target probability of \\(20\\%\\) before the snap who ends up open enough to get a target probability of \\(50\\%\\) when the ball is thrown. This difference is attributed to the closest defensive back who would be credited with \\(-0.3\\) coverage targets. If on the same play another receiver had a pre-snap target probability \\(30\\%\\) but a target probability of \\(0\\%\\) at throw time, the closest defensive back would be credited with \\(+0.3\\) coverage targets. The other effect attempts to measure how the quarterback is deterred from throwing when that particular defensive back is in the area of the receiver by comparing the probability of a target to the actual result. So if a certain receiver has a target probability of \\(60\\%\\) at the time of the throw and isn’t targeted, the closest defender is credited with \\(+0.6\\) deterrence targets."
  },
  {
    "objectID": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#results",
    "href": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#results",
    "title": "Our 2021 Big Data Bowl Submission",
    "section": "Results",
    "text": "Results\nHaving produced these four models that gave us estimates of the influence of the defensive backs on a given play, we can accumulate the results over an entire season to produce an estimate of individual skill across the dimensions described by the models. As there is no straightforward way to measure the relative value of these skills, we chose to combine the individual skill percentiles for each defender as a measure of their overall skill. With that as the estimate, these are our top 15 pass-defending defensive backs in 2018:\n\n\n\nleaderboard\n\n\nA full leaderboard of these can be found on our Shiny app in the Overall Rankings tab. To help display these results and a few other metrics (such as how difficult the defensive assignment was based on pre-snap target probability and actual separation from the receiver at throw time) we developed player cards for each qualifying defender. For example, this is our card for #1 defender Richard Sherman:\n\n\n\nsherman_card\n\n\nThese cards can also be found on our Shiny app under Player Cards."
  },
  {
    "objectID": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#further-work",
    "href": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#further-work",
    "title": "Our 2021 Big Data Bowl Submission",
    "section": "Further Work",
    "text": "Further Work\nThere are several things that we didn’t consider or that would be interesting extensions of this project. Two clear ones are interceptions added and fumbles added, as those are hugely impactful football plays that can swing the outcomes of games. We also only considered raw changes in aggregating the player stats (i.e. targets prevented and drops added), but using EPA added instead would certainly be a better metric, since not all drops are created equal. In addition, it is not clear that a drop added is worth the same as a target deterred – an assumption we made – and EPA would help solve this problem too. It would also be interesting to test how similar our metrics are between seasons to confirm that our metrics are measuring the stable skill of a defender."
  },
  {
    "objectID": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#appendix",
    "href": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#appendix",
    "title": "Our 2021 Big Data Bowl Submission",
    "section": "Appendix",
    "text": "Appendix\nAll of our code is hosted in two Github repos: hjmbigdatabowl/bdb2021, which hosts our modeling code, and hjmbigdatabowl/bdb2021-shiny, which hosts the Shiny app.\nThere is extensive documentation for our code on our pkgdown site.\nThe Shiny app can be found here, which lets you explore our models, results, and player ratings."
  },
  {
    "objectID": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#thoughts",
    "href": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#thoughts",
    "title": "Our 2021 Big Data Bowl Submission",
    "section": "Thoughts",
    "text": "Thoughts\nFirst, a disclaimer for everything that follows: I’m extremely proud of what we submitted, and the work that we did. We spent hundreds of hours working on this project over about three months, which is an enormous undertaking for four people with other full-time commitments. Especially in the final weeks, I was spending well over ten hours a week working on this project, and probably closer to fifteen or twenty. All of that said, in the grand scheme of things, the 300 hours or so collective hours that we spent conceiving of and working on this project is nowhere near the amount of time and effort that could be poured into a project that a data science team of four was working on full-time for three months. Given more time, I believe there are significant ways in which we could have improved our final product.\n\nStrengths\n\nCommunication and interpretability of the results. At the end of the day, when you work in a front office (as Hugh, John, and I know), you need coaches to believe what you’re telling them, and building a product that they can understand and explain is a major step in that direction. Overly complex or convoluted methodology will only get your work ignored, and I think that we did a particularly good job of building a product that is interpretable and useful.\nBuilding a model that evaluates something useful. At the end of the day, I’m happy with the statistics we chose to put forward. In my view, barring special events like fumbles forced, tackling, and interceptions (all valuable things that should be worked on further to expand this project), the way that we evaluate defensive back performance by looking at what we view as the four major components – deterring passes by reputation, deterring passes with good positioning and coverage, closing down receivers, and breaking up passes – are the four most important skills for defensive backs in the NFL.\nDivvying up credit in a clever way. I also think that the way we opted to divvy up the credit among the defenders in the two catch probability models was particularly clever, and not something that most teams would do. As we laid out in our submission, it doesn’t make sense to divvy credit evenly or give all credit to the nearest defender to the target, and I think that the approach that we took was both novel and easily defensible.\n\n\n\nWeaknesses + Potential Improvements\n\nClearly, weighing each of the four components of defense that we measured equally is the wrong way to go. A better strategy would be to try to correlate them with defensive EPA or a similar stat and use the correlation coefficients / R-Squared values / MSEs / etc. to weigh the four components based on how strongly they predict EPA. The problem, though, is that we don’t have a defensive EPA, which means we’d need to model it. In addition, it’s unclear how to model a counterfactual. What I mean by that is that for a statistic like deterrence, we need to measure the EPA caused by a defender not being thrown at, which is an inherently difficult thing to model. So difficult, in fact, that I believe that the first NFL team to come up with a good way of doing this will have a Moneyball-esque leg up against the competition until other teams catch up (Ravens analysts, I’m looking at you).\nThere are other aspects of defensive performance that we’re not measuring. Two important categories are turnovers generated and YAC prevented. Both of these are hugely valuable for preventing points, and both are models that we didn’t have time to build.\nStability testing. We’re probably interested in how stable our metrics are across seasons. For example, does Stephon Gilmore being a 96 overall this year predict his rating next year at all? Since we only have one season of data, the answer is that we don’t know. This type of stability testing would be useful for predicting future performance, though.\n\n\n\nGeneral Thoughts\nAgain, I’m proud of what we’ve done here. In particular, spot-checking some of the numbers has been fascinating. There’s often very little correlation between metrics, which seems good, and often the correlations can be negative. The way to think about this, as my teammate Hugh eloquently put it, is that we should think about the negative correlations as being similar to \\(ISO\\) vs. \\(K%\\) in the MLB: A player who is fantastic in coverage and thus often deters throws necessarily will have fewer opportunities to break up passes, meaning he will see a lower score for closing, breakups, or both. We see this type of behavior with Gilmore, Peterson, Sherman, and more. It’s also been interesting to find that our models think the same corners are really good as the eye test does, which is an encouraging sign. Interestingly, though, some of our top-ranked defensive backs are players who don’t tend to make top lists, and some of our lowest-ranked ones (like Jason McCourty and Jalen Ramsey), do. This is reminiscent of the attitude changes about certain MLB players in the early 2010s as we began developing more advanced statistics to measure performance (i.e. Derek Jeter’s defense being terrible and Michael Bourn being sneakily good).\nAnd with that, I’ve wrapped up my addendum to our submission. Feel free to reach out with any questions. My contact info is on my blog, my Github, my LinkedIn, etc. Thanks for reading!"
  },
  {
    "objectID": "posts/2021-01-13-a-gentle-introduction-to-markov-chains-and-mcmc/index.html",
    "href": "posts/2021-01-13-a-gentle-introduction-to-markov-chains-and-mcmc/index.html",
    "title": "A Gentle Introduction to Markov Chains and MCMC",
    "section": "",
    "text": "Introduction\nEvery other Friday at work we have a meeting called All Hands. During the first half of All Hands a member of the team gives a presentation, which is split up into two pieces: A personal presentation – your favorite food, TV shows, books, etc. – and a mini-lesson, which can be about any topic of interest that’s unrelated to work. Yesterday it was my turn, and I gave my mini-lesson on Markov Chains and Markov Chain Monte Carlo. This post memorializes what I covered.\n\n\nMarkov Chains\nFirst, what is a Markov Chain? It’s easiest to break it down into it’s component parts. A Markov Chain is a chain, or sequence of events, that follow the Markov Property. And the Markov Property is pretty intuitive: The Markov Property says that the next state in a sequence (chain) is only dependent on the current state. Statisticians would call this “memorylessness,” and we can write out the property in its true mathematical form below, where \\(X\\) is a random variable and \\(x\\_{t}\\) is the probability distribution that \\(X\\) takes on at time \\(t\\).\n\\[\np(x_{t+1} | x_{t}, x_{t-1}, x_{t-2}, .. x_{0}) = p(x_{t+1} | x_{t})\n\\]\nIn plain English, all this definition is saying is that if you are following the Markov Property, then where you go next is only determined by where you are now, and how you got to where you are has no impact. At the end of my talk, one of my coworkers commented that this property is actually quite beautiful in a real-world sense, and I feel the same way. It certainly could have been the example of a guiding principle that I choose to follow that I used in my personal presentation.\nSo, now that we know what a Markov Chain is, let’s walk through an example. The most commonly seen type of Markov Chain is called a random walk. Simply, a random walk is a Markov Chain where the next state is just determined by the current state plus some random noise. I’ve coded up an example below:\n\n## library(purrr)\n## library(magrittr)\n## library(ggplot2)\n\nrandomly_walk &lt;- function(.ix = c(), n_steps = 100) {\n  results &lt;- numeric(n_steps)\n  for (i in 1:(n_steps-1)) {\n    results[i + 1] &lt;- results[i] + rnorm(1, 0, 1)\n  }\n  \n  return(results)\n}\n\n(\n  random_walk &lt;- randomly_walk() %&gt;%\n    tibble(position = .)\n)\n\n# A tibble: 100 × 1\n   position\n      &lt;dbl&gt;\n 1   0     \n 2  -1.13  \n 3   1.52  \n 4  -0.0912\n 5  -0.348 \n 6  -1.66  \n 7  -0.884 \n 8  -0.491 \n 9   0.179 \n10  -0.812 \n# … with 90 more rows\n\n\nThat table shows a random walk with 100 steps, generated by adding standard normal noise to the position after each step. Let’s plot it and see what it looks like.\n\nrandom_walk %&gt;%\n  rownames_to_column(var = 'time') %&gt;%\n  mutate(time = time %&gt;% as.numeric()) %&gt;%\n  ggplot(aes(time, position)) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nCool! The walk starts at 0, and then jumps around randomly a bunch until \\(t=100\\). It’ll be more interesting once we simulate 20 random walks.\n\nn_steps &lt;- 500\nn_chains &lt;- 20\ntwenty_walks &lt;- map(\n  1:n_chains, \n  randomly_walk,\n  n_steps\n) %&gt;%\n  tibble(position = .) %&gt;%\n  unnest(cols = c(position)) %&gt;%\n  mutate(\n    time = rep(1:n_steps, n_chains),\n    walk = map(1:n_chains, rep, n_steps) %&gt;% \n      unlist() %&gt;% \n      as.factor()\n  )\n\ntwenty_walks %&gt;%\n  ggplot() +\n  aes(time, position, color = walk) +\n  geom_line() +\n  theme_minimal() + \n  theme(legend.position = 'none')\n\n\n\n\n\n\n\n\nSo, what’s going on here? It basically looks how we’d expect. At any given time point, the mean position of the 20 is about zero, but the standard deviation of those positions goes up over time. Specifically, at any given time \\(t\\), the standard deviation of the positions should be roughly equal to \\(\\sqrt t\\), because of how the variance is compounding. Remember, these random walks are Markov Chains because at every time \\(t\\), I defined the position \\(y\\_{t+1}\\) to be \\(y\\_{t} + \\mathcal{N}(0, 1)\\), or the the next position is the current position plus a standard normal noise (i.e. zero-centered with unit variance).\nCool, so now we have an idea of what a Markov Chain is and how a random walk is an example of one. Now, why do we care? What kinds of problems can we solve with Markov Chains? It turns out that one thing we can use them to do is to calculate intractable integrals. What does this mean? Well, remembering back to a calculus class once upon a time, we know if we have some function \\(f(x) = 2x\\), we can integrate that function by following a one of a couple of rules. In this case, that rule is to raise the coefficient in front of the \\(x\\) to turn it into a power, such that the new exponent equals the old one plus one, and the new coefficient equals the old one divided by the new exponent. For \\(f(x)\\), we find \\(F(x) = \\int f(x) = x^{2} + c\\), where \\(c\\) is a constant. However, in many applications, such as Bayesian statistics, we run into functions of hundreds or thousands of parameters that are intractable to integrate. In other words, even really, really powerful calculators can’t integrate them: there are just too many parameters. So, we’re stuck. How do we integrate a function that even a super powerful calculator can’t? In steps Markov Chain Monte Carlo, coming to the rescue.\n\n\nMarkov Chain Monte Carlo\nIt turns out that we can use Markov Chains to approximate the integral in cases where we can’t calculate it directly. This is an incredible powerful discovery, and one that we’ve only been able to really take advantage of in the past twenty or so years, as computing power has grown exponentially. So, how do we actually do it? Let’s frame it as a simple problem that’s isomorphic to the actual problem at hand.\nImagine you are the ruler of an island kingdom, which has four islands. Island 1 has population 1, Island 2 has population 2, Island 3 has population 3, and Island 4 has population 4. And, imagine that you want to spend time on each island proportional to the percentage of the total population of your kingdom that it makes up. In other words, you want to spend 10% of your time on Island 1, and so on. But, you have a problem: You don’t know how to add. Imagine that the only mathematical operation you know how to do is divide. Can you figure out a way to spend your time how you want without being able to calculate the total population of your kingdom?\nMost likely, how you’d solve this problem isn’t immediately obvious, but there are a few brilliant algorithms that help us achieve our goal. One of them is proposed to you by two of your friends, Metropolis and Hastings. I’ve coded up their suggestion below:\n\nrun_rwmh &lt;- function(n_iters = 1000, island_populations = 1:4) {\n  locations &lt;- numeric(n_iters)\n  \n  ## randomly choose an island to start on\n  locations[1] &lt;- sample(island_populations, 1)\n\n  for (i in 1:(n_iters-1)) {\n    \n    ## propose a new island to go to\n    proposal_island &lt;- sample(setdiff(island_populations, locations[i]), 1) \n    \n    ## if that island has more people, always go\n    if (proposal_island &gt; locations[i]) {\n      locations[i + 1] &lt;- proposal_island\n    } else {\n      ## if it has fewer people, flip a coin with probability\n      ##   proportional to the ratio of the populations to\n      ##   decide whether to go or stay\n      acceptance_probability &lt;- proposal_island / locations[i]\n      locations[i + 1] &lt;- \n        sample(\n          c(proposal_island, locations[i]), 1, \n          prob = c(acceptance_probability, 1 - acceptance_probability)\n        )\n    }\n  }\n  return(locations)\n}\n\nHere’s the algorithm your friends propose:\n\nPick a random island to start on.\nOn each day, randomly select a new island to go to (the proposal island).\nDo one of the following, depending on the populations of the islands:\n\nIf the proposal island has more people than the current island, go to the proposal island.\nIf it has fewer people, then flip a coin with probability equal to the proposal island’s population divided by the current island’s. If the coin comes up heads, go to the proposal island.\n\nDo it again a bunch of times.\n\nSo, how does this algorithm perform? Let’s try it out!\n\nrun_rwmh(n_iters = 10) %&gt;%\n  tibble(island = .) %&gt;%\n  group_by(island) %&gt;%\n  summarize(days_spent = n(), .groups = 'drop') %&gt;%\n  mutate(day_proportion = days_spent / sum(days_spent))\n\n# A tibble: 4 × 3\n  island days_spent day_proportion\n   &lt;dbl&gt;      &lt;int&gt;          &lt;dbl&gt;\n1      1          1            0.1\n2      2          1            0.1\n3      3          3            0.3\n4      4          5            0.5\n\n\nUnsurprisingly, with only 10 iterations the algorithm does not perform particularly well. But what about if we give it a lot more time? Let’s try 10,000 iterations.\n\nsome_islands &lt;- run_rwmh(n_iters = 10000) %&gt;%\n  tibble(island = .) %&gt;%\n  group_by(island) %&gt;%\n  summarize(days_spent = n(), .groups = 'drop') %&gt;%\n  mutate(day_proportion = days_spent / sum(days_spent),\n         error_margin = day_proportion / (island / sum(island)) - 1)\n\nmean_error_margin &lt;- mean(some_islands$error_margin)\nsd_error_margin &lt;- sd(some_islands$error_margin)\nsome_islands\n\n# A tibble: 4 × 4\n  island days_spent day_proportion error_margin\n   &lt;dbl&gt;      &lt;int&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n1      1       1000          0.1        0      \n2      2       1958          0.196     -0.0210 \n3      3       3063          0.306      0.0210 \n4      4       3979          0.398     -0.00525\n\n\nMuch better! After 10,000 iterations, we’re spending almost the exact proportion of time on each island that we want to be, as evidenced by the tiny error margins. In addition, the standard deviation of the error margins is 0.01735, which is tiny. That’s awesome! But what about if the system is more complex? Like, what if we had 100 islands?\n\nmore_islands &lt;- run_rwmh(n_iters = 10000, island_populations = 1:100) %&gt;%\n  tibble(island = .) %&gt;%\n  group_by(island) %&gt;%\n  summarize(days_spent = n(), .groups = 'drop') %&gt;%\n  mutate(day_proportion = days_spent / sum(days_spent),\n         error_margin = day_proportion / (island / sum(island)) - 1)\n\nmean_error_margin &lt;- mean(more_islands$error_margin)\nsd_error_margin &lt;- sd(more_islands$error_margin)\nmore_islands\n\n# A tibble: 100 × 4\n   island days_spent day_proportion error_margin\n    &lt;dbl&gt;      &lt;int&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n 1      1          1         0.0001      -0.495 \n 2      2          8         0.0008       1.02  \n 3      3          9         0.0009       0.515 \n 4      4         19         0.0019       1.40  \n 5      5          8         0.0008      -0.192 \n 6      6         13         0.0013       0.0942\n 7      7         14         0.0014       0.0100\n 8      8         11         0.0011      -0.306 \n 9      9         19         0.0019       0.0661\n10     10         28         0.0028       0.414 \n# … with 90 more rows\n\n\nNo problem! Even with the extra islands, the mean error margin is still zero, and the standard deviation of the error margins is 0.23184, which is also small, but not as small as the simpler system. It’s true that a more complex system (i.e. more islands) would mean that we need more iterations to converge in probability to the proportions we’re shooting for, but the algorithm will still work with enough time. Let’s try running it one more time on the complex system, but this time with a million iterations.\n\nmore_iters &lt;- run_rwmh(n_iters = 1000000, island_populations = 1:100) %&gt;%\n  tibble(island = .) %&gt;%\n  group_by(island) %&gt;%\n  summarize(days_spent = n(), .groups = 'drop') %&gt;%\n  mutate(day_proportion = days_spent / sum(days_spent),\n         error_margin = day_proportion / (island / sum(island)) - 1)\n\nmean_error_margin &lt;- mean(more_iters$error_margin)\nsd_error_margin &lt;- sd(more_iters$error_margin)\nmore_iters\n\n# A tibble: 100 × 4\n   island days_spent day_proportion error_margin\n    &lt;dbl&gt;      &lt;int&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n 1      1        199       0.000199      0.00495\n 2      2        432       0.000432      0.0908 \n 3      3        614       0.000614      0.0336 \n 4      4        726       0.000726     -0.0834 \n 5      5        985       0.000985     -0.00515\n 6      6       1157       0.00116      -0.0262 \n 7      7       1409       0.00141       0.0165 \n 8      8       1601       0.00160       0.0106 \n 9      9       1695       0.00170      -0.0489 \n10     10       1952       0.00195      -0.0142 \n# … with 90 more rows\n\n\nLooks like that did the trick! The standard deviation of the error margins fell to 0.02015, just as we expected.\nThis algorithm is called the Metropolis-Hastings Algorithm, and it’s one of many in the class of Markov Chain Monte Carlo algorithms. Some others are the Gibbs Sampler and Hamiltonian Monte Carlo, both of which are frequently used in Bayesian statistics for estimating the parameters of regression models with hundreds of thousands of parameters. In short, these algorithms allow us to solve problems that were literally impossible to solve only two decades ago or so, which is an amazing feat!\n\n\nRecap\n\nMarkov Chains are not that scary! They’re just a memoryless sequence of events, meaning that where you came from doesn’t impact where you go next.\nMarkov Chain Monte Carlo algorithms like the Metropolis-Hastings can be quite simple, and let us solve impossibly hard problems."
  },
  {
    "objectID": "posts/2021-02-11-highlights-from-rstudio-global/index.html",
    "href": "posts/2021-02-11-highlights-from-rstudio-global/index.html",
    "title": "Highlights From rstudio::global",
    "section": "",
    "text": "rstudio::global, this year’s iteration of the annual RStudio conference, was a few weeks ago. Here were some highlights:"
  },
  {
    "objectID": "posts/2021-02-11-highlights-from-rstudio-global/index.html#talks",
    "href": "posts/2021-02-11-highlights-from-rstudio-global/index.html#talks",
    "title": "Highlights From rstudio::global",
    "section": "Talks",
    "text": "Talks\nThere were a few talks I really loved:\n\nUsing R to Up Your Experimentation Game, by Shirbi Ish-Shalom. On experimentation, sequential testing, taking big swings, and being statistically rigorous\nMaintaining the House the Tidyverse Built, by Hadley Wickham. On building and maintaining the Tidyverse, and what package maintenance in the real world is like when you have millions of downloads.\noRganization: How to Make Internal R Packages Part of Your Team, by Emily Riederer. On how using internal packages (like collegeviner at CollegeVine!) can improve your R workflow and make teamwork in R dramatically easier, smoother, and more efficient.\nFairness and Data Science: Failures, Factors, and Futures, by Grant Fleming. On model fairness, bias, and evaluation techniques, and why they’re important to get right."
  },
  {
    "objectID": "posts/2021-02-11-highlights-from-rstudio-global/index.html#cool-new-things",
    "href": "posts/2021-02-11-highlights-from-rstudio-global/index.html#cool-new-things",
    "title": "Highlights From rstudio::global",
    "section": "Cool New Things",
    "text": "Cool New Things\n\nfinetune, Max Kuhn’s new tune-adjacent package, is live (albeit a little buggy)! It has some cool new model tuning algorithms, including racing methods with tune_race_anova() and tune_race_win_loss(), in addition to my personal favorite: tune_sim_anneal() for Simulated Annealing! Link to the talk\nMajor improvements to shiny, including some serious caching upgrades that’ll improve performance dramatically! Link to the talk"
  },
  {
    "objectID": "posts/2021-02-11-highlights-from-rstudio-global/index.html#other-highlights",
    "href": "posts/2021-02-11-highlights-from-rstudio-global/index.html#other-highlights",
    "title": "Highlights From rstudio::global",
    "section": "Other Highlights",
    "text": "Other Highlights\n\nMeeting a bunch of people in the breakout sessions! This year, there were virtual “tables” where you could drag your avatar to “sit down”, and once you were close enough to a table you could hear all of its conversation."
  },
  {
    "objectID": "series.html",
    "href": "series.html",
    "title": "Series",
    "section": "",
    "text": "Heavily inspired by The Missing Semester of Your CS Education, this is a series on all of the aspects of doing data science that you probably didn’t learn in class or from doing research. I’ve learned a lot in the past few years about what it takes to do data science professionally and, frankly, my data science education had some gaping holes. The goal of this series is to help others fill some of those holes (to the best of my ability) by sharing some of the things I’ve learned along the way from great mentors and terrible mistakes.\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\nApr 1, 2023\n\n\nWriting Internal Libraries for Analytics Work\n\n\n14 min\n\n\n\n\nApr 5, 2023\n\n\nUnit Testing Analytics Code\n\n\n14 min\n\n\n\n\nApr 24, 2023\n\n\nPull Requests, Code Review, and The Art of Requesting Changes\n\n\n12 min\n\n\n\n\nMay 6, 2023\n\n\nWorkflow Orchestration\n\n\n8 min\n\n\n\n\nMay 14, 2023\n\n\nExperiment Tracking and Model Versioning\n\n\n9 min\n\n\n\n\nMay 28, 2023\n\n\nDependency Management\n\n\n10 min\n\n\n\n\nJun 6, 2023\n\n\nA Gentle Introduction to Docker\n\n\n12 min\n\n\n\n\nJun 21, 2023\n\n\nHow Can Someone Else Use My Model?\n\n\n10 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "series.html#the-missing-semester-of-your-ds-education",
    "href": "series.html#the-missing-semester-of-your-ds-education",
    "title": "Series",
    "section": "",
    "text": "Heavily inspired by The Missing Semester of Your CS Education, this is a series on all of the aspects of doing data science that you probably didn’t learn in class or from doing research. I’ve learned a lot in the past few years about what it takes to do data science professionally and, frankly, my data science education had some gaping holes. The goal of this series is to help others fill some of those holes (to the best of my ability) by sharing some of the things I’ve learned along the way from great mentors and terrible mistakes.\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\nApr 1, 2023\n\n\nWriting Internal Libraries for Analytics Work\n\n\n14 min\n\n\n\n\nApr 5, 2023\n\n\nUnit Testing Analytics Code\n\n\n14 min\n\n\n\n\nApr 24, 2023\n\n\nPull Requests, Code Review, and The Art of Requesting Changes\n\n\n12 min\n\n\n\n\nMay 6, 2023\n\n\nWorkflow Orchestration\n\n\n8 min\n\n\n\n\nMay 14, 2023\n\n\nExperiment Tracking and Model Versioning\n\n\n9 min\n\n\n\n\nMay 28, 2023\n\n\nDependency Management\n\n\n10 min\n\n\n\n\nJun 6, 2023\n\n\nA Gentle Introduction to Docker\n\n\n12 min\n\n\n\n\nJun 21, 2023\n\n\nHow Can Someone Else Use My Model?\n\n\n10 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "series.html#ab-testing",
    "href": "series.html#ab-testing",
    "title": "Series",
    "section": "A/B Testing",
    "text": "A/B Testing\nA series on common pitfalls in A/B testing and how sequential testing solves many common A/B testing problems.\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\nMar 25, 2022\n\n\nA/B Testing: A Primer\n\n\n4 min\n\n\n\n\nApr 9, 2022\n\n\nRunning A/B Tests\n\n\n5 min\n\n\n\n\nApr 10, 2022\n\n\nCalling A/B Tests\n\n\n7 min\n\n\n\n\nApr 17, 2022\n\n\nSequential Testing\n\n\n5 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "series.html#all-posts",
    "href": "series.html#all-posts",
    "title": "Series",
    "section": "All Posts",
    "text": "All Posts\nTo see all of my blog posts, go to the posts page"
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Matt Kaye",
    "section": "",
    "text": "I like writing code for fun, to explore interesting questions and problems, and to improve the tools I use.\nBroadly speaking, I’d break down this work into three buckets: R packages I author or maintain, contributions to open-source libraries, and other side projects."
  },
  {
    "objectID": "code.html#r-packages",
    "href": "code.html#r-packages",
    "title": "Matt Kaye",
    "section": "R Packages",
    "text": "R Packages\n\nslackr is an R wrapper of the Slack API\nfitbitr is an R wrapper of the Fitbit API\nlightMLFlow is an opinionated R wrapper of the MLFlow REST API"
  },
  {
    "objectID": "code.html#open-source-contributions",
    "href": "code.html#open-source-contributions",
    "title": "Matt Kaye",
    "section": "Open-Source Contributions",
    "text": "Open-Source Contributions\n\nAdding JSON schema validation to MLFlow\nAdding a MAP@K implementation to recmetrics\nAdding a minimal CI/CD process to recmetrics"
  },
  {
    "objectID": "code.html#miscellaneous-side-projects",
    "href": "code.html#miscellaneous-side-projects",
    "title": "Matt Kaye",
    "section": "Miscellaneous Side Projects",
    "text": "Miscellaneous Side Projects\n\nA small Flask app for getting notified when companies you care about post jobs you’re interested in lives at jobcrawler.matthewrkaye.com. Feel free to use it!\nA writeup of my team’s submission to the 2021 NFL Big Data Bowl lives on our Github Pages site.\nAn over-the-top (yet imperfect) R solution to one of our data science technical interview questions: (roughly) implement K-Means lives in my ugly-kmeans repo.\nThe code behind a cursory exploration of the tail behavior of ESPN’s college football win probably model lives in my [espn-cfb-win-prob repo] (https://github.com/mrkaye97/espn-cfb-win-prob)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi! I’m Matt",
    "section": "",
    "text": "Hi! I’m Matt\n\nI’m a data scientist at CollegeVine\n\n\nI care about doing better data science by leaning into domain knowledge and learning from software engineers"
  },
  {
    "objectID": "blogroll.html",
    "href": "blogroll.html",
    "title": "Blogroll",
    "section": "",
    "text": "R-Bloggers"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Matt Kaye",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nHow Can Someone Else Use My Model?\n\n\nAPIs, REST, and Production Services\n\n\n\n\ndata science\n\n\nR\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nJun 21, 2023\n\n\n10 min\n\n\n\n\n\n\n\n\nA Gentle Introduction to Docker\n\n\nI need to run my code somewhere other than my machine. How do I do it?\n\n\n\n\nR\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nJun 6, 2023\n\n\n12 min\n\n\n\n\n\n\n\n\nDependency Management\n\n\nYou should be using poetry, or renv, or conda, or something similar\n\n\n\n\ndata science\n\n\nR\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nMay 28, 2023\n\n\n10 min\n\n\n\n\n\n\n\n\nExperiment Tracking and Model Versioning\n\n\nKnowing what you’ve tried, and what’s in production\n\n\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nMay 14, 2023\n\n\n9 min\n\n\n\n\n\n\n\n\nWorkflow Orchestration\n\n\nA Beginner’s Guide to Shutting Down Your Machine at Night\n\n\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nMay 6, 2023\n\n\n8 min\n\n\n\n\n\n\n\n\nPull Requests, Code Review, and The Art of Requesting Changes\n\n\n\n\n\n\n\nR\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nApr 24, 2023\n\n\n12 min\n\n\n\n\n\n\n\n\nUnit Testing Analytics Code\n\n\nHow do you know your code actually works?\n\n\n\n\nR\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2023\n\n\n14 min\n\n\n\n\n\n\n\n\nWriting Internal Libraries for Analytics Work\n\n\nOr packages, or modules, or whatever you wish to call them\n\n\n\n\nR\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nApr 1, 2023\n\n\n14 min\n\n\n\n\n\n\n\n\nBalancing Classes in Classification Problems\n\n\nAnd why it’s generally a bad idea\n\n\n\n\nR\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nApr 1, 2023\n\n\n10 min\n\n\n\n\n\n\n\n\nCalibration and Evaluating Classification Models\n\n\nMetrics for probability predictions\n\n\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nMar 20, 2023\n\n\n8 min\n\n\n\n\n\n\n\n\nInterpreting AUC-ROC\n\n\n\n\n\n\n\ndata science\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMar 9, 2023\n\n\n10 min\n\n\n\n\n\n\n\n\nExploring the Tail Behavior of ESPN’s Win Probability Model\n\n\n\n\n\n\n\ndata science\n\n\nstatistics\n\n\nsports analytics\n\n\n\n\n\n\n\n\n\n\n\nJan 9, 2023\n\n\n9 min\n\n\n\n\n\n\n\n\nSequential Testing\n\n\n\n\n\n\n\ndata science\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nApr 17, 2022\n\n\n5 min\n\n\n\n\n\n\n\n\nCalling A/B Tests\n\n\n\n\n\n\n\ndata science\n\n\nstatistics\n\n\na/b testing\n\n\n\n\n\n\n\n\n\n\n\nApr 10, 2022\n\n\n7 min\n\n\n\n\n\n\n\n\nRunning A/B Tests\n\n\n\n\n\n\n\ndata science\n\n\nstatistics\n\n\na/b testing\n\n\n\n\n\n\n\n\n\n\n\nApr 9, 2022\n\n\n5 min\n\n\n\n\n\n\n\n\nA/B Testing: A Primer\n\n\n\n\n\n\n\ndata science\n\n\nstatistics\n\n\na/b testing\n\n\n\n\n\n\n\n\n\n\n\nMar 25, 2022\n\n\n4 min\n\n\n\n\n\n\n\n\nDeploying MLFlow on Heroku (with Heroku Postgres, S3, and nginx for basic auth)\n\n\n\n\n\n\n\nmlops\n\n\n\n\n\n\n\n\n\n\n\nDec 26, 2021\n\n\n7 min\n\n\n\n\n\n\n\n\nNotes on Hiring Data Analysts + Scientists\n\n\n\n\n\n\n\ndata science\n\n\nhiring\n\n\n\n\n\n\n\n\n\n\n\nDec 22, 2021\n\n\n14 min\n\n\n\n\n\n\n\n\nWorking With Your Fitbit Data in R\n\n\n\n\n\n\n\nR\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nJun 8, 2021\n\n\n6 min\n\n\n\n\n\n\n\n\nHighlights From rstudio::global\n\n\n\n\n\n\n\nR\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2021\n\n\n2 min\n\n\n\n\n\n\n\n\nWhat’s New in slackr 2.1.0\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nFeb 7, 2021\n\n\n2 min\n\n\n\n\n\n\n\n\nA Gentle Introduction to Markov Chains and MCMC\n\n\n\n\n\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2021\n\n\n10 min\n\n\n\n\n\n\n\n\nWhat I’ve Been Drinking in Quarantine (Plus a Cocktail Lesson)\n\n\n\n\n\n\n\nfood + bev\n\n\n\n\n\n\n\n\n\n\n\nJan 10, 2021\n\n\n15 min\n\n\n\n\n\n\n\n\nOur 2021 Big Data Bowl Submission\n\n\n\n\n\n\n\ndata science\n\n\nsports analytics\n\n\n\n\n\n\n\n\n\n\n\nJan 7, 2021\n\n\n15 min\n\n\n\n\n\n\nNo matching items"
  }
]