[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Matt Kaye",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nOn Doing Data Science\n\n\nWhat does a data science project actually take?\n\n\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nAug 3, 2023\n\n\n10 min\n\n\n\n\n\n\n\n\nUnit Testing dbt Models\n\n\nLetting yourself sleep at night by ensuring your SQL is correct\n\n\n\n\ndbt\n\n\nanalytics engineering\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nJul 9, 2023\n\n\n26 min\n\n\n\n\n\n\n\n\nLessons Learned From Running R in Production\n\n\nAnd why I probably won’t be doing it again\n\n\n\n\nR\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nJun 29, 2023\n\n\n31 min\n\n\n\n\n\n\n\n\nHow Can Someone Else Use My Model?\n\n\nAPIs, REST, and Production Services\n\n\n\n\ndata science\n\n\nR\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nJun 21, 2023\n\n\n10 min\n\n\n\n\n\n\n\n\nA Gentle Introduction to Docker\n\n\nI need to run my code somewhere other than my machine. How do I do it?\n\n\n\n\nR\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nJun 6, 2023\n\n\n12 min\n\n\n\n\n\n\n\n\nDependency Management\n\n\nYou should be using poetry, or renv, or conda, or something similar\n\n\n\n\ndata science\n\n\nR\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nMay 28, 2023\n\n\n10 min\n\n\n\n\n\n\n\n\nExperiment Tracking and Model Versioning\n\n\nKnowing what you’ve tried, and what’s in production\n\n\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nMay 14, 2023\n\n\n9 min\n\n\n\n\n\n\n\n\nWorkflow Orchestration\n\n\nA Beginner’s Guide to Shutting Down Your Machine at Night\n\n\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nMay 6, 2023\n\n\n8 min\n\n\n\n\n\n\n\n\nPull Requests, Code Review, and The Art of Requesting Changes\n\n\n\n\n\n\n\nR\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nApr 24, 2023\n\n\n12 min\n\n\n\n\n\n\n\n\nUnit Testing Analytics Code\n\n\nHow do you know your code actually works?\n\n\n\n\nR\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2023\n\n\n14 min\n\n\n\n\n\n\n\n\nWriting Internal Libraries for Analytics Work\n\n\nOr packages, or modules, or whatever you wish to call them\n\n\n\n\nR\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nApr 1, 2023\n\n\n14 min\n\n\n\n\n\n\n\n\nBalancing Classes in Classification Problems\n\n\nAnd why it’s generally a bad idea\n\n\n\n\nR\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nApr 1, 2023\n\n\n10 min\n\n\n\n\n\n\n\n\nCalibration and Evaluating Classification Models\n\n\nMetrics for probability predictions\n\n\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nMar 20, 2023\n\n\n8 min\n\n\n\n\n\n\n\n\nInterpreting AUC-ROC\n\n\n\n\n\n\n\ndata science\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMar 9, 2023\n\n\n10 min\n\n\n\n\n\n\n\n\nExploring the Tail Behavior of ESPN’s Win Probability Model\n\n\n\n\n\n\n\ndata science\n\n\nstatistics\n\n\nsports analytics\n\n\n\n\n\n\n\n\n\n\n\nJan 9, 2023\n\n\n9 min\n\n\n\n\n\n\n\n\nSequential Testing\n\n\n\n\n\n\n\ndata science\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nApr 17, 2022\n\n\n5 min\n\n\n\n\n\n\n\n\nCalling A/B Tests\n\n\n\n\n\n\n\ndata science\n\n\nstatistics\n\n\na/b testing\n\n\n\n\n\n\n\n\n\n\n\nApr 10, 2022\n\n\n7 min\n\n\n\n\n\n\n\n\nRunning A/B Tests\n\n\n\n\n\n\n\ndata science\n\n\nstatistics\n\n\na/b testing\n\n\n\n\n\n\n\n\n\n\n\nApr 9, 2022\n\n\n5 min\n\n\n\n\n\n\n\n\nA/B Testing: A Primer\n\n\n\n\n\n\n\ndata science\n\n\nstatistics\n\n\na/b testing\n\n\n\n\n\n\n\n\n\n\n\nMar 25, 2022\n\n\n4 min\n\n\n\n\n\n\n\n\nDeploying MLFlow on Heroku (with Heroku Postgres, S3, and nginx for basic auth)\n\n\n\n\n\n\n\nmlops\n\n\n\n\n\n\n\n\n\n\n\nDec 26, 2021\n\n\n7 min\n\n\n\n\n\n\n\n\nNotes on Hiring Data Analysts + Scientists\n\n\n\n\n\n\n\ndata science\n\n\nhiring\n\n\n\n\n\n\n\n\n\n\n\nDec 22, 2021\n\n\n14 min\n\n\n\n\n\n\n\n\nWorking With Your Fitbit Data in R\n\n\n\n\n\n\n\nR\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nJun 8, 2021\n\n\n6 min\n\n\n\n\n\n\n\n\nHighlights From rstudio::global\n\n\n\n\n\n\n\nR\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2021\n\n\n2 min\n\n\n\n\n\n\n\n\nWhat’s New in slackr 2.1.0\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nFeb 7, 2021\n\n\n2 min\n\n\n\n\n\n\n\n\nA Gentle Introduction to Markov Chains and MCMC\n\n\n\n\n\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2021\n\n\n10 min\n\n\n\n\n\n\n\n\nWhat I’ve Been Drinking in Quarantine (Plus a Cocktail Lesson)\n\n\n\n\n\n\n\nfood + bev\n\n\n\n\n\n\n\n\n\n\n\nJan 10, 2021\n\n\n15 min\n\n\n\n\n\n\n\n\nOur 2021 Big Data Bowl Submission\n\n\n\n\n\n\n\ndata science\n\n\nsports analytics\n\n\n\n\n\n\n\n\n\n\n\nJan 7, 2021\n\n\n15 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi! I’m Matt",
    "section": "",
    "text": "Hi! I’m Matt\n\nI’m a data scientist at Klaviyo\n\n\nI care about doing better data science by leaning into domain knowledge and learning from software engineers"
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Matt Kaye",
    "section": "",
    "text": "I like writing code for fun, to explore interesting questions and problems, and to improve the tools I use.\nBroadly speaking, I’d break down this work into three buckets: R packages I author or maintain, contributions to open-source libraries, and other side projects."
  },
  {
    "objectID": "code.html#r-packages",
    "href": "code.html#r-packages",
    "title": "Matt Kaye",
    "section": "R Packages",
    "text": "R Packages\n\nslackr is an R wrapper of the Slack API\nfitbitr is an R wrapper of the Fitbit API\nlightMLFlow is an opinionated R wrapper of the MLFlow REST API"
  },
  {
    "objectID": "code.html#open-source-contributions",
    "href": "code.html#open-source-contributions",
    "title": "Matt Kaye",
    "section": "Open-Source Contributions",
    "text": "Open-Source Contributions\n\nAdding JSON schema validation to MLFlow\nAdding a MAP@K implementation to recmetrics\nAdding a minimal CI/CD process to recmetrics"
  },
  {
    "objectID": "code.html#miscellaneous-side-projects",
    "href": "code.html#miscellaneous-side-projects",
    "title": "Matt Kaye",
    "section": "Miscellaneous Side Projects",
    "text": "Miscellaneous Side Projects\n\nA small Flask app for getting notified when companies you care about post jobs you’re interested in lives at jobcrawler.matthewrkaye.com. Feel free to use it!\nA writeup of my team’s submission to the 2021 NFL Big Data Bowl lives on our Github Pages site.\nAn over-the-top (yet imperfect) R solution to one of our data science technical interview questions: (roughly) implement K-Means lives in my ugly-kmeans repo.\nThe code behind a cursory exploration of the tail behavior of ESPN’s college football win probably model lives in my [espn-cfb-win-prob repo] (https://github.com/mrkaye97/espn-cfb-win-prob)."
  },
  {
    "objectID": "posts/2023-06-29-lessons-learned-from-running-r-in-production/lessons-learned-from-running-r-in-production.html",
    "href": "posts/2023-06-29-lessons-learned-from-running-r-in-production/lessons-learned-from-running-r-in-production.html",
    "title": "Lessons Learned From Running R in Production",
    "section": "",
    "text": "A couple weeks ago, I wrote a high-level post on REST APIs. One thing that I noted was that I couldn’t, in good faith, recommend running R (or Plumber, a common library used to create APIs in R) in any type of high-load production system.\nAdmittedly, this was a pretty inflammatory thing to say. I know there’s a whole community of R developers working on R in production, as well as lots of believers in R for production services. I know this, in part, because I’ve been one of them in the past. But more on that in a bit.\nIn that aside in my last post, I commented that the reasons why I won’t be running R in production anymore were out of scope. This post is intended to explain those reasons in much more technical detail, to the extent that I’m capable."
  },
  {
    "objectID": "posts/2023-06-29-lessons-learned-from-running-r-in-production/lessons-learned-from-running-r-in-production.html#introduction",
    "href": "posts/2023-06-29-lessons-learned-from-running-r-in-production/lessons-learned-from-running-r-in-production.html#introduction",
    "title": "Lessons Learned From Running R in Production",
    "section": "",
    "text": "A couple weeks ago, I wrote a high-level post on REST APIs. One thing that I noted was that I couldn’t, in good faith, recommend running R (or Plumber, a common library used to create APIs in R) in any type of high-load production system.\nAdmittedly, this was a pretty inflammatory thing to say. I know there’s a whole community of R developers working on R in production, as well as lots of believers in R for production services. I know this, in part, because I’ve been one of them in the past. But more on that in a bit.\nIn that aside in my last post, I commented that the reasons why I won’t be running R in production anymore were out of scope. This post is intended to explain those reasons in much more technical detail, to the extent that I’m capable."
  },
  {
    "objectID": "posts/2023-06-29-lessons-learned-from-running-r-in-production/lessons-learned-from-running-r-in-production.html#r-evangelism",
    "href": "posts/2023-06-29-lessons-learned-from-running-r-in-production/lessons-learned-from-running-r-in-production.html#r-evangelism",
    "title": "Lessons Learned From Running R in Production",
    "section": "R Evangelism",
    "text": "R Evangelism\nFirst thing’s first: I love R. I’ve been a bit of an R evangelist for the past five years or so, and I think that R provides fantastic tooling that helps me do my day to day work dramatically (think maybe 3-4x) faster than equivalent tooling in Python, SQL, etc. I think I could argue strongly that the Tidyverse suite of tools has had a larger impact on how I write analytical code and how I think about data wrangling problems – in addition to just how I program in general – than any other single technical thing I’ve ever come across. In particular, purrr introduced me to functional programming and using functional patterns in the analytical code I write, and I haven’t looked back since.\nI say this because I don’t want the rest of this post to seem as if it’s coming from someone parroting the same Python lines about how “it’s a general purpose programming language” and how “R is made for statisticians so it’s not meant for production” or any of the other usual arguments against R. My view is that most of these arguments are just people being dogmatic, and that most of those common criticisms of R are being leveled by people who have never actually worked in R.\nI’ve argued with my fair share of people on the internet about R in production, and am aware of the usual pro-R arguments. I know about the Put R In Prod talks, and have used Plumber and RestRserve. I’m familiar with vetiver and the suite of MLOps tools that the Tidymodels team has been working on building out. In the past, I’ve referenced things like Put R in Prod as evidence that you can, in fact, run R in production. But I always felt a bit queasy about it: How was it, I’d ask myself, that I could really only find one reference of a company genuinely running R in production, when virtually every place that does machine learning that I’m aware of has experience with Python, Rust, Scala, or similar? This post is a long-form answer to that question."
  },
  {
    "objectID": "posts/2023-06-29-lessons-learned-from-running-r-in-production/lessons-learned-from-running-r-in-production.html#production-services",
    "href": "posts/2023-06-29-lessons-learned-from-running-r-in-production/lessons-learned-from-running-r-in-production.html#production-services",
    "title": "Lessons Learned From Running R in Production",
    "section": "Production Services",
    "text": "Production Services\nBefore I get into the guts of this post, I want to re-emphasize part of the tagline. When I say “production” in this post, I mean high-load production systems. I’m not talking about Shiny apps. I’m not talking about APIs getting one request every few seconds. I’m not talking about “offline” services where response times don’t particularly matter. I’ve had lots of success using R in all of those settings, and I think R is a great tool for solving problems in those spaces.\nThis post is about high-load, online systems. You might think of this, roughly, as a system that’s getting, say, more than one request per second on average, at least five requests per second at peak times, and there’s a requirement that the service responds in something like 500 milliseconds (p95) with a p50 of maybe 100. For the rest of this post, that is the kind of system I’m describing when I say “production.”"
  },
  {
    "objectID": "posts/2023-06-29-lessons-learned-from-running-r-in-production/lessons-learned-from-running-r-in-production.html#problems",
    "href": "posts/2023-06-29-lessons-learned-from-running-r-in-production/lessons-learned-from-running-r-in-production.html#problems",
    "title": "Lessons Learned From Running R in Production",
    "section": "Problems",
    "text": "Problems\nWe’ve run into a number of problems with R in production. In broad strokes, the issues we’ve had have come from both Plumber, the API library we were using, and R itself. The next few sections cover some of the issues that caused the most headaches for us, and ultimately led us to switch over to FastAPI.\n\nGunicorn, Web Servers, and Concurrency\nFirst and foremost: R is single-threaded. This is one of the most common criticisms I hear about R running in production settings, especially in the Python vs. R for production discussions I’ve been in. Of course, those discussions tend to ignore that Python also runs single-threaded, but I digress.\n\n\n\n\n\n\nThis post will be a bit more technical than some of my others. Since it’s already going to be long, I won’t be doing as much explaining the meanings of terms like “single-threaded” or similar.\n\n\n\nR running single-threaded and not managing concurrency particularly well isn’t a problem in and of itself. Other languages (Python, Ruby, etc.) that are very often used in production systems of all sizes have the same issue. The problem with R in particular is that unlike Python, which has Gunicorn, Uvicorn, and other web server implementations, and Ruby, which has Puma and similar, R has no widely-used web server to help it run concurrently. In practice, this means that if you, for instance, were to run a FastAPI service in production, you’d generally have a “leader” that delegates “work” (processing requests) to workers. Gunicorn or Uvicorn would handle this for you. This would mean that your service can handle as many concurrent requests as you have workers without being blocked.\nAs I mentioned, R has no equivalent web server implementation, which, in combination with running single-threaded, means that a Plumber service really can only handle one request at a time before getting blocked. In my view, this makes running high-load production services in R a non-starter, as concurrency and throughput are the ultimate source of lots of scalability problems in APIs. Yes, Plumber does indeed integrate with future and promises to allow for some async behavior, but my view is that it’s hard to make an argument that async Plumber is a viable substitute for a genuinely concurrent web server.\nBut let’s put aside the “non-starter” bit for a second, and let’s imagine that you, like me, want to try everything in your power to get R working in production. The following sections will cover other issues we’ve run into, and a number of workarounds we attempted, to varying degrees of success.\n\n\nTypes and Type Conversion\nIn my opinion, one of the biggest issues with R is the type system. R is dynamically typed, and primitive types are generally represented as length-one vectors. That’s why these two variables are of the same type:\n\nclass(1)\n\n[1] \"numeric\"\n\nclass(c(1, 2))\n\n[1] \"numeric\"\n\n\nThis is a big problem. What happens when we try to serialize the number 1 to JSON?\n\njsonlite::toJSON(1)\n\n[1] \n\n\nIt returns [1] – as in: A length-one list, where the one element is the number one. Of course, you can set auto_unbox = TRUE, but that has other issues:\n\njsonlite::toJSON(1, auto_unbox = TRUE)\n\n1 \n\n\nThis is fine, but the problem with auto_unbox = TRUE is that if you have a return type that is genuinely a list, it could sometimes return a list, and sometimes return a single number, depending on the length of the thing being returned:\n\nget_my_fake_endpoint &lt;- function(x) {\n  jsonlite::toJSON(x + 1, auto_unbox = TRUE)\n}\n\nget_my_fake_endpoint(1)\n\n2 \n\nget_my_fake_endpoint(c(1, 2))\n\n[2,3] \n\n\nIn these two examples, I’ve gotten two different response types depending on the length of the input: One was a list, the other was an integer. This means that, without explicit handling of this edge case, your client has no guarantee of the type of the response it’s going to get from the server, which will inevitably be a source of errors on the client side.\nIn every other programming language that I’m aware of being used in production environments, this is not the case. For instance:\n\nimport json\nimport sys\n\nx = 1\ny = [1, 2]\n\nprint(type(x))\nprint(type(y))\n\njson.dump(x, sys.stdout)\njson.dump(y, sys.stdout)\n\n&lt;class 'int'&gt;\n&lt;class 'list'&gt;\n1[1, 2]\n\n\nIn Python, the number 1 is an integer type. The list [1, 2] is a list type. And the JSON library reflects that. No need for unboxing.\nBut there’s more! R (and Plumber) also do not enforce types of parameters to your API, as opposed to FastAPI, for instance, which does via the use of pydantic. That means that if you have a Plumber route that takes an integer parameter n and someone calls your route with ?n=foobar, you won’t know about that until the rest of your code runs, at which point you might get an error about n being non-numeric.\nHere’s an example:\nlibrary(plumber)\n\npr() %&gt;%\n  pr_get(\n    \"/types\",\n    function(n) {\n      n * 2\n    }\n  ) %&gt;%\n  pr_run()\nObviously, n is indented to be a number. You can even define it as such in an annotation like this:\n#* @param n:int \nBut R won’t enforce that type declaration at runtime, which means you need to explicitly handle all of the possible cases where someone provides a value for n that is not of type int. For instance, if you call that service and provide n=foobar, you’d see the following in your logs (and the client would get back an unhelpful HTTP 500 error):\n&lt;simpleError in n * 2: non-numeric argument to binary operator&gt;\nIf you do the equivalent in FastAPI, you’d have vastly different results:\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/types\")\nasync def types(n: int) -&gt; int:\n  return n * 2\nRunning that API and making the following call returns a very nice error:\ncurl \"http://127.0.0.1:8000/types?n=foobar\" | jq\n\n{\n  \"detail\": [\n    {\n      \"loc\": [\n        \"query\",\n        \"n\"\n      ],\n      \"msg\": \"value is not a valid integer\",\n      \"type\": \"type_error.integer\"\n    }\n  ]\n}\nI didn’t need to do any type checking. All I did was supply a type annotation, just like I could in Plumber, and FastAPI, via pydantic, did all the lifting for me. I provided foobar, which is not a valid integer, and I get a helpful error back saying that the value I provided for n is not a valid integer. FastAPI also returns an HTTP 422 error (the error code is configurable), which tells the client that they did something wrong, as opposed to the 500 that Plumber returns, indicating that something went wrong on the server side.\n\n\nClients and Testing\nAnother issue with Plumber is that it doesn’t integrate nicely with any testing framework, at least that I’m aware of. In FastAPI, and every other web framework that I’m familiar with, there’s a built-in notion of a test client, which lets you “call” your endpoints as if you were an external client. In Plumber, we’ve needed to hack similar behavior together using testthat by spinning up the API in a background process, and then running a test suite against the local instance of the API we spun up, and then spinning down. This has worked fine, but it’s clunky and much harder to maintain than a genuine, out-of-the-box way to do testing that really should ship with the web framework. I’ve heard of callthat, but I’ve never actually tried it for solving this problem.\n\n\nPerformance\nWhen I’ve defended R in that past, I’ve also heard a common complaint about it’s speed. There are very often arguments that R is slow, full-stop. And that’s not true, or at least mostly not true. Especially relative to Python, you can write basically equally performant code in R as you can in numpy or similar. But some things in R are slow. For instance, let’s serialize some JSON:\n\nlibrary(jsonlite)\n\niris &lt;- read.csv(\"fastapi-example/iris.csv\")\n\nresult &lt;- microbenchmark::microbenchmark(\n  tojson = {toJSON(iris)},\n  unit = \"ms\", \n  times = 1000\n)\n\npaste(\"Mean runtime:\", round(summary(result)$mean, 4), \"milliseconds\")\n\n[1] \"Mean runtime: 3.3928 milliseconds\"\n\n\nNow, let’s try the same in Python:\n\nfrom timeit import timeit\nimport pandas as pd\n\niris = pd.read_csv(\"fastapi-example/iris.csv\")\n\nN = 1000\n\nprint(\n  \"Mean runtime:\", \n  round(1000 * timeit('iris.to_json(orient = \"records\")', globals = locals(), number = N) / N, 4), \n  \"milliseconds\"\n)\n\nMean runtime: 0.485 milliseconds\n\n\nIn this particular case, Python’s JSON serialization runs 6-7x faster than R’s. And if you’re thinking “that’s only one millisecond, though!” you’d be right. But the general principle is important even if the magnitude of the issue in this particular case is not.\nJSON serialization is the kind of thing that you’re going to need to do if you’re building an API, and you generally want it to be as fast as possible to limit overhead. It also takes longer and longer as the JSON itself is more complicated. So while in this particular case we’re talking about microseconds of difference, the underlying issue is clear: Plumber uses jsonlite to serialize JSON under the hood, and jsonlite is nowhere near as fast as roughly equivalent Python JSON serialization for identical data and the same result.\nThe takeaway here is that while it may be true that vectorized R code to create a feature for a model or low-level BLAS or LAPACK code that R calls to perform matrix multiplication should be equally performant to the equivalent Python, R can sometimes have overhead, like in JSON serialization, that becomes apparent as the size and complexity of both the body of the request as well as the response body scale up. There are certainly other examples of the same overhead. When we moved a Plumber service to FastAPI with no changes to the logic itself, we got about a 5x speedup in how long it took to process requests. And just to reiterate: That 5x speedup had nothing to do with changes to logic, models, or anything tangible about the code. All of that was exactly the same.\n\n\nIntegration with Tooling\nAnother issue with R for production is that very often in production services, we want to do some things on the following (in no particular order and not exhaustive) list:\n\nServe predictions from one version of a model to some users and another version of a model to other users.\nHave reports and alerts of when errors happen in our API.\nUse a database - such as a feature store - for retrieving features to run through our models.\n\nThere are tools for doing all of these things. The first would be a feature flagging tool like LaunchDarkly. The second would be an error monitoring tool like Sentry. And the last would be a feature store like Feast which might use something like Redis under the hood.\nPython supports all of these tools. All of them have APIs in Python, and are easily integrated into a FastAPI, Flask, or Django service. R has no bindings for any of them, meaning that if you wanted to run R in a system where you use feature flags, for instance, you’d need to roll your own flagging logic or find a more obscure tool that supports R. That’s fine for some teams, but writing feature flagging logic isn’t generally a good use of a data scientist’s time. And especially not when there are a whole variety of great tools that you can grab off the shelf and slot into a Python service seamlessly.\nThis issue expands beyond just production tooling, too. For instance, there are a number of MLOps tools for all parts of the machine learning process, such as Weights & Biases for experiment tracking and model versioning, Evidently for monitoring, Bento for model serving, and so on, that all only have Python bindings. That’s not to say that there are no tools that support R – some, like MLFlow certainly do – but the set of tools that support R is a strict, and small, subset of the ones that support Python. I’m also aware of the great work that the Tidymodels team is doing on Vetiver, Pins, and related packages in the Tidymodels MLOps suite, but the reality is that these tools are far behind the state of the art (but are catching up!)."
  },
  {
    "objectID": "posts/2023-06-29-lessons-learned-from-running-r-in-production/lessons-learned-from-running-r-in-production.html#workarounds",
    "href": "posts/2023-06-29-lessons-learned-from-running-r-in-production/lessons-learned-from-running-r-in-production.html#workarounds",
    "title": "Lessons Learned From Running R in Production",
    "section": "Workarounds",
    "text": "Workarounds\nOur team tried out a bunch of ideas to get around these issues before ultimately abandoning R in favor of FastAPI.\nWe “solved” the types issues R has by having lots of type validation at request time, and making use of JSON schema validation to the extent that we could to limit the number of edge cases we ran into. We use MLFlow for model tracking, and don’t really need some of the more “sophisticated” MLOps tools mentioned before. But the first issue – and the biggest – was the concurrency issue, which we ultimately failed to overcome.\n\nLoad Balancing on PaaS Tools\nThe first “fix” for R’s concurrency issue we tried was the most expensive one: Buying our way out of the problem. We horizontally scaled our R service up from a a single instance to having multiple instances of the service behind a load balancer, which bought us a significant amount of runway. It’s expensive, but this could be a reasonably good solution to R’s concurrency issues for most teams. However, there’s only so far that you can scale horizontally before needing to address the underlying issues. For instance, if you have a model that takes 250ms to run predictions through and return to the client, on average you can process four requests per second per instance of your API. But since R is running single-threaded, you probably can really only run about one or two requests per second before you start to get concerned about requests backing up. If one request takes one second, now you could have four more requests in the queue waiting to be processed, and so on.\nHorizontal scaling fixes this issue to some extent, but the magnitude of the problem scales linearly as the amount of throughput to the service increases. So ultimately, it’s inevitable that you’ll need to either address the underlying problem of the performance of the service, or spend potentially exorbitant amounts of money to buy your way out of the problem.\n\n\nNGINX As A Substitute\nWe also tried to get around R’s lack of an ASGI server like Uvicorn by sitting our Plumber API behind an NGINX load balancer. The technical nuts and bolts were a little involved, so I’ll just summarize the highlights here. We used a very simple NGINX conf template:\n## nginx.conf\n\nevents {}\n\nhttp {\n  upstream api {\n\n      ## IMPORTANT: Keep ports + workers here\n      ## in sync with ports + workers declared\n      ## in scripts/run.sh\n\n      server localhost:8000;\n      server localhost:8001;\n      server localhost:8002;\n      server localhost:8003;\n  }\n\n  server {\n    listen $PORT;\n    server_name localhost;\n    location / {\n      proxy_pass http://api;\n    }\n  }\n}\nThen, we’d boot the API as follows:\n## run.sh\n\n# !/bin/bash\n\n## IMPORTANT: Keep ports + workers here\n## in sync with ports + workers declared\n## in nginx.conf\n\nRscript -e \"plumber::options_plumber(port = 8000); source('app.R')\"&\nRscript -e \"plumber::options_plumber(port = 8001); source('app.R')\"&\nRscript -e \"plumber::options_plumber(port = 8002); source('app.R')\"&\nRscript -e \"plumber::options_plumber(port = 8003); source('app.R')\"&\n\nsed -i -e 's/$PORT/'\"$PORT\"'/g' /etc/nginx/nginx.conf && nginx -g 'daemon off;'\nThe basic premise was that we’d boot four instances of our Plumber API as background processes, and then let NGINX load balance between them.\nThis worked well in theory (and also in practice, to an extent) until we ran into an odd problem: At certain levels of load, the “workers” started to die and not reboot, which resulted in cascading failures. Essentially, one worker would go down, resulting in more load on the other workers, until a second worker went down, causing the service to spiral and eventually crash. You can see this happening in the load test below.\n\n\n\n\n\nAt relatively low levels of traffic (~25 RPS) the service starts to have issues. Those issues snowball, until eventually every request begins failing. Note that 25 requests per second sounds like a lot, but that load is distributed across four workers, meaning each worker is attempting to handle 5-6 requests per second.\nFor some services, burning down under this amount of load is fine. But it was disconcerting for us, especially since the amount of actual work that the endpoint we were hitting in our load test was doing was just extracting a pre-computed item from a list. The vast majority of the overhead in these API calls was JSON serialization, type checking, and network latency, but the R service itself was only taking about 10ms to process the request (read: extract the necessary element) once the endpoint got the request.\nThe problem for us was that if the service is burning down in this case where we’re doing virtually no lifting at all after getting 25 or so requests per second, what happens when the processing time for a request jumps up from 10ms to 250ms for creating features, making predictions, and so on? In that world, I’d expect that even behind NGINX, our service could probably only safely process about 5 requests per second before it starts getting dicey and us needing to start thinking again about horizontally scaling to more instances, and that wasn’t nearly enough headroom to be comfortable with in a production system."
  },
  {
    "objectID": "posts/2023-06-29-lessons-learned-from-running-r-in-production/lessons-learned-from-running-r-in-production.html#wrapping-up",
    "href": "posts/2023-06-29-lessons-learned-from-running-r-in-production/lessons-learned-from-running-r-in-production.html#wrapping-up",
    "title": "Lessons Learned From Running R in Production",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nI want to wrap up by tying everything here back to what I discussed at the start. I’m very much not an R hater: Quite the opposite. I think R is an amazing language that’s made it so much easier and more enjoyable to do my job day-to-day, and I hoped against hope that we could figure out a way to run R in production. I thought most of the normal complaints I’d heard about R “just not being production-grade” weren’t rigorous enough, and that made me want to give it a shot to prove the haters wrong, in some sense.\nIt turned out, unfortunately, that I was the one that was wrong. As we scaled up our R in production rig, it became increasingly apparent that there were some very real problems with R that we really couldn’t get around without either putting lots of work and duct tape into fixing them, or throwing money at them. And Python – FastAPI in particular – was so much simpler, more reliable, and gave me dramatically more confidence than I had in Plumber.\nOnce upon a time, I’d hoped that this post would’ve been something of a love letter to R: A story of triumph, where we figured out how to tweak and tune and architect just right, and got a stable, fast R service running in production to prove all of the doubters and dogmatic Pythoners wrong. But unfortunately, it didn’t turn out that way. So my hope in writing this post was to expose some more of the nuts and bolts for why I won’t be trying to run R in production again, or at least not in the short term. I don’t believe in sweeping statements like “R isn’t built for production” since I don’t actually know what they mean, and they’re not helpful for getting to the root of the problem with R. But as I discovered, “R doesn’t have an equivalent of Gunicorn or Puma” is a very legitimate flaw with R that makes it very difficult – not impossible, but very difficult – to run R in production.\nBut there’s a larger point here, that maybe is an undertone of this whole post, and to some degree all of the conversation about R vs. Python for production. The reality is that lots of software teams are running Python in production, which means that Python and its community are focused on building tooling in Python to service that use case. To my knowledge, there aren’t many teams running R in production, and so the focus and will doesn’t seem to be there in the same way it is for Python. And maybe that’s what matters most at the end of the day: Python is a safer choice since more people are using it, which means it continues to be worked on more, which makes it faster, and safer, and so on. And the cycle continues.\nI hope that R can have that same focus on production, some day."
  },
  {
    "objectID": "posts/2023-06-29-lessons-learned-from-running-r-in-production/lessons-learned-from-running-r-in-production.html#addenda",
    "href": "posts/2023-06-29-lessons-learned-from-running-r-in-production/lessons-learned-from-running-r-in-production.html#addenda",
    "title": "Lessons Learned From Running R in Production",
    "section": "Addenda",
    "text": "Addenda\nThere have been a few points that have come up in discussion as this post has circulated through some communities online, so I’d like to add some thoughts to address some of those. I very much appreciate all of the attention this post has gotten!\n\nPlumber vs. R Itself\nThis post is about R in production, but it’s also about Plumber. I recognize that I conflate Plumber and R throughout this post, and that many of the points here are comparing, for example, Plumber and FastAPI. That wasn’t exactly my intention. Especially when it comes to web servers, there are two points I’d like to emphasize.\nFirst, the reason I seem to be conflating Plumber (a library) and R (a language) is that Plumber is the only viable, widely used API framework in R. So, to some extent, that means that problems with Plumber are problems with R, since there really aren’t viable alternatives, at least to my knowledge.\nSecond, FastAPI is only one alternative, but the point I was trying to make was broader: Every web framework that I’m aware of utilizes some kind of concurrent web server, a la Uvicorn, Gunicorn, or Puma. So while these points about the lack of a battle-hardened, concurrent web server implementation in Plumber apply when comparing it to FastAPI, they also apply when comparing it to Flask or Django, which you’d run behind Gunicorn or similar, Rails, which you’d run behind Puma or similar, Rocket (in Rust), which ships with its own multi-threaded, asynchronous server, and so on. At its core, this really isn’t a comparison of Plumber and FastAPI: It’s a comparison of Plumber and every other web framework that I am aware of.\n\n\nPerformance Gains From Adopting FastAPI\nI made a comment about a 5x improvement in throughput by switching to FastAPI:\n\nwe got about a 5x speedup in how long it took to process requests\n\nI should have added a clarifying point: This 5x improvement was achieved even after accounting for scaling down from four instances of a Plumber service to a single instance of a FastAPI service with four workers. So it wasn’t the case that the throughput improvement was due to concurrency, as the number of “workers” in some sense remained the same.\n\n\nValve\nValve is an admittedly very interesting package that attempts to handle some of the concurrency issues that this post has discussed. And in an effort to be transparent: I’ve done virtually no research on Valve, and have no experience working with it. But immediately, there’s a major issue: If you’re running code in a production system (as defined here), there are some philosophical requirements that I strongly believe need to be met about the level of comfort I have when considering adopting new libraries. Code in production needs to be stable. Code in production needs to be debuggable and maintainable. Code in production, ideally, should be treated as inherently risky and costly.\nAt work, we often like to talk about a concept called an innovation token, and about choosing boring technology. The basic principle is this: Your team only gets a few innovation tokens to spend on things that are lesser-known, lesser-used, more complicated to set up or maintain, and so on, and you must spend them wisely. For instance, if your team is passionate about typed functional programming like ours is, you might write your front end in PureScript, like we have.\nBut writing our front end in PureScript costs us an innovation token. PureScript isn’t commonly used, and so it’s harder, for instance, to StackOverflow your way out of issues you’re running into, because there are fewer people who will have had the same issues as you in the past. You might think of this issue as there being less “common knowledge” about PureScript, or less prior art that you and your team can rely on. So by all means, use PureScript, but just know that this is costing you one of those tokens, and treat the decision as a costly one.\nUsing a tool like Valve, similarly, costs an innovation token, and it’s not one that I’d be willing to spend. If virtually nobody is using Valve in production, it’s not on CRAN, and the documentation is sparse, it’ll be an order of magnitude harder to follow best practices (if they even exist) and fix issues that will inevitably come up in using it. In my view, this is a non-starter. To some degree, a theme of this whole post has been that using R itself in production costs an innovation token, and spending another one on Valve when you could just as easily use Flask, FastAPI, or some similar framework is a choice that I, personally, would not feel comfortable making. That’s not to say that Valve wouldn’t work or do what’s advertised on the tin, or anything like that. It’s just that when something inevitably goes wrong, I feel much more comfortable in my ability to find the solution to the problem when it’s a Flask problem or a FastAPI problem as opposed to if it’s a Valve problem.\nUltimately, as data scientists, we get paid to deliver business value. We don’t get paid to work out weird kinks in an uncommonly used library, or any other version of fitting square pegs into round holes. And so my view is that in cases like this, the boring technology – read: the one that everyone else uses – tends to be the one that ultimately lets us do what we get paid to do. This is the crux of the sadness I felt in moving from R to FastAPI: I was making a conscious choice to give up something that I love in the name of pragmatism, which is never an easy call to make.\n\n\nTypes and stopifnot\nOne proposed fix for the “R doesn’t do a good job of handling types” issues that I outlined above is to use something like this to check types:\n#* Double a number\n#*\n#* @param n:int The number to double\n#*\n#* @get /double\nfunction(n) {\n  stopifnot(is.integer(n))\n  \n  n * 2\n}\nThis attempts to stop processing the request if the API receives a value for n that is not an integer. Unfortunately, it doesn’t work. And ironically, the reason why it doesn’t work is because of Plumber not enforcing types. If you run this API and make a request to /double with n=foobar, you’ll get the following response:\ncurl \"http://127.0.0.1:7012/double?n=foobar\" | jq\n\n{\n  \"error\": \"500 - Internal server error\",\n  \"message\": \"Error in (function (n) : is.integer(n) is not TRUE\\n\"\n}\nBut if you make the request with n=2, which you would expect to return 4, you’ll get the same error:\ncurl \"http://127.0.0.1:7012/double?n=2\" | jq\n\n{\n  \"error\": \"500 - Internal server error\",\n  \"message\": \"Error in (function (n) : is.integer(n) is not TRUE\\n\"\n}\nThere are three major problems here.\n\nParsing Integer-ish Variables\nThe first is that query parameters in Plumber are treated as strings and need to be coerced to other types, since Plumber doesn’t enforce the types as defined. This means that is.integer(n) will always return FALSE, no matter what value is provided. To get this to work, you’d need to do something like this: is.integer(as.integer(n)). But this also doesn’t work, since is.integer(as.integer(\"foo\")) returns TRUE. So this means that what we actually need to do is add yet another type check. Something like this would work, but look how cumbersome it is:\n#* Double a number\n#*\n#* @param n:int The number to double\n#*\n#* @get /double\nfunction(n) {\n  n &lt;- as.integer(n)\n  \n  stopifnot(is.integer(n) && !is.na(n))\n  \n  n * 2\n}\nLet’s run this code for a few examples:\n\nn1 &lt;- \"2\"\nn2 &lt;- \"foo\"\nn3 &lt;- \"2,3\"\n\ndouble &lt;- function(n) {\n  n &lt;- as.integer(n)\n  \n  stopifnot(is.integer(n) && !is.na(n))\n  \n  n * 2\n}\n\ndouble(n1)\n\n[1] 4\n\ndouble(n2)\n\nWarning in double(n2): NAs introduced by coercion\n\n\nError in double(n2): is.integer(n) && !is.na(n) is not TRUE\n\ndouble(n3)\n\nWarning in double(n3): NAs introduced by coercion\n\n\nError in double(n3): is.integer(n) && !is.na(n) is not TRUE\n\n\nGreat! That works. It’s just ugly since we now need to handle both the integer case and the NA case, but it’s not the end of the world.\n\n\nHTTP Status Codes\nThe second problem with the original stopifnot approach is that the API returns an HTTP 500 error – which generally means that something went wrong on the server side, unrelated to the client’s request – instead of an HTTP 400, which should be used for indicating bad parameter values. There’s lots of documentation on what HTTP status codes to return to the client for different request outcomes. Here’s one from Mozilla. In this case, the 500 indicates that the server did something wrong, and, aside from reading the error message, the client has no way of knowing what happened.\nTo fix this, we’d need to add a custom implementation of some logic that’s similar to stopifnot, but not exactly the same. The key is that stopifnot aborts the request, forcing an HTTP 500. If we want to return 4XX instead, we need to short-circuit the processing that happens in the endpoint.\n\n\n\n\n\n\nSomething to note is that if you’re working in a production service, error handling is something you need to have anyways. So I wouldn’t consider this to be too much overhead, but the amount of additional overhead is compounded by Plumber’s lack of proper tooling for these types of problems out of the box.\n\n\n\nSomething like this might work as a quick fix:\nerror_400 &lt;- function(res, msg) {\n  code &lt;- 400L\n\n  res$status &lt;- code\n  res$body &lt;- list(\n    status_code = code,\n    message = msg\n  )\n}\n\n#* Double a number\n#*\n#* @param n:int The number to double\n#* @serializer unboxedJSON\n#*\n#* @get /double\nfunction(req, res, n) {\n  n &lt;- as.integer(n)\n  if (!is.integer(n) || is.na(n)) {\n    error_400(res, \"N must be an integer.\")\n  } else {\n    return(n * 2)\n  }\n}\nIn practice, you’d want something much more robust than this. Some helpful examples for structuring Plumber errors are laid out in this post. But if we run the API now, let’s see how our error looks.\ncurl \"http://127.0.0.1:7012/double?n=foobar\" | jq\n\n{\n  \"status_code\": 400,\n  \"message\": \"N must be an integer.\"\n}\nMuch better.\n\n\nStack Traces and R Errors\nAnd that brings me to the last issue: The error message the client gets back from the API is an R error, which is a problem. We don’t want to return stack traces or errors that expose too much of the guts of the API to the client, as those could be used by a malicious actor to perform an attack. But secondly, and maybe more importantly: Not everyone speaks R, and so returning an R error is not helpful. The point of an API is to abstract away the R, Python, or any other implementation details from the client, and returning R errors does not achieve that goal.\nInstead, we want two things. First, we want to return an HTTP 4XX to the client to indicate that they did something wrong. And second, we want the error message they get back to indicate what they did wrong. Note that I achieved that in the changes I made above. Now, if the client supplies n=foobar, they get back an error message with a 400 status code and a message telling them that N must be an integer, so they know what they did wrong and how to fix it.\nSuspiciously, after all of this additional implementation to handle types and provide helpful errors, the Plumber service returns errors that look similar to what FastAPI gives out of the box, which is largely the point I’m trying to make in this post. While yes, it is certainly possible to handle all of these issues in Plumber and program around them, other frameworks give you them for free, and I generally don’t want to spend my time working out kinks in R’s already shoddy type system and in Plumber itself, when I could get the same from some type hints in the corresponding FastAPI code, and then get back to providing real value. That’s just my preference."
  },
  {
    "objectID": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html",
    "href": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html",
    "title": "Our 2021 Big Data Bowl Submission",
    "section": "",
    "text": "This post is my team’s 2021 NFL Big Data Bowl submission. My team was made up of me, Hugh McCreery (Baltimore Orioles), John Edwards (Seattle Mariners), and Owen McGrattan (DS student at Berkeley). I’m proud of what we’ve put forth here, and hopefully you find it interesting. At the end (after the Appendix), I’ve added some overall thoughts on things we were curious about or feel like we could have done better, as well as what we view as the biggest strengths and weaknesses of our submission. Enjoy!"
  },
  {
    "objectID": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#meta",
    "href": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#meta",
    "title": "Our 2021 Big Data Bowl Submission",
    "section": "",
    "text": "This post is my team’s 2021 NFL Big Data Bowl submission. My team was made up of me, Hugh McCreery (Baltimore Orioles), John Edwards (Seattle Mariners), and Owen McGrattan (DS student at Berkeley). I’m proud of what we’ve put forth here, and hopefully you find it interesting. At the end (after the Appendix), I’ve added some overall thoughts on things we were curious about or feel like we could have done better, as well as what we view as the biggest strengths and weaknesses of our submission. Enjoy!"
  },
  {
    "objectID": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#introduction",
    "href": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#introduction",
    "title": "Our 2021 Big Data Bowl Submission",
    "section": "Introduction",
    "text": "Introduction\nOur project aims to measure the ability of defensive backs at performing different aspects of their defensive duties: deterring targets (either by reputation or through good positioning), closing down receivers, and breaking up passes. We do this by fitting four models, two for predicting the probability that a receiver will be targeted on a given play and two for predicting the probability that a pass will be caught, which we then use to aggregate the contributions of defensive backs over the course of the season."
  },
  {
    "objectID": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#modeling-framework",
    "href": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#modeling-framework",
    "title": "Our 2021 Big Data Bowl Submission",
    "section": "Modeling Framework",
    "text": "Modeling Framework\nWe opted to use XGBoost for all of our models. At a high level, we chose a tree booster for its ability to find complex interactions between predictors, something we anticipated would be necessary for this project. Tree boosting also allows for null values to be present, which helped us divvy up credit in the catch probability models (more on this later). Finally, tree boosting is a relatively simple, easy-to-tune algorithm that generally performs extremely well, as was the case for us."
  },
  {
    "objectID": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#catch-probability",
    "href": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#catch-probability",
    "title": "Our 2021 Big Data Bowl Submission",
    "section": "Catch Probability",
    "text": "Catch Probability\nOur catch probability model has two distinct components: The catch probability at throw time – as in, the chance that the pass is caught at the time the quarterback releases the ball – and the catch probability at arrival time – the chance the pass is caught at the time the ball arrives. These probabilities are distinct since a lot can happen between throw release and throw arrival. First, we will walk through the features that are used in building each model.\nFor the throw time model (which we will refer to as the “throw” model) and the arrival time model (the “arrival” model), the most important predictors by variable importance were what we expected entering this project: the distance of the receiver to the closest defender, the position of the receiver in the \\(y\\) direction (i.e. distance from the sideline), the distance of the throw, the position of the football in the \\(y\\) direction at arrival time (this is mostly catching throws to the sideline and throw aways), the velocity of the throw, the velocity and acceleration of the targeted receiver, and a composite receiver skill metric. For the arrival model, we use many of the same features. However, we do not account for the distance of the defenders to the throw vector – which accounts for the ability to break up a pass mid-flight – because the throw has already arrived.\nThese two models both perform quite well, and far better than random chance. The throw model accurately predicts \\(74\\%\\) of all passes, with strong precision (\\(84\\%\\)) and recall (\\(76\\%\\)), and an AUC of \\(0.81\\). As can be expected, our arrival model outperforms the throw model in all measures - accurately predicting \\(78\\%\\) of all passes with a precision of \\(88\\%\\), a recall of \\(79\\%\\), and an AUC of \\(.87\\). All of these metrics were calculated on a held-out data set not used in model training. Below are plots of the calibration of the predictions of each of the models on the same held-out set.\n \nWe can do a few particularly interesting things with the predictions from these two models in tandem. Namely, we can use the two to calculate marginal effects of the play of the defensive backs. A simple example is as follows: For a given pass attempt, our throw model estimates that there is a \\(80\\%\\) chance of a catch. By the time that pass arrives, however, our arrival model estimates instead that there is a \\(50\\%\\) chance of a catch. Ultimately, the play results in a drop. In total, our defense can get credit for \\(+.8\\) drops, but we can break it down into \\(+.3\\) drops worth from closing on the receiver and \\(+.5\\) drops from breaking up the play, based on how the individual components differ. In other words, we subtract the probability of a catch at arrival time from the probability at throw time to get the credit for closing down the receiver, and we subtract the true outcome of the play from the probability of a catch at arrival time to get the credit for breaking up the pass.\nThe main challenge comes not from calculating the overall credit on the play, but from the distribution of credit among the defenders. In the previous example where we have to credit the defense with \\(+.8\\) drops added, who exactly on the defense do we give that credit to? There are a couple of heuristics that might make sense. One option would be to just split the credit up evenly among the defense, but this would be a bad heuristic because some defenders will have more of an impact on a pass being caught than others, and thus deserve more credit. We might also give all of the credit to the nearest defender, but that would be unfair to players who are within half a yard of the play and are also affecting its outcome but would get no credit under this heuristic. Ultimately, we opted to use the models to engineer the credit each player deserves by seeing how the catch probabilities would change if we magically removed them from the field, which we believe to be a better heuristic than the previous ones described. To implement this heuristic, we remove one defender from our data and re-run the predictions to see how big the magnitude of the change in catch probability is. The bigger the magnitude difference, the more credit that player gets. Then, we calculate the credit each defender gets with \\(credit_{i} = \\frac{min(d_{i}, 0)}{\\sum_{i} min(d_{i}, 0)}\\) where \\(d_{i}\\) is the catch probability without that player on the field minus the catch probability with him on the field. In other words, if one player gets \\(75\\%\\) of the credit for a play and the play is worth \\(+.8\\) drops added, then that player gets \\(.8 \\cdot .75 = +.6\\) drops of credit, and the remaining \\(+.2\\) drops are divvied up amongst the other defenders in the same fashion."
  },
  {
    "objectID": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#target-probability",
    "href": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#target-probability",
    "title": "Our 2021 Big Data Bowl Submission",
    "section": "Target Probability",
    "text": "Target Probability\nOur target model is based on comparing the probabilities that a receiver is targeted before the play begins and when the ball is thrown with the actual receiver targeted. We can use these probabilities to make estimates of how well the defender is covering (are receivers less likely to be thrown the ball because of the pre-throw work of a defensive back?) and how much respect they get from opposing offenses (do quarterbacks tend to make different decisions at throw time because of the defensive back?).\nTo determine the probability of a receiver being targeted before the play, we chose to take a naive approach. Each receiver on the field is assigned a “target rate” of \\(\\frac{targets}{\\sqrt{plays}}\\), which is then adjusted for the other receivers on the field and used as the only feature for a logit model. The idea of this rate was to construct a statistic that rewarded receivers for showing high target rates over a large sample but also gave receivers who play more often more credit.\nThe model for target probability at the time of throw was a tree booster similar to the two catch probability models. This model uses positional data, comparing the receiver position to the QB and the three closest defenders along with a variety of situational factors such as the distance to the first down line, time left, weather conditions, and how open that receiver is relative to others on the play to determine how likely that receiver is to be targeted.\nThe pre-snap model, which by design only considers the players on the field for the offense, performs relatively well for the lack of information with an AUC of \\(.59\\) and is well-calibrated on a held-out dataset. The pre-throw model performs much better given the extra information, with \\(89\\%\\) recall, \\(94\\%\\) precision, and \\(.94\\) AUC. Calibration plots of the models on held-out data are below.\n \nWe can estimate how a defensive back is impacting the decisions made by the quarterback through two effects. The first, comparing the target probability before the play to the target probability at the time of the throw is meant to estimate how well the receiver is covered on the play. For example, consider a receiver with a target probability of \\(20\\%\\) before the snap who ends up open enough to get a target probability of \\(50\\%\\) when the ball is thrown. This difference is attributed to the closest defensive back who would be credited with \\(-0.3\\) coverage targets. If on the same play another receiver had a pre-snap target probability \\(30\\%\\) but a target probability of \\(0\\%\\) at throw time, the closest defensive back would be credited with \\(+0.3\\) coverage targets. The other effect attempts to measure how the quarterback is deterred from throwing when that particular defensive back is in the area of the receiver by comparing the probability of a target to the actual result. So if a certain receiver has a target probability of \\(60\\%\\) at the time of the throw and isn’t targeted, the closest defender is credited with \\(+0.6\\) deterrence targets."
  },
  {
    "objectID": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#results",
    "href": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#results",
    "title": "Our 2021 Big Data Bowl Submission",
    "section": "Results",
    "text": "Results\nHaving produced these four models that gave us estimates of the influence of the defensive backs on a given play, we can accumulate the results over an entire season to produce an estimate of individual skill across the dimensions described by the models. As there is no straightforward way to measure the relative value of these skills, we chose to combine the individual skill percentiles for each defender as a measure of their overall skill. With that as the estimate, these are our top 15 pass-defending defensive backs in 2018:\n\n\n\nleaderboard\n\n\nA full leaderboard of these can be found on our Shiny app in the Overall Rankings tab. To help display these results and a few other metrics (such as how difficult the defensive assignment was based on pre-snap target probability and actual separation from the receiver at throw time) we developed player cards for each qualifying defender. For example, this is our card for #1 defender Richard Sherman:\n\n\n\nsherman_card\n\n\nThese cards can also be found on our Shiny app under Player Cards."
  },
  {
    "objectID": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#further-work",
    "href": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#further-work",
    "title": "Our 2021 Big Data Bowl Submission",
    "section": "Further Work",
    "text": "Further Work\nThere are several things that we didn’t consider or that would be interesting extensions of this project. Two clear ones are interceptions added and fumbles added, as those are hugely impactful football plays that can swing the outcomes of games. We also only considered raw changes in aggregating the player stats (i.e. targets prevented and drops added), but using EPA added instead would certainly be a better metric, since not all drops are created equal. In addition, it is not clear that a drop added is worth the same as a target deterred – an assumption we made – and EPA would help solve this problem too. It would also be interesting to test how similar our metrics are between seasons to confirm that our metrics are measuring the stable skill of a defender."
  },
  {
    "objectID": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#appendix",
    "href": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#appendix",
    "title": "Our 2021 Big Data Bowl Submission",
    "section": "Appendix",
    "text": "Appendix\nAll of our code is hosted in two Github repos: hjmbigdatabowl/bdb2021, which hosts our modeling code, and hjmbigdatabowl/bdb2021-shiny, which hosts the Shiny app.\nThere is extensive documentation for our code on our pkgdown site.\nThe Shiny app can be found here, which lets you explore our models, results, and player ratings."
  },
  {
    "objectID": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#thoughts",
    "href": "posts/2021-01-07-our-2021-big-data-bowl-submission/index.html#thoughts",
    "title": "Our 2021 Big Data Bowl Submission",
    "section": "Thoughts",
    "text": "Thoughts\nFirst, a disclaimer for everything that follows: I’m extremely proud of what we submitted, and the work that we did. We spent hundreds of hours working on this project over about three months, which is an enormous undertaking for four people with other full-time commitments. Especially in the final weeks, I was spending well over ten hours a week working on this project, and probably closer to fifteen or twenty. All of that said, in the grand scheme of things, the 300 hours or so collective hours that we spent conceiving of and working on this project is nowhere near the amount of time and effort that could be poured into a project that a data science team of four was working on full-time for three months. Given more time, I believe there are significant ways in which we could have improved our final product.\n\nStrengths\n\nCommunication and interpretability of the results. At the end of the day, when you work in a front office (as Hugh, John, and I know), you need coaches to believe what you’re telling them, and building a product that they can understand and explain is a major step in that direction. Overly complex or convoluted methodology will only get your work ignored, and I think that we did a particularly good job of building a product that is interpretable and useful.\nBuilding a model that evaluates something useful. At the end of the day, I’m happy with the statistics we chose to put forward. In my view, barring special events like fumbles forced, tackling, and interceptions (all valuable things that should be worked on further to expand this project), the way that we evaluate defensive back performance by looking at what we view as the four major components – deterring passes by reputation, deterring passes with good positioning and coverage, closing down receivers, and breaking up passes – are the four most important skills for defensive backs in the NFL.\nDivvying up credit in a clever way. I also think that the way we opted to divvy up the credit among the defenders in the two catch probability models was particularly clever, and not something that most teams would do. As we laid out in our submission, it doesn’t make sense to divvy credit evenly or give all credit to the nearest defender to the target, and I think that the approach that we took was both novel and easily defensible.\n\n\n\nWeaknesses + Potential Improvements\n\nClearly, weighing each of the four components of defense that we measured equally is the wrong way to go. A better strategy would be to try to correlate them with defensive EPA or a similar stat and use the correlation coefficients / R-Squared values / MSEs / etc. to weigh the four components based on how strongly they predict EPA. The problem, though, is that we don’t have a defensive EPA, which means we’d need to model it. In addition, it’s unclear how to model a counterfactual. What I mean by that is that for a statistic like deterrence, we need to measure the EPA caused by a defender not being thrown at, which is an inherently difficult thing to model. So difficult, in fact, that I believe that the first NFL team to come up with a good way of doing this will have a Moneyball-esque leg up against the competition until other teams catch up (Ravens analysts, I’m looking at you).\nThere are other aspects of defensive performance that we’re not measuring. Two important categories are turnovers generated and YAC prevented. Both of these are hugely valuable for preventing points, and both are models that we didn’t have time to build.\nStability testing. We’re probably interested in how stable our metrics are across seasons. For example, does Stephon Gilmore being a 96 overall this year predict his rating next year at all? Since we only have one season of data, the answer is that we don’t know. This type of stability testing would be useful for predicting future performance, though.\n\n\n\nGeneral Thoughts\nAgain, I’m proud of what we’ve done here. In particular, spot-checking some of the numbers has been fascinating. There’s often very little correlation between metrics, which seems good, and often the correlations can be negative. The way to think about this, as my teammate Hugh eloquently put it, is that we should think about the negative correlations as being similar to \\(ISO\\) vs. \\(K%\\) in the MLB: A player who is fantastic in coverage and thus often deters throws necessarily will have fewer opportunities to break up passes, meaning he will see a lower score for closing, breakups, or both. We see this type of behavior with Gilmore, Peterson, Sherman, and more. It’s also been interesting to find that our models think the same corners are really good as the eye test does, which is an encouraging sign. Interestingly, though, some of our top-ranked defensive backs are players who don’t tend to make top lists, and some of our lowest-ranked ones (like Jason McCourty and Jalen Ramsey), do. This is reminiscent of the attitude changes about certain MLB players in the early 2010s as we began developing more advanced statistics to measure performance (i.e. Derek Jeter’s defense being terrible and Michael Bourn being sneakily good).\nAnd with that, I’ve wrapped up my addendum to our submission. Feel free to reach out with any questions. My contact info is on my blog, my Github, my LinkedIn, etc. Thanks for reading!"
  },
  {
    "objectID": "posts/2023-03-20-metrics-calibration/calibration.html",
    "href": "posts/2023-03-20-metrics-calibration/calibration.html",
    "title": "Calibration and Evaluating Classification Models",
    "section": "",
    "text": "There’s something of a complexity trajectory in evaluating classification models that I’ve observed over the past few years. It starts with accuracy. But soon after learning about accuracy, data scientists are taught that accuracy is problematic for two reasons:\n\nIt doesn’t work well with unbalanced classes. This is the “if 95% of people don’t have cancer and you always predict ‘no cancer’, your model isn’t actually good” argument.\nIt doesn’t make any distinction between types of errors. In particular, it weighs false positives and false negatives equally, which may not be appropriate for the problem being solved.\n\nThese are both perfectly valid drawbacks of using accuracy as a metric. So then we move on. Next stop: precision and recall."
  },
  {
    "objectID": "posts/2023-03-20-metrics-calibration/calibration.html#introduction",
    "href": "posts/2023-03-20-metrics-calibration/calibration.html#introduction",
    "title": "Calibration and Evaluating Classification Models",
    "section": "",
    "text": "There’s something of a complexity trajectory in evaluating classification models that I’ve observed over the past few years. It starts with accuracy. But soon after learning about accuracy, data scientists are taught that accuracy is problematic for two reasons:\n\nIt doesn’t work well with unbalanced classes. This is the “if 95% of people don’t have cancer and you always predict ‘no cancer’, your model isn’t actually good” argument.\nIt doesn’t make any distinction between types of errors. In particular, it weighs false positives and false negatives equally, which may not be appropriate for the problem being solved.\n\nThese are both perfectly valid drawbacks of using accuracy as a metric. So then we move on. Next stop: precision and recall."
  },
  {
    "objectID": "posts/2023-03-20-metrics-calibration/calibration.html#precision-and-recall",
    "href": "posts/2023-03-20-metrics-calibration/calibration.html#precision-and-recall",
    "title": "Calibration and Evaluating Classification Models",
    "section": "Precision and Recall",
    "text": "Precision and Recall\nPrecision and recall are the two next most common classification metrics I’ve seen. Precision is the percentage of the time that your model is correct when it labels something as true. Recall is the percentage of the actual true examples that your model labels as true. These metrics are important for different reasons.\nA very precise model doesn’t make very many Type I errors. For instance, if you’re predicting whether or not someone has cancer, a very precise model is “trustworthy” in the sense that if it tells you that they do have cancer, they most likely do. You might think about precision as a metric in hiring: You probably want your hiring process to be very good at evaluating good candidates. A high-precision hiring process would mean that when you think you’ve found a person who would be a great fit on your team, you’re very likely correct.\nRecall is a bit different: It’s the percentage of the true labels that your model finds. A high-recall model suffers few false negatives: When something actually belongs to the true class, your model very often predicts it as such. You might think about this in the context of our cancer example from before. A higher recall model would mean that your model catches more of the cancer cases.\nDepending on your use case, you might optimize for one of these or the other. Or you could use a blend of the two, the most common of which is the F1 score, which is the harmonic mean of precision and recall. The idea of the F1 score is to optimize for a balance of both precision and recall, as opposed to optimizing for one at the cost of the other."
  },
  {
    "objectID": "posts/2023-03-20-metrics-calibration/calibration.html#predicting-probabilities",
    "href": "posts/2023-03-20-metrics-calibration/calibration.html#predicting-probabilities",
    "title": "Calibration and Evaluating Classification Models",
    "section": "Predicting Probabilities",
    "text": "Predicting Probabilities\nYou might be reading this thinking about how this is all about predicting classes, but very often we care about predicting probabilities. For instance, at CollegeVine we make predictions about each student’s chances of getting into their favorite colleges and universities. It’s not useful for students if we predict an acceptance or a rejection. After all, they want to know their chance, and to make a class prediction would mean that we determine a cutoff point at which we decide that if someone’s chance is above that threshold, they’ll get in. And if not, they won’t.\nThe problem is that there is no such threshold. More likely, college admissions is a bit of a game of chance: If five students each have a 20% chance of getting in to Carleton, for instance, I’d expect that about one of the five would get in on average. But it’d be disingenuous to make a class prediction, and I’m not sure how we’d even do that. For the five previously mentioned students, we’d expect one to get in. But if we were predicting classes we’d either predict all five to be accepted or all five to be rejected depending on where we set our threshold, and neither of those is the most likely scenario.\nWith all of that in mind, what metrics do we use instead? There are three metrics that we look at when we’re evaluating our models: The Brier Skill Score, AUC, and calibration. I’ve already written about AUC, and for the purposes of this post, I’m going to focus on the one I view as the most important: Calibration."
  },
  {
    "objectID": "posts/2023-03-20-metrics-calibration/calibration.html#calibration",
    "href": "posts/2023-03-20-metrics-calibration/calibration.html#calibration",
    "title": "Calibration and Evaluating Classification Models",
    "section": "Calibration",
    "text": "Calibration\nThe most important metric we track is calibration, which we’ll often evaluate by looking at a calibration plot. Below is an example of such a plot from FiveThirtyEight\n\nThe idea behind calibration is to answer the basic question “When my model said that someone had a 31% chance of winning an election, did they actually win about 31% of the time?” As you get more data, you can group your predictions into buckets to answer this question. For instance, if we’re predicting college chances we might take all the times that we said someone had between a 30 and 35% chance of getting accepted, and we’d calculate the actual proportion of the time that they were accepted. If it’s about 32.5% (close to the mean prediction in our bucket), we’d say that our model is making well-calibrated predictions in that bucket. If our model makes well-calibrated predictions of admissions chances across all buckets, it’s fair to say that it’s well-calibrated in general.\nCalibration is important because it directly affects how we interpret our model’s predictions. If we’re making probability predictions and they’re not well-calibrated, then when we say something has a 25% chance of happening, or a 50% chance of happening, or a 90% chance of happening, those numbers aren’t actually meaningful. It might be the case that the 50% probablity event happens more than the 25% one and less that the 90% one, but that isn’t even a guarantee. It also is probably not the case with a badly-calibrated model that the 50% predicted probability event happens twice as often as the 25% one.\nFor instance, let’s imagine we’re working on a classic machine learning example (and real-world) problem: Email spam detection. Ultimately, we need to predict a class: Given an email, we need our model to tell us if it’s spam or not. But for a large proportion of classifiers, this requires setting a probability threshold. For instance, if our model says there’s a greater than 50% chance that some email is spam, we mark it as spam. If false positives (a real email being marked as spam) are more problematic than false negatives (a spam email being marked as not spam), then we might increase our probability threshold to 80%, for example, which would make it “harder” for an email to be marked as spam. But with the higher threshold, the emails that we do mark as spam we’re more confident about. Often times, we’ll use domain knowledge to determine this threshold. I’ve had many conversations where we set arbitrary thresholds based on our experience or our gut instincts about how the system we’re working in should work. Often, those conversations end with something like “80% feels about right” and we go with that.\nHopefully you’re starting to see the issue here: If our model’s predictions are poorly calibrated, then it’s not possible to make a decision like that. We can’t lean into our domain knowledge about any particular number being a threshold that makes sense, because the probabilities we’re predicting don’t actually mean anything in practice. In other words, the fact that we don’t know if when we say 80%, it’s actually 95% or 35% or some other number makes it impossible to make decisions based on our predictions. In short, if our predictions aren’t well-calibrated, it’s not possible to reason about them in any meaningful way. They can also very easily be misleading.\nAs I’ve mentioned before, this is especially important is the probabilities themselves are the prediction. If you’re telling a student that their chances of getting into Carleton are 27%, it goes without saying that when you say they have a 27% chance, that if they were to apply four times they’d get in about once on average. If they get in about one in eight times instead, the miscalibrated prediction could have a meaningful, negative effect on their college outcomes. For instance, if you severely overpredict a student’s chances and they end up applying to fewer schools as a result, there’s an increased likelihood of them getting in nowhere, which would be a particularly bad outcome. In this case, better-calibrated predictions lead directly to better decision-making.\nIn statistics world, it might be helpful to think of poor calibration as a bias issue: You might think of better-calibrated predictions as being less biased, in the sense that the expected value of the outcome (the actual long-run frequency of a student being accepted) is closer to what your prediction was."
  },
  {
    "objectID": "posts/2023-03-20-metrics-calibration/calibration.html#wrapping-up",
    "href": "posts/2023-03-20-metrics-calibration/calibration.html#wrapping-up",
    "title": "Calibration and Evaluating Classification Models",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nHopefully I’ve convinced you that the calibration of your classifier’s predictions is important and practically meaningful. If a model’s predictions are poorly calibrated, it’s difficult (or impossible) to reason about them in a practical sense. Miscalibrated predictions can also be misleading, because we often naturally interpret probabilities as long-run frequencies. For instance “If I flip this coin 100 times, it’ll come up heads about half of the time.” You might think about working with poorly calibrated predictions being similar to flipping a biased coin when you’re unaware of its biases.\nIn the next post, I’ll talk about class imbalances and why balancing your classes can be a particularly bad idea."
  },
  {
    "objectID": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html",
    "href": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html",
    "title": "Exploring the Tail Behavior of ESPN’s Win Probability Model",
    "section": "",
    "text": "It’s College Football Playoff season, which means I’ve been watching a lot of games lately. And I find myself complaining pretty often about how badly calibrated I think ESPN’s win probability model is. In particular, I’ve noted a bunch of examples – or at least enough for it to feel like a bunch – of games where ESPN’s model gives a team a win probability that feels way too extreme in a situation where they’re clearly winning. I’m not talking about giving a team an 80% chance when they should have a 60% chance. The cases I’ve been curious about are something more like teams getting a 99.7% chance of winning when, at least in my opinion, they should be getting something more like a 98% chance."
  },
  {
    "objectID": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#introduction",
    "href": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#introduction",
    "title": "Exploring the Tail Behavior of ESPN’s Win Probability Model",
    "section": "Introduction",
    "text": "Introduction\n\nFor classification problems like predicting win probabilities, model calibration is, at the highest level, the answer to the question “When my model says that Michigan has a 62% chance to win, do they actually end up winning about 62% of the time?” A well-calibrated model will have relatively low error over the long run. As we get more and more data, we’d expect that the amount of error in our calibration numbers would go down, and hopefully the predicted probabilities start to converge to the actual win probabilities as the games play out. For more on calibration, check out this cool FiveThirtyEight post.\n\nYou might be reading this thinking that the difference (both in absolute terms and ratio terms) between 60 and 80 percent is way bigger than the difference between, say, 98 and 99.7. And you’d be right. But I’d encourage you to think about it like this: The team that’s the underdog in the latter case has either a 2% chance (the first case) or a 0.3% chance (the second case). If you’re applying the same ratio of win probabilities back of the napkin math, that increase feels a lot bigger.\n\nFor the statistically inclined folks in the back, the “right” way to do this is just to use odds ratios, which would show that going from 98% to 99.7% is a massive magnitude odds (or log-odds) increase.\n\nSo in a nutshell, what I’ve been curious about is the tail behavior of the ESPN model – I’m trying to answer the question of how ESPN’s model does at predicting events we know are unlikely. How often, for instance, does a team that ESPN gives a 0.5% mid-game chance of winning actually end up winning? My suspicion, based on my anecdotal evidence from watching games and complaining over the years, has been that the model is badly calibrated in the tails. I’ve been on the record arguing that ESPN’s model gives win probabilities greater than 99% way too often, and can usually be heard saying things like “Well, they’re almost definitely going to win. But 99%? I’m not sure…”\nSo then, to the question. I looked into a couple of things: 1. How well-calibrated is their model in general? 2. When ESPN gives a team a very high chance of winning (&gt;98%), how often do they actually win? 3. Does the model perform better or worse for ranked teams?"
  },
  {
    "objectID": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#calibration",
    "href": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#calibration",
    "title": "Exploring the Tail Behavior of ESPN’s Win Probability Model",
    "section": "Calibration",
    "text": "Calibration\nFirst, how well-calibrated is the model in general? I usually like to look at calibration plots to evaluate models, similar to the ones in the FiveThirtyEight post above.\nThis first plot is the overall calibration of the model at kickoff time. What we’re looking for are the points to roughly lie along the dashed line, which is the line y = x.\n\n\n\nTwo main things to notice in that plot:\n\nThe model, on aggregate, is quite well calibrated.\nThe model looks like it’s off by a bit in the lower tail, where it appears to be predicting win probabilities that are too low. That’s a sample size issue. For instance, there were 64 games where the model gave the home team a worse than 5% chance to win, and the home team ended up winning 6.25% in those games. But generating a confidence interval for that proportion gives us a range of 1.25%-12%, which is too wide to scold the model for that mistake\n\nWe can also look at the same plot, broken down by the teams playing. For instance, the following plot is broken down by whether neither team is ranked, one team is, or both teams are:\n\n\n\nIn this case, even with relatively wide error bars, we see that the model seems to perform worse for games where both teams are ranked. And it’s pretty clearly the best in games where neither team is ranked."
  },
  {
    "objectID": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#edge-cases",
    "href": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#edge-cases",
    "title": "Exploring the Tail Behavior of ESPN’s Win Probability Model",
    "section": "Edge Cases",
    "text": "Edge Cases\nNext, I was curious about how often teams given very high chances of winning ended up doing so. Anecdotally, I’ve found myself complaining the most about games like the Oregon - Oregon State game from 2022 where ESPN gives Oregon a 98.3% chance of winning when they’re up 18 with 6:53 left in the third. Of course, I’m leaning into confirmation bias. But it’s hard to not think to yourself that with more than 20 minutes of football to go, Oregon State only wins that game in a wild comeback less than two in one hundred times. I’m not sure what I’d view as a more “correct” estimate of their win probability, but seventeen in a thousand seems low to me. Maybe even thirty in a thousand (3%) would be better.\nOne note is that the 3% probability I’d probably lobby for doesn’t feel that different from the 1.7% that ESPN gave, but that’s an odds ratio of 1.79, which is a big difference in practice. For instance, that’s a similar odds ratio to what you’d get if you went from a 35% chance to a 50% chance, which is significant. In FiveThirtyEight world, that’s the difference between being right in the middle of the “toss-up” category vs. being solidly in the “lean Oregon” category.\nSo anyways, back to Oregon - Oregon State. I was curious about games like that: Games where, with more than, say, five minutes to go and one team leading by at most three scores (24 points), how often ESPN was right when they gave the leading team a better than 98% chance of winning the game.\nAs it turns out, ESPN’s model is doing pretty well in the tails on the whole. See the table below:\n\n\n\n\n\n\n\n\n\n\n\nRanked\nOverall Win %\nWin % CI Lower Bound\nWin % CI Upper Bound\nN\n\n\n\n\nBoth\n0.54%\n0%\n1.63%\n184\n\n\nOne\n1%\n0.4%\n1.7%\n1002\n\n\nNeither\n1.07%\n0.7%\n1.52%\n2427\n\n\nAll\n1.02%\n0.72%\n1.36%\n3613\n\n\n\n\nRanked corresponds to how many of the teams in the game were ranked (i.e. “both” means “both teams were ranked”). “all” is all of the data pooled together.\nThe main takeaway from the table above is that when ESPN gives a team a &lt;2% chance of winning a game, that tends to not be a severe underestimate as I was expecting. Across the crosstabs I checked, even the high end of a 95% confidence interval for the proportion of the time that the underdog would go on to win was below the 2% threshold."
  },
  {
    "objectID": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#wrapping-up",
    "href": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#wrapping-up",
    "title": "Exploring the Tail Behavior of ESPN’s Win Probability Model",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nAll told, I didn’t end up confirming my suspicions. At least from a cursory look through the data, ESPN’s model seems to be performing quite well in the tails. Or, at the very least, it’s not making predictions that are as ridiculous as I had thought they were. I still have my suspicions and will surely continue finding individual cases that don’t make sense to me intuitively, but after poking around a little I at least feel less concerned about the model making egregious predictions – as far as I can tell, it’s doing a pretty good job on average."
  },
  {
    "objectID": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#future-work",
    "href": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#future-work",
    "title": "Exploring the Tail Behavior of ESPN’s Win Probability Model",
    "section": "Future Work",
    "text": "Future Work\nA couple of other things jump out at me as being worth exploring:\n\nHow well did the model do vs. my intuitions? In games where I was on the record as thinking the win probabilities given were far too high (or low), how do I perform?\nHow does ESPN’s model perform by other common ML metric standards? For instance, does its AUC outperform (e.g.) Vegas? (Almost certainly not). Or how negative is the model’s Brier Skill Score when using Vegas as a baseline?\nDoes the model perform better or worse for certain teams? Maybe some teams are being consistently overrated or underrated by the model."
  },
  {
    "objectID": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#appendix",
    "href": "posts/2023-01-09-exploring-the-tail-behavior-of-espn-s-win-probability-model/index.html#appendix",
    "title": "Exploring the Tail Behavior of ESPN’s Win Probability Model",
    "section": "Appendix",
    "text": "Appendix\nYou can find the code to reproduce this analysis on my Github."
  },
  {
    "objectID": "posts/2021-01-10-what-i-ve-been-drinking-in-quarantine/index.html",
    "href": "posts/2021-01-10-what-i-ve-been-drinking-in-quarantine/index.html",
    "title": "What I’ve Been Drinking in Quarantine (Plus a Cocktail Lesson)",
    "section": "",
    "text": "The past ten-or-so months of limited activity due to Covid-19 have been a slog for everyone, to say the least. I’ve been fortunate to have avoided the worst of it, having spent much of the past ten months living in rural northwestern Connecticut.\nMany of us have had lots of time to explore hobbies over this time, and something that has gained lots of popularity – editor’s note: unsurprisingly – is drinking! In particular, there seems to have been a surge in home bartending and cocktail lessons happening during Covid, since hobbyist bartending – which has been, and continues to be, a hobby of mine – is a great Covid activity! By bartending, you get to learn about cocktails and build your skills to impress your friends when social gatherings start happening again, and you get to drink during it! What could be better than that?"
  },
  {
    "objectID": "posts/2021-01-10-what-i-ve-been-drinking-in-quarantine/index.html#beers-ive-been-loving",
    "href": "posts/2021-01-10-what-i-ve-been-drinking-in-quarantine/index.html#beers-ive-been-loving",
    "title": "What I’ve Been Drinking in Quarantine (Plus a Cocktail Lesson)",
    "section": "Beers I’ve Been Loving",
    "text": "Beers I’ve Been Loving\n\nGreen, Very Green, Juice Machine, and Haze by Tree House Brewing Company in Charlton, MA. Bascially, you can’t go wrong with Tree House. It’s one of the best breweries in the country, is all over the top ratings on Untappd, and makes almost univerally awesome beer. The ones I’ve listed are all very hazy, juicy, fruity (although not as much as some others, like Saturated and Iridescent) New England Style IPAs. That means they’re a little less bitter, a little less hoppy, and a little more like drinking orange juice than some of the other IPAs you’ve probably had elsewhere (Goose Island, Dogfish Head, etc.).\nFocal Banger and Heady Topper from The Alchemist in Stowe, VT. Similar to Tree House, The Alchemist is a wildly popular New England brewery with some awesome beers. I’ve liked all of the ones I’ve tried, but Heady Topper and Focal Banger are especially awesome. I actually prefer the two of them to most of the Tree House beers (bar Very Green, probably). They’re both a little less fruity than the Tree Houses, which I prefer.\nYou Drive Us Wild, by Grimm in New York City. This is another beer with a similar profile to the Tree House / Alchemist groups (are you sensing a pattern?). Grimm is also an awesome brewery, and I’ve loved almost everything of theirs that I’ve had. Magnetic Compass and Tesseract are other highlights."
  },
  {
    "objectID": "posts/2021-01-10-what-i-ve-been-drinking-in-quarantine/index.html#liquors-and-amari-ive-been-drinking",
    "href": "posts/2021-01-10-what-i-ve-been-drinking-in-quarantine/index.html#liquors-and-amari-ive-been-drinking",
    "title": "What I’ve Been Drinking in Quarantine (Plus a Cocktail Lesson)",
    "section": "Liquors and Amari I’ve Been Drinking",
    "text": "Liquors and Amari I’ve Been Drinking\nThese are my go-to glasses to sip on. Generally, liquor before or during dinner, and amaro after dinner. “Amaro” is the Italian word for bitter, so reader beware: the amari (especially Fernet) are very bitter.\n\nLagavulin 8 Year\nDel Maguey Chichicapa\nTequila Ocho Plata\nCynar\nCampari\nFernet Branca"
  },
  {
    "objectID": "posts/2021-01-10-what-i-ve-been-drinking-in-quarantine/index.html#cocktails",
    "href": "posts/2021-01-10-what-i-ve-been-drinking-in-quarantine/index.html#cocktails",
    "title": "What I’ve Been Drinking in Quarantine (Plus a Cocktail Lesson)",
    "section": "Cocktails",
    "text": "Cocktails\nSo, now for the main event. As promised, some great cocktails to try! A preface: I like a weird profile of cocktail. I’m not huge on sweet drinks, so the overly sweet margaritas from your neighborhood cantina aren’t what you’ll find here. I like sour, bitter, smoky, spicy, and herbal with just a touch of sweet. I’ll start with a few simple drinks (just a few easy to find ingredients), and then go through a couple of favorites that are a little more niche. At the end, I’ll give some general pointers on general cocktail making things and ingredients.\n\nNegroni\nThe Negroni is my all time favorite drink. It’s bitter, sweet, complex, and easy to make! Traditionally, it’s equal parts gin, Campari, and sweet vermouth:\n\n1oz gin\n1oz Campari\n1oz sweet vermouth\nOrange or lemon twist garnish\n\nAdd as much ice as you can fit into a beaker or other glass that you can stir in, and add the ingredients. Stir about thirty seconds until the glass is well chilled. Strain into the glass of your choosing. Garnish.\nFor a Negroni, you probably want a neutral, London dry style gin. I’ve found that Beefeater works great, and isn’t particularly expensive. The vermouth is the most important piece here, as it gives all kinds of interesting flavors to the drink depending on the brand you use. You’ll want to spend the extra few dollars to get something great, like Carpano Antica.\nPersonally, as a mezcal lover, I often find myself swapping out the gin in my Negronis for Del Maguey Vida, which is a high-quality mezcal that’s great for making cocktails with. I’ve also found that using apple brandy is delicious as well, and using bourbon gets you close to a Boulevardier. Feel free to experiment!\n\n\nManhattan\nThe Manhattan is another classic. I use rye in mine, but you can get away with bourbon too (although you might get weird from bartenders are certain bars for doing so).\n\n2oz rye whiskey\n1oz sweet vermouth\n4-6 dashes of bitters\nMaraschino cherry garnish\n\nAdd as much ice as you can fit into a beaker or other glass that you can stir in, and add the ingredients (preferably bitters-first so they don’t just sit on top of the ice). Stir about thirty seconds until the glass is well chilled. Strain into the glass of your choosing. Garnish.\nSince the Manhattan is so simple, you really need to use high quality ingredients. I’ve found Rittenhouse Rye to be a fantastic rye, especially given the price. As before, you probably want Carpano Antica for the vermouth.\n\n\nMargarita\nYet another classic, everyone’s familiar with a margarita. It’s simple and delicious.\n\n2oz tequila\n1oz lime juice\n.75oz triple sec, Cointreau, or 1:1 simple syrup\n\nAdd ingredients to a shaker with as much ice as you can fit and shake vigorously for 30 seconds until the shaker is well chilled. Double strain into your glass of choice. Optionally, you can salt the rim of your glass.\nYou have some options for this one. Again, as a mezcal lover, I often swap the tequila for mezcal (or go 50/50). I also prefer less sweet drinks, so if .75oz of sugary stuff isn’t enough for you, just add some more!\n\n\nMoscow Mule\nThe copper cup drink, and a super easy, delicious vodka cocktail.\n\n2oz vodka\n3-5oz ginger beer, depending on how strong you like it\nsqueeze of lime juice\n\nFor this one, I normally just add the ingredients to a glass, stir, and sip away! Using more ginger beer will mellow out the drink a lot.\nNow, with some delicious, easy classics that won’t let you down out of the way, on to some more involved drinks! Some of these require harder-to-find ingredients (or just more ingredients), but I think they’re all delicious and worth a shot.\n\n\nLast Word\nThe Last Word is an interesting one. It’s sweet, sour, herbal, and definitely not everybody’s cup of tea. It’s another easy equal-parter:\n\n.75oz lime juice\n.75oz green Chartreuse\n.75oz Luxardo Maraschino liqueur\n.75oz gin\nMaraschino cherry garnish\n\nAdd the ingredients to a cocktail shaker with as much ice as you can fit, and shake vigorously for about 30 seconds until the shaker is well chilled. Double strain into your glass of choice.\nThis is a weird mix of ingredients, but it comes together to make a complex, delicious drink! For the adventurous, it’s definitely worth a shot.\n\n\nJungle Bird\nThe Jungle Bird is a bitter drink fan’s tiki drink. It’s a delicious mix of tiki and bitter, and is a real crowd-pleaser.\n\n1.5oz dark rum\n1.5oz pineapple juice\n.75oz Campari\n.5oz lime juice\n.5oz 1:1 simple syrup\n\nAdd the ingredients to a cocktail shaker with as much ice as you can fit, and shake vigorously for about 30 seconds until the shaker is well chilled. Double strain into your glass of choice.\nThe Jungle Bird is another favorite of mine. It’s not too bitter, not too sweet, and you also get some lime and pineapple in there, which I love both of. It’s a tough one to go wrong with, since it’s not as strong as something like a Manhattan, less bitter than a Negroni, and less sweet than a Margarita can be.\n\n\nCorpse Reviver #2\nAnother personal favorite of mine! The middle child of a trio of Corpse Revivers, I find this one to be totally delicious. Probably named for being the drink that’ll get you on your feet the next morning (need confirmation on that).\n\n1.5oz gin\n1.5oz Lillet Blanc\n1.5oz lemon juice\n.75oz triple sec or Cointreau\n&lt;.25 (small dash) absinthe, Pernod, or green Chartreuse\n\nAdd the ingredients to a cocktail shaker with as much ice as you can fit, and shake vigorously for about 30 seconds until the shaker is well chilled. Double strain into your glass of choice.\nThis is another personal favorite. It’s sour, a little sweet, and very complex. The Lillet adds a lot of character and plays really well with the lemon and the herbal notes from the absinthe.\n\n\nEnzoni\nThe Enzoni is a remix of the Negroni that’s a little sweeter, and uses fresh grapes! It’s equally delicious, and similar in profile to a Jungle Bird (a little sweet and less bitter than the Negroni).\n\n1oz gin\n1oz Campari\n.75oz lemon juice\n.5oz 1:1 simple syrup\n5 white grapes\n\nMuddle the grapes with the simple syrup in the bottom of a cocktail shaker. Then, add as much ice as you can with the rest of the ingredients, and shake vigorously for about 30 seconds until the shaker is well chilled. Double strain into your glass of choice.\nThe Enzoni is an awesome drink. The acidity from the lemon and the sweetness and flavor from the grapes make this a unique and delicious drink.\n\n\nBitter Giusseppe\nFinally, a bitters-forward drink! The Bitter Giusseppe is simple, and checks a lot of boxes for me. It’s sour and bitter, and it’s also a low alcohol content drink!\n\n2oz Cynar\n1oz sweet vermouth\n.25oz lemon juice\n4 dashes of bitters\n\nAdd as much ice as you can fit into a cocktail shaker with the ingredients and shake vigorously for about 30 seconds until the shaker is well chilled. Double strain into your glass of choice.\nAs usual, you probably want to be using Carpano Antica as your vermouth here. It adds a lot of interesting flavors to the drink. This one is awesome for an easy sipper, and the acidity from the lemon really takes it over the top"
  },
  {
    "objectID": "posts/2021-01-10-what-i-ve-been-drinking-in-quarantine/index.html#general-pointers",
    "href": "posts/2021-01-10-what-i-ve-been-drinking-in-quarantine/index.html#general-pointers",
    "title": "What I’ve Been Drinking in Quarantine (Plus a Cocktail Lesson)",
    "section": "General Pointers",
    "text": "General Pointers\n\nCocktail Making\n\nThe reason we shake or stir our drinks is not to mix the ingredients together. Well, it is, but that’s not the primary reason. We’re shaking or stirring to dilute the drink. If you want an example of the importance of dilution, try making a Negroni in a glass without any ice, and just stir it together and taste it. Unless you like an extremely alcohol-forward, bitter drink (which you might, I do), you’ll probably like it far better after stirring with ice. The dilution and chilling of the drink does a lot to enhance the flavor, make the drink more enjoyable, and take the edge off. Don’t skip it!\nYour ingredients matter. High quality ingredients will make better tasting drinks. You really don’t want to be trying to mask bad rye in a Manhattan, for example. You want to be showcasing a great rye!\nSpeaking of ingredients: squeeze your own juice, or at least your own lemon and lime juice. The stuff from the bottle with all of the preservatives is just not the same. If you don’t believe me, do a blind taste test of a margarita made with fresh lime juice vs. one with bottled juice, and see what you think. It’s worth it.\nYou can (and should) make your own simple syrup! It’s easy and takes two minutes, so there’s no reason to spend $7 on a small bottle from the store. All of my recipes call for 1:1 syrup, which means 1 part sugar, 1 part water. If you’re making it, that just means put a cup of sugar and a cup of water in a pot and heat it up gently until the sugar is dissolved. It will keep in the fridge for about a month. If you make 2:1 syrup, cut the amounts in the recipes in half. 2:1 syrup will keep in the fridge for far longer than 1:1 syrup.\n\n\n\nIngredients\n\nBuy lemons and limes. It makes all the difference in your drinks.\nThere are a lot of different types of liquor discussed here. These are the brands I like, but feel free to experiment with others! The key is to make drinks you like:\n\nGin I use Beefeater for everything. It’s relatively inexpensive, available everywhere, and a great, neutral-flavored London dry gin. For a more herbal gin, go for Hendricks.\nTequila This is a blog post in itself. At the very least, get a 100% agave tequila. Espolón and Olmeca Altos are good choices. If you want sipping tequila, you want to be buying something from a NOM (producer) who doesn’t use diffusers or autoclaves. This means NOT Clase Azul, Casamigos, or Patron. Good tequilas are fermented in barrels after the agave is roasted in ovens made of brick or stone, and NOMs using diffusers and autoclaves are cutting corners by using chemicals and high-pressure chambers to decrease their costs and speed up the process. The product suffers as a result, and they often mask bad product by adding sugar to their tequilas, which is why many people will tell you that Clase Azul is “smooth.” It is. That’s added sugar making it taste like that. There are a number of great (harder to find) tequilas that are both great for drinking and an opportunity to support distillers doing things the right way. A few that I love are Fortaleza, Tapatio, Tequila Ocho, and Siete Leguas.\nMezcal Everything by Del Maguey is great. Chichicapa is an incredible sipping mezcal (but more expensive), and I use Vida for all of my mezcal cocktails.\nVodka Expensive vodka is not necessarily good vodka. Personally, I strongly dislike Tito’s, Absolut, and Grey Goose. I’ve found that Tower (from Texas), Smirnoff, and Russian Standard all work fine, and are all far less expensive. A good vodka should taste and smell like nothing, and that’s basically what all three of those vodkas will give you. If you want to impress your friends, fill a Grey Goose bottle with Smirnoff.\nRye As I said before, I’ve had great success with Rittenhouse. It’s a great rye, and it’s not very expensive. Knob Creek is also great if you want to spend the extra money.\nBourbon If you want to use bourbon in a Manhattan (or just to sip), two that I like a lot are Buffalo Trace and Maker’s Mark.\nRum For white rum, I normally just use Bacardi Superior. Plantation is another good one, albeit slightly more expensive. Dark rum preferences will depend on your taste, as some are much sweeter than others. Again, Plantation makes good rum. Or, if you’re ever traveling abroad post-Covid and want to bring back Havana Club Especial duty free, I’d recommend that. We can’t buy it in the U.S. because of the trade embargo with Cuba, which is a shame for many reasons, including Havana Club being great rum.\n\nOther ingredients\n\nVermouth Dolin is fine, Carpano is great. Spend the extra money, it’s worth it.\nGinger Beer, Tonic, etc. My personal favorite brand for mixers is Fever Tree. Their stuff is a little more expensive, but it’s worth it. It’s all great, and will make your drinks even better.\n\nPro-tip: Lots of better bars and restaurants will have a secret “bartender’s choice” option that isn’t listed on the menu. This is a great way to broaden your cocktail horizons. Historically, I’ve just asked for things like “A mezcal drink that’s a little smoky, sour, or bitter, but not too sweet” and have discovered some great drinks! The key is letting a knowledgeable bartender know in broad strokes what you like, and letting them be creative!\nExperiment, and make drinks you like! At the end of the day, you’re drinking for you, so why not enjoy your drink while you’re at it?"
  },
  {
    "objectID": "posts/2023-03-09-on-auc-roc/index.html",
    "href": "posts/2023-03-09-on-auc-roc/index.html",
    "title": "Interpreting AUC-ROC",
    "section": "",
    "text": "AUC goes by many names: AUC, AUC-ROC, ROC-AUC, the area under the curve, and so on. It’s an extremely important metric for evaluating machine learning models and it’s an uber-popular data science interview question. It’s also, at least in my experience, the single most commonly misunderstood metric in data science.\nI’ve heard several common misunderstandings or flat-out falsehoods from people in all kinds of roles discussing AUC. The biggest offenses tend to come from overcomplicating the topic. It’s easy to see the Wikipedia page for the ROC curve and be confused, intimidated, or some combination of the two. ROC builds off of other fundamental data science concepts – the true and false positives rates of a classifier – so it’s certainly not a good place to start learning about metrics for evaluating the performance of models.\nThe most common cause for confusion about AUC seems to come from the plot of the ROC curve, and nothing particularly special about AUC itself. Generally, I’ll hear AUC explained as being the area under the ROC curve, and that it’s all about testing how well your model balances false positives and false negatives. That’s all well and good, but it doesn’t give someone new to AUC any intuition about what AUC actually means in practice. For instance, let’s imagine we’re trying to predict the chance that a student is accepted at Carleton College – a quite common problem at CollegeVine! How does saying “AUC tells me about how my model is balancing false negatives and false positives” tell me anything about how well my model is doing at predicting that student’s chances?\nThe main issue I have with this factual-yet-unhelpful explanation of AUC is just that: While it may be true, it doesn’t get to the point. And even worse, it’s sometimes used as a crutch: A fallback answer when someone feels stuck when asked how to interpret AUC in real, practical terms.\nSo in this post, I’ll focus on just one thing, then: Answering the question above about how to interpret AUC."
  },
  {
    "objectID": "posts/2023-03-09-on-auc-roc/index.html#what-is-auc",
    "href": "posts/2023-03-09-on-auc-roc/index.html#what-is-auc",
    "title": "Interpreting AUC-ROC",
    "section": "What is AUC?",
    "text": "What is AUC?\nAs I mentioned, it’s usually not helpful to try to explain AUC to someone by telling them that it’s just the area under the ROC curve, or that it’s a metric you can use for predicting probabilities as opposed to predicting classes, or that it’s a metric trying to balance false positives and false negatives. None of those things get to the crux of the problem.\nSo what is AUC, then? It’s pretty simple: Let’s imagine a model \\(M\\) being evaluated on data \\(X\\) where \\(X\\) contains some instances of the true class and some instances of the false class. The AUC of \\(M\\) on \\(X\\) is the probability that given a random item from \\(X\\) belonging to the true class (\\(T\\)) and another random item from \\(X\\) belonging to the false class (\\(F\\)), that the model predicts that the probability of \\(T\\) being true (belonging to the true class) is higher than the probability of \\(F\\) being true (belonging to the true class).\nLet’s go back to the example about Carleton admissions, and let’s imagine that we have a model that gives a probability of admission to Carleton given some information about a student. If I give the model one random accepted student and one random rejected student, the AUC of the model is the probability that the accepted student had a higher chance of acceptance (as estimated by the model) than the rejected student did.\nFor more on this, I’d refer everyone to this fantastic blog post by the team at Google, which does a great job at explaining further and/or better."
  },
  {
    "objectID": "posts/2023-03-09-on-auc-roc/index.html#a-simple-implementation",
    "href": "posts/2023-03-09-on-auc-roc/index.html#a-simple-implementation",
    "title": "Interpreting AUC-ROC",
    "section": "A Simple Implementation",
    "text": "A Simple Implementation\nThe easiest way to convey this idea might be to show a simple implementation of AUC. Below is some R code.\nFirst, let’s start by writing a function to do exactly what’s described above. Again, here’s the algorithm given some evaluation data:\n\nChoose a random item from the true class.\nChoose a random item from the false class.\nMake a prediction on each of the two items.\nIf the predicted probability for the actually true item is greater than the predicted probability for the actually false item, return true. Otherwise, return false. If they’re equal, flip a coin.\nRepeat 1-4 many times, and calculate the proportion of the time your model guessed correctly. This is your AUC.\n\nNow, let’s write this in R with a little help from some vectorization.\n\nlibrary(rlang)\nlibrary(dplyr)\nlibrary(tibble)\n\n## Our AUC implementation\n## In this implementation, we take a data frame containing a \"truth\" (i.e. whether\n## the example is _actually_ in either the true class or the false class)\n## and an \"estimate\" (our predicted probability).\n## This implementation is in line with how {{yardstick}} implements all of its metrics\ninterpretable_auc &lt;- function(data, N, truth_col = \"truth\", estimate_col = \"estimate\") {\n  \n  ## First, subset the data down to just trues and just falses, separately\n  trues &lt;- filter(data, .data[[truth_col]] == 1)\n  falses &lt;- filter(data, .data[[truth_col]] == 0)\n  \n  ## Sample the predicted probabilities for N `true` examples, with replacement\n  random_trues &lt;- sample(trues[[estimate_col]], size = N, replace = TRUE)\n  \n  ## Do the same for N `false` examples\n  random_falses &lt;- sample(falses[[estimate_col]], size = N, replace = TRUE)\n\n  ## If the predicted probability for the actually true\n  ##  item is greater than that of the actually false item,\n  ##  return `true`. \n  ## If the two are equal, flip a coin.\n  ## Otherwise, return false.\n  true_wins &lt;- ifelse(\n    random_trues == random_falses,\n    runif(N) &gt; 0.50,\n    random_trues &gt; random_falses\n  )\n  \n  ## Compute the percentage of the time our model was \"right\"\n  mean(true_wins)\n}\n\nNext, we can test our simple implementation against yardstick on some real data. For the sake of demonstration, I just used the built-in mtcars data. Here’s how the data looks:\n\nlibrary(knitr)\nlibrary(kableExtra)\n\n## Doing a little data wrangling\ndata &lt;- mtcars %&gt;%\n  transmute(\n    vs = as.factor(vs),\n    mpg,\n    cyl\n  ) %&gt;%\n  as_tibble()\n\ndata %&gt;%\n  slice_sample(n = 6) %&gt;%\n  kable(\"html\", caption = 'Six rows of our training data') %&gt;%\n  kable_styling(position = \"center\", full_width = TRUE)\n\n\nSix rows of our training data\n\n\nvs\nmpg\ncyl\n\n\n\n\n1\n32.4\n4\n\n\n0\n16.4\n8\n\n\n0\n15.8\n8\n\n\n1\n30.4\n4\n\n\n1\n17.8\n6\n\n\n1\n24.4\n4\n\n\n\n\n\n\n\nNow, let’s fit a few logistic regression models to the data to see how our AUC implementation compares to the yardstick one.\n\nlibrary(purrr)\nlibrary(yardstick)\n\n## Simplest model -- Just an intercept. AUC should be 50%\nmodel1 &lt;- glm(vs ~ 1, data = data, family = binomial)\n\n## Adding another predictor\nmodel2 &lt;- glm(vs ~ mpg, data = data, family = binomial)\n\n## And another\nmodel3 &lt;- glm(vs ~ mpg + cyl, data = data, family = binomial)\n\n## Make predictions for all three models\npreds &lt;- tibble(\n  truth = data$vs,\n  m1 = predict(model1, type = \"response\"),\n  m2 = predict(model2, type = \"response\"),\n  m3 = predict(model3, type = \"response\")\n)\n\n## For each model, compute AUC with both methods: Yardstick (library) and \"homemade\"\nmap_dfr(\n  c(\"m1\", \"m2\", \"m3\"),\n  ~ {\n    yardstick &lt;- roc_auc(preds, truth = truth, estimate = !!.x, event_level = \"second\")$.estimate\n    homemade &lt;- interpretable_auc(preds, N = 100000, truth_col = \"truth\", estimate_col = .x)\n    tibble(\n      model = .x,\n      yardstick = round(yardstick, digits = 2),\n      homemade = round(homemade, digits = 2)\n    )\n  }\n) %&gt;%\n  kable(\"html\", caption = 'Yardstick vs. Our Implementation') %&gt;%\n  kable_styling(position = \"center\", full_width = TRUE)\n\n\nYardstick vs. Our Implementation\n\n\nmodel\nyardstick\nhomemade\n\n\n\n\nm1\n0.50\n0.50\n\n\nm2\n0.91\n0.91\n\n\nm3\n0.95\n0.95\n\n\n\n\n\n\n\nAs we’ve seen here, AUC actually shouldn’t be all that much of a cause for confusion! The way I like to frame it is this: The AUC of your model is how good your model is at making even-odds bets. If I give your model two options and ask it to pick which one it thinks is more likely, a “better” model (by AUC standards) will be better at identifying the true class more often.\nIn real terms, that’s a meaningful, good thing. If we’re trying to predict the probability of a cancer patient having cancer, it’s important that our model can distinguish between people with cancer and people without it when given one person from each class. If it couldn’t - meaning the model was either randomly guessing or doing worse than random - the AUC would be 50% (or below 50%, in the worse-than-random disaster scenario)."
  },
  {
    "objectID": "posts/2023-03-09-on-auc-roc/index.html#additional-thoughts",
    "href": "posts/2023-03-09-on-auc-roc/index.html#additional-thoughts",
    "title": "Interpreting AUC-ROC",
    "section": "Additional Thoughts",
    "text": "Additional Thoughts\nI also often hear the misconception that AUC is sensitive to things like class imbalance. This means that if the true class makes up a disproportionately large (or small) proportion of the evaluation data, that can skew the AUC. But based on the intuition we just built before, that’s of course not true. The key thing to remember is that the model is given one true and one false example. In choosing those, it doesn’t matter if the true class only makes up 0.005% of all of the examples in the evaluation data: AUC is only evaluating the model on its ability to determine which of the two is the true class.\nHowever, there is one thing related to class imbalance, and just sample size in general, that would affect AUC, which is the raw number of examples of each class in the evaluation data. If, for instance, you had only a single instance of the true class in the evaluation set, then the AUC of the model is entirely determined by how good the predictions of the model are on that single example. For instance, if we have a single true class and the model predicts a 100% probability of it being true, then, assuming the predictions for all of the other examples in the evaluation set are not 100%, the AUC of the model as evaluated on that data is 100%. This isn’t necessarily because the model is “good” in any sense, but just because the model is over-indexing to a single good prediction in the evaluation set. In practice though, this AUC estimate wouldn’t generalize. As we got more data, the predictions for all the true classes would certainly not all be 100%, so the AUC of the model would go down over time.\nFortunately, there’s an easy fix for this problem. AUCs are a point estimate, but we could also estimate a margin of error or a confidence interval for our AUC. For a situation where we only have a single instance of the true class in the evaluation set, the margin of error for our AUC would be very wide."
  },
  {
    "objectID": "posts/2023-03-09-on-auc-roc/index.html#wrapping-up",
    "href": "posts/2023-03-09-on-auc-roc/index.html#wrapping-up",
    "title": "Interpreting AUC-ROC",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nHopefully this post helped give a better intuition for what AUC actually is! A couple of major takeaways:\n\nAUC doesn’t need to be this super complicated thing about trading off between false positives and negatives and trying many different classification thresholds and such. In my opinion, it’s much simpler to just think about it as the likelihood of a guess that your model makes between two choices being correct.\nAUC isn’t affected by class imbalances."
  },
  {
    "objectID": "posts/2021-12-22-notes-on-hiring-data-analysts-scientists/index.html",
    "href": "posts/2021-12-22-notes-on-hiring-data-analysts-scientists/index.html",
    "title": "Notes on Hiring Data Analysts + Scientists",
    "section": "",
    "text": "In the past four months, I’ve been involved in hiring for two new roles at CollegeVine: a second data scientist, and our first data analyst. I’ve learned a lot along the way: Things that work, things that don’t, and things to ask in order to maximize the amount of signal we’re getting from our interviews. This post will sketch out our hiring process (the processes are very similar for our DS and DA roles, with slightly different questions), and I’ll add some notes about things I’ve learned along the way. It’s been a long time since I’ve written anything! I’m excited, so here goes."
  },
  {
    "objectID": "posts/2021-12-22-notes-on-hiring-data-analysts-scientists/index.html#introduction",
    "href": "posts/2021-12-22-notes-on-hiring-data-analysts-scientists/index.html#introduction",
    "title": "Notes on Hiring Data Analysts + Scientists",
    "section": "",
    "text": "In the past four months, I’ve been involved in hiring for two new roles at CollegeVine: a second data scientist, and our first data analyst. I’ve learned a lot along the way: Things that work, things that don’t, and things to ask in order to maximize the amount of signal we’re getting from our interviews. This post will sketch out our hiring process (the processes are very similar for our DS and DA roles, with slightly different questions), and I’ll add some notes about things I’ve learned along the way. It’s been a long time since I’ve written anything! I’m excited, so here goes."
  },
  {
    "objectID": "posts/2021-12-22-notes-on-hiring-data-analysts-scientists/index.html#our-process",
    "href": "posts/2021-12-22-notes-on-hiring-data-analysts-scientists/index.html#our-process",
    "title": "Notes on Hiring Data Analysts + Scientists",
    "section": "Our Process",
    "text": "Our Process\nOur hiring process only has a few steps. We try to keep things simple and quick:\n\nA phone screen with the hiring manager (30-45 minutes)\nA technical interview with two data scientists (60-90 minutes)\nA technical interview with two software developers (60 minutes, data science only)\nTwo cultural / behavioral interviews\n\nSince my portion of the process is the data science part, I’ll leave the phone screens, behavioral rounds, and system design round out of this post."
  },
  {
    "objectID": "posts/2021-12-22-notes-on-hiring-data-analysts-scientists/index.html#the-data-science-analysis-interview",
    "href": "posts/2021-12-22-notes-on-hiring-data-analysts-scientists/index.html#the-data-science-analysis-interview",
    "title": "Notes on Hiring Data Analysts + Scientists",
    "section": "The Data Science / Analysis Interview",
    "text": "The Data Science / Analysis Interview\nI’ve done a whole bunch of data science interviews from both sides of the table. Some have been better than others. Often, while on the hot seat, I’ve gotten a smorgasborg of questions that felt like they came from the first page of the first Google result for “Data science interview questions.” A handful of examples:\n\nWhat’s the difference between supervised and unsupervised learning?\nWhat’s the difference between linear regression and logistic regression?\nWhat’s a p-value?\nHow do you know when something is an outlier?\n\nAnd many, many more. In my view, these questions are fine. They ask about a relevant concept to data science and put your skills to the test. But I have two issues with them. First, they’re neither challenging nor questions that are easy to build on in terms of difficulty. Second, they’re not going to be good questions to figure out someone’s ability level.\n\nCreating Challenging Questions\nWhen I say the questions above aren’t challenging, I mean that these are the kinds of questions that someone who’s taken a single stats or machine learning class would be able to answer. This is fine if you’re trying to figure out if someone has taken one of said classes, but that isn’t our goal. We’re trying to determine your ability to succeed as a data scientist or analyst. This means that we need to know more than just your academic background: We need to know how you reason about stats and ML, and how you think in general. How do you approach a statistics problem where you haven’t seen the solution before? Can you intuit an answer and defend it?\nAs a result, we’ve designed our questions to be challenging enough that you wouldn’t have seen them in a class you took, and to build off of one another. For example, we often ask the following question:\n\nImagine you’re looking at a distribution of the heights of men in Spain. How might you find which men are outliers?\n\nSure, easy question. There are a handful of fine answers here, but we’re generally looking for something along the lines of using a Z-Score or the IQR. Either of those shows that you’ve seen some rule of thumb in a statistics class and realize that you can apply it to this problem.\nBut then, we ask a follow-up:\n\nNow, let’s imagine you’re looking at the distribution of incomes of men in Spain, instead of their heights. Does your approach change at all?\n\nThis is a question that most candidates need to think about for a little bit. At first, it seems simple. But we give you a hint: We explicitly ask if your approach to the problem would change, which is a push to think about how it might change. Many candidates struggle with this question, seemingly for a few reasons:\n\nThey understand the hint, but don’t immediately realize why the Z-Score or the IQR approach breaks down, so they feel stuck.\nThey understand why those approaches don’t work, but it doesn’t jump out at them what they should do about it.\n\nThese types of responses aren’t surprising to us: In statistics classes, you normally work with normally distributed data where nice properties and rules of thumb hold up, but now we have a problem: Everyone knows that incomes are skewed, and so now what do we do? Some candidates stick to their guns and insist the IQR or Z-score would still be fine. Here’s how I’d answer the question:\n\nFirst, I’d make a point that incomes are right-skewed. Everyone probably has this image in their head already, but it’s important.\nNext, I’d note that the IQR / Z-score approach would break down, since the rules of thumb we use about 95% of the data (e.g.) lying within 2 standard deviations of the mean only works on a normal distribution, which incomes do not follow. This means we can’t just arbitrarly say “Oh, this person is more than 2 standard deviations from the mean, so he’s an outlier!” anymore.\nFinally, I’d think about other approaches. One might be something fancy like an isolation forest, but I think there’s a simpler approach that’d work: Since incomes are severely right skewed, we could try taking the log of the incomes to see if the logged income looks roughly normally distributed. If it does, we can fall back on our IQR or Z-score approach.\n\nThe point here is to ask the candidate a question that makes them think intuitively about having to solve a real-world problem (in this case, one we face all the time) that they probably haven’t seen before, which gives us a lot of signal about their statistical foundations and intuitions.\nWe follow up this follow-up with another:\n\nNow, let’s imagine we’re looking for outliers in the heights of men again, but this time they’re from Sweden and Spain. Does your approach change?\n\nSimilar question here, with no real clear, immediate answer that jumps out at most candidates. A reasonable response would be to just separate the distributions into Swedish and Spanish (since we know that Swedes are taller than Spaniards, on average), and then fall back on the Z-score or IQR approach again. Again, not a particularly challenging, in the weeds, or niche technical question by any stretch, but definitely one that lets us really get a sense for your intuitive ability.\n\nTL;DR: We build questions that aren’t tricky (in the trick question sense), but should’t have an immediately obvious canned answer. These types of questions should give us a window into how you reason intuitvely about statistics and machine learning concepts at all levels of complexity.\n\n\n\nLayering Questions\nThis point is a nice segue into the second issue I noted above: It’s important to build questions that have multiple layers of difficulty to them, so that if someone immediately answers your question, you don’t completely shift gears and move to a different topic. Instead, we want to keep digging, so that we can figure out just how much you know. The question I laid out above is a good example of a simple, relatively straightforward question with multiple layers.\nHere’s another example:\n\nImagine you’re asking people their symptoms and trying to figure out if they have COVID or not. Sometimes you’ll say someone has COVID when they don’t, and sometimes you’ll say they don’t when they do. What are these two types of mistakes called? And which one do you think is worse? Why?\n\nThis is a question about Type I and Type II error (also known as false positives and false negatives, respectively). Most candidates realize this right away, and then make an argument for why they think a Type II error (the false negative) is a worse mistake. Generally, the argument centers on someone who is infected unknowingly spreading COVID. That’s a great answer. It shows that they can reason about different types of mistakes and can make an argument for why we might want to minimize one or the other. But this isn’t a particularly challenging question.\nWe ask a follow-up:\n\nNow, let’s imagine you get some COVID test results back from the people whose symptoms you were asking about. What’s wrong with this statement: ‘If COVID tests have a 10% false negative rate and you got a COVID test and it’s negative, that means there’s a 10% chance it’s wrong and you’re actually positive.’?\n\nThis question is a little more challenging, and it builds of the types of error we discussed in the previous question. Here, you need to realize what a false negative is, and it’s easy to get the conditioning flipped. In this case, the false negative rate of the test being 10% means that the probability that you test negative given that you have COVID is 10%. This is not the same as saying that the probability that you have COVID given that you test negative is 10%. In the second case, the conditioning is flipped around backwards. Most candidates get hung up on this, and rightfully so. It’s a tricky question to work out without pen and paper.\nFor those that get through that question, we have another, more difficult follow-up:\n\nOkay, so you got a negative test result, but you know the false negative rate of COVID tests is 10%. Imagine you wanted to calculate the actual chance you were positive given that you just got your negative result. What other information would you need to do your calculation?\n\nFor the Bayesians in the back, this question should be a slam dunk. It’s an obvious Bayes’ Rule question: we’re asking you to calculate p( COVID | negative test ), so you can use Bayes’ Rule to find the answer. It turns out the other information you’d need to do this calculation (in addition to the false negative rate) are a true negative rate and a prior, and you’re golden.\nLastly, once a candidate successfully identified Bayes’ Rule and (hopefully) discussed priors, we’d ask them how they’d elicit this prior. There’s no “right” answer here, but there are a couple options that are better than others:\n\nAsk an expert\nUse something relatively uninformative\nTake an “empirical” approach and use something like the overall positive test rate\n\nAny of these answers would be totally reasonable, given that there’s a whole literature on prior elicitation.\nAnd that’s one example of a multi-layer question we might ask in a data science interview. The vast majority of candidates won’t get through all the parts, and that’s totally fine! Errr, maybe “fine” isn’t actually the right word: That’s the goal. The point is that we’re constructing a question that lets us learn a lot about you: We learn how much you know about Type I / II error and how you reason about them. We learn about if you understand conditional probability and Bayes’ Rule. And we learn how you reason about prior elicitation. We also learn how you argue for decisions you make, and how you communicate complicated statistical concepts (Bayes’ Rule and Type I / II error aren’t simple). And finally, we learn something about the depth of your knowledge. The first part of this question you’d probably know the answer to if you’d taken an introductory statistics or ML class. The second part you’d likely see in a probability course or an inferential statistics course. The third part would also probably come up in one of those two courses, and if not, then certainly in a Bayesian statistics course. And finally, the fourth part you’d likely only see in a very advanced inferential statistics course or a Bayesian course. So not only do we see how you reason about statistics, how much you know, and how you communicate difficult concepts, we also learn something about the types of problems you’ve seen or worked on in the past, whether they be in classes or in a former job or internship. This is a lot of useful information."
  },
  {
    "objectID": "posts/2021-12-22-notes-on-hiring-data-analysts-scientists/index.html#the-take-home",
    "href": "posts/2021-12-22-notes-on-hiring-data-analysts-scientists/index.html#the-take-home",
    "title": "Notes on Hiring Data Analysts + Scientists",
    "section": "The Take Home",
    "text": "The Take Home\nThe other piece to our interview process is a take home project. For data science, we have a take-home that you can find in our CollegeVine DS Hiring Github repo. For data analysts, we ask them to bring in any project they’ve done in the past: Anything from a homework assignment to another company’s take home.\nWe spend about 25-30 minutes going through the take home project, and we’re looking for a few main things. For the most part, candidates are good at describing what they’ve done, so we’re generally trying to dig to figure out why they made the decisions they did. Can they defend them? How did they think through the pros and cons of each decision? What’s their thought process like? The idea here is that everyone in a data-related role will need to make decisions where there’s no clear “right” answer, so we want to see why you chose to make a pie chart instead of a box plot, or chose to use XGBoost instead of logistic regression. Can you talk me through the pros and cons of each option?\nIn the data science take home we give, there are also some tricks we’re trying to test the candidates out on. In an effort to not give away our whole bag of tricks, I invite everyone to give the exercise a shot! I’d even be happy to take a look through your projects to see what you come up with. The short of it is that there are some issues in the data that, if you don’t notice them and do something to fix them, will end up ruining the predictions your model produces. We don’t expect anyone to catch all of these issues in the short amount of time we ask them to spend on the problem, but we hope that they’ll be thoughtful about how they’d solve them once we point them out in the interview.\nFinally, there’s one elephant in the room here that’s important to address: Many people feel negatively about take homes. So do we – they tend to be long and unnecessarily complicated, and often feel like a waste of the candidate’s time. In our view, though, the take home is necessary for one main reason: It lets you familiarize yourself with a data science problem beforehand, so that when you get to the interview we can hit the ground running. In a sense, we’re giving you the rules of the game in advance so we can start playing right away. This lets us avoid any confusion or annoying ambiguity with regards to the types of problems were asking about, and to entirely avoid requiring candidates to reason through questions about entirely hypothetical problems that they’ve never seen before. For these reasons, and also because there’s historically been lots of signal that we’ve gotten from how candidates respond to our questions about the project they bring in, we’ve decided to stick with the take home. In addition, we don’t screen out any candidates on the basis of their take home. We don’t ask them to be submitted in advance, so every candidate who does a take home gets an interview."
  },
  {
    "objectID": "posts/2021-12-22-notes-on-hiring-data-analysts-scientists/index.html#wrapping-up",
    "href": "posts/2021-12-22-notes-on-hiring-data-analysts-scientists/index.html#wrapping-up",
    "title": "Notes on Hiring Data Analysts + Scientists",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nIn a nutshell, that’s our whole data scientific technical interview! We discuss a take home you bring in, and then we talk through some miscellaneous statistics and machine learning questions like the ones we discussed above. In general, we’re not looking for any specific expertise or knowledge – we’re a start up, after all. Instead, we’re testing candidates to see how they reason about problems that are similar to the ones they’ll work on at CollegeVine, and how they explain the ins and outs of different possible solutions to those problems. We’re looking at their intuitions about statistics and machine learning, their ability to think on their feet when faced with questions they don’t immediately know the answer to, and their curiosity and creativity when it comes to solving challenging questions. At the end of the day, it’s these traits, not any hyper-specific technical knowledge, that’ll make for a great CV data team member."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-03-25-a-b-testing-a-primer/index.html",
    "href": "posts/series/a-b-testing/2022-03-25-a-b-testing-a-primer/index.html",
    "title": "A/B Testing: A Primer",
    "section": "",
    "text": "This is the first post in a series I’m planning on writing on A/B testing. In this post, I’ll lay out a top-level overview of what A/B testing is and why companies do it. In future posts, I plan on diving into some common pitfalls, bad habits, and anti-patterns I’ve seen, and the systems we’ve put in place to mitigate them and allow our team to run statistically rigorous, fast A/B tests to make informed product decisions as quickly as possible.\nAt work, we generally try to keep documents like this written at a high level: The objective is for them to be understandable and useful for general audience. That will be the case here too, for the most part."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-03-25-a-b-testing-a-primer/index.html#whats-an-ab-test",
    "href": "posts/series/a-b-testing/2022-03-25-a-b-testing-a-primer/index.html#whats-an-ab-test",
    "title": "A/B Testing: A Primer",
    "section": "What’s an A/B Test?",
    "text": "What’s an A/B Test?\nSo, what is an A/B test, anyways? It’s probably easiest to explain with an example:\nLet’s imagine that I had been tracking the click rate on my blog posts over time. It’s pretty terrible – let’s say that the rate that someone clicks into any particular post from the main menu page is 5%. This means that of all of the views of my blog’s main page, only 5% of those page views actually result in a click on one of my posts. Pretty miserable, right?\nBut today I’m feeling optimistic. Right now, when a user hovers over a post title, it gets underlined in red. “But wait!” I think. What would happen if I made the underline blue instead?\nAnd now, I have an A/B test. In this test, the “A” group (or the “control”) is the current state of the world: The red underline. The “B” group (or the “variant” or “treatment” group) is the proposed change: The blue underline.\nThe basic idea of an A/B test is to run these two versions of my blog side-by-side, measuring the click rate in each version, and seeing which version ends up performing better. If the blue underline version – the variant – ends up increasing the click rate to my blog posts, then the conclusion is that I’d be better off permanently changing the underline to blue."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-03-25-a-b-testing-a-primer/index.html#why-test",
    "href": "posts/series/a-b-testing/2022-03-25-a-b-testing-a-primer/index.html#why-test",
    "title": "A/B Testing: A Primer",
    "section": "Why Test?",
    "text": "Why Test?\nIn my trivial example above, the color of the underline doesn’t seem super consequential (and it’s not). But this isn’t always the case. For instance, Facebook changed their notification icon color from blue to red once upon a time, and the rest was history. Amazon might A/B test a new model for recommending products to users, or Netflix a new model for recommending shows. A company doing lots of email marketing might A/B test different types of ways of addressing their emails (e.g. “Dear Matt” vs. “Hey Matt”), and so, so much more. Changes like these can have enormous business implications, and, as such, A/B testing makes up the backbone of so much of the tech and products we interface with every day. Companies want to maximize their conversion rates, click rates, revenues, etc. and A/B testing is one tool in their tool box for optimizing all of the metrics they care about.\nIf there’s one takeaway here, it’s this: Someone wants to make their product “better” in some sense, and to figure out whether or not a new idea of theirs is actually better than the current state of the world, they test it.\n\nIn statistics world, generally A/B tests boil down to testing “conversion rates” against each other, which usually means that the tests being run are Chi-square tests of independence of the proportions of success across the two groups. If the variant is significantly better than the control, we call the test for the variant and roll it out to 100% of users. You might also use a t-test to (e.g.) test if the variant results in significantly more sessions than the control does, or you might use a time series technique like Bayesian structural time series to do pre/post testing to compare user behavior before and after a treatment was applied. For the curious, Google has published an awesome R package called CausalImpact (and an associated talk and some papers, I believe) on this."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-03-25-a-b-testing-a-primer/index.html#up-next",
    "href": "posts/series/a-b-testing/2022-03-25-a-b-testing-a-primer/index.html#up-next",
    "title": "A/B Testing: A Primer",
    "section": "Up Next…",
    "text": "Up Next…\nAs I mentioned before, the rest of this series of posts will focus, roughly, on the following topics: 1. Okay, so we know what an A/B test is, but how do we actually run one? 2. What are the most common anti-patterns, pitfalls, and bad habits that I’ve seen, and why are they problematic? 3. What are we doing to correct those issues to allow our team to run fast, statistically rigorous A/B tests?"
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-10-running-a-b-tests/index.html",
    "href": "posts/series/a-b-testing/2022-04-10-running-a-b-tests/index.html",
    "title": "Running A/B Tests",
    "section": "",
    "text": "This is the second post in a series on A/B testing. In the last post, I gave a high-level overview of what A/B testing is and why we might do it. This post will go a few steps farther. I’ll discuss how an A/B test is run, what we’re looking for along the way, and what happens when we call it one way or the other. This will set us up for the next post, which will discuss the mechanics of A/B testing."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-10-running-a-b-tests/index.html#how-can-i-run-an-ab-test",
    "href": "posts/series/a-b-testing/2022-04-10-running-a-b-tests/index.html#how-can-i-run-an-ab-test",
    "title": "Running A/B Tests",
    "section": "How Can I Run An A/B Test?",
    "text": "How Can I Run An A/B Test?\nIn the last post, I laid out a hypothetical A/B test where I was considering changing the underline color for links on my blog from red to blue. As a refresher: Blue was the variant (the new proposed color) and red was the control (the current state of the world). We were testing to see if a blue underline would cause significantly more users to click the links to my blog posts. “But,” you ask, “how does the test actually happen?” That’s a great question! But first, a disclaimer: I’m not an engineer, so I can only give a bird’s eye view of how we do it at CollegeVine. I’m sure there are many other solutions used by other companies.\nAt CV, we use a tool called LaunchDarkly for running A/B tests. Essentially, LaunchDarkly lets us set up “feature flags” and show the version of the code that’s “behind” them to only certain users. For example, you might imagine you were rolling out a risky new change, and wanted to QA it first. One way we’ve done this kind of thing at CV is to put the risky change behind a feature flag, and then roll it out to our own team. Then, our team can QA and if anything looks off we can either fix the issues or revert the changes before rolling out to external users.\nA/B testing with LD works similarly. Instead of only showing a new version of the code to internal users, we use a feature flag that shows each version of the code to a certain proportion of users, at random. The idea is to use the feature flag in LD to randomly sample users of our site into either the control group or the variant group. Then we track metrics over time to see if the variant is outperforming the control group."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-10-running-a-b-tests/index.html#my-experiment-is-running.-now-what",
    "href": "posts/series/a-b-testing/2022-04-10-running-a-b-tests/index.html#my-experiment-is-running.-now-what",
    "title": "Running A/B Tests",
    "section": "My Experiment Is Running. Now What?",
    "text": "My Experiment Is Running. Now What?\nBack to our hypothetical experiment on my blog. Now, half of users are seeing red-underlined links, and half are seeing blue underlines, at random. So now, we need a way to track the conversion rate of those links. In step a whole bunch of business intelligence (BI) tools, and other tools that brand themselves as being tools for all different flavors of analytics. At CV, we use a tool called Heap for user analytics (including A/B testing).\nLet’s imagine that my blog were wired up to Heap, and tracking page views on my landing page and clicks of the links on that page to individual posts behind the scenes. In Heap, we could visualize the conversion rate from the landing page to any post in a funnel or in a table, where the conversion rate between the two is the proportion of users who hit the landing page that end up clicking one of the links (“converting”) to a post. We could also view these numbers in a table, where we’d have one cell that has the total number of sessions on the landing page and another cell with the number of sessions on the posts, and the conversion rate is the latter divided by the former (roughly).\nSince we have our feature flag set up to track which users are being placed in each group, we can also group by that “property” in Heap, which lets us separate our analysis into the control and the variant. This means that we can compare the conversion rates for the red underline and the blue underline, which is exactly what we’re trying to do! Generally, we’ll set up a Heap dashboard with the funnel we’re interested in so we can track out metrics over time."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-10-running-a-b-tests/index.html#interpreting-the-metrics",
    "href": "posts/series/a-b-testing/2022-04-10-running-a-b-tests/index.html#interpreting-the-metrics",
    "title": "Running A/B Tests",
    "section": "Interpreting the Metrics",
    "text": "Interpreting the Metrics\nNow that the funnel is set up, you’re watching results stream in. Let’s imagine that at some point in time, each group has 1000 users (i.e. 1000 users have seen the variant and another 1000 have seen the control), and 250 users in the variant group converted while only 200 in the control group did. From there, we can calculate our conversion rates as 25% (variant) and 20% (control). And for the purposes of keeping this post simple, let’s assume that lift is big enough for us (by some definition of “big enough”, which we’ll get to in a later post). In that case, we call our test for the variant. In practice, this means we route all traffic to the variant instead of splitting it 50/50, and then we can remove the feature flag from our code and boom! We now have some cool blue underlines for the links on the blog.\nBut back to the lift being big enough: In practice, is knowing that the variant is performing 25% better than the control enough to call the test for the variant? Making this call in a rigorous, informed way is what the rest of the posts in this series will discuss."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-06-03-docker/docker.html",
    "href": "posts/series/doing-data-science/2023-06-03-docker/docker.html",
    "title": "A Gentle Introduction to Docker",
    "section": "",
    "text": "This post is part of a series called The Missing Semester of Your DS Education."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-06-03-docker/docker.html#introduction",
    "href": "posts/series/doing-data-science/2023-06-03-docker/docker.html#introduction",
    "title": "A Gentle Introduction to Docker",
    "section": "Introduction",
    "text": "Introduction\nIf you’re doing data science work, it’s likely you’ll eventually come across a situation where you need to run your code somewhere else. Whether that “somewhere” is the machine of a teammate, an EC2 box, a pod in a Kubernetes cluster, a runner in your team’s CI/CD rig, on a Spark cluster, and so on depends greatly on the problem you’re solving. But the ultimate point is the same: Eventually, you’ll need to be able to package your code up, put it somewhere in the world other than your local machine, and have it run just like it has been for you."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-06-03-docker/docker.html#enter-docker",
    "href": "posts/series/doing-data-science/2023-06-03-docker/docker.html#enter-docker",
    "title": "A Gentle Introduction to Docker",
    "section": "Enter: Docker",
    "text": "Enter: Docker\nDocker seems very complicated at first glance. And there’s a lot of jargon: Images, containers, volumes, and more, and that doesn’t even begin to cover the world of container orchestration: Docker-Compose, Kubernetes, and so on. But at its core, you can think of Docker as a little environment – not too unlike your local machine – that has a file system, configuration, etc. that’s packaged up into a magical box that you can run on any computer, anywhere. Or at least on computers that have Docker installed.\nIt might be simplest to consider a small example.\n\n\n\n\n\n\nNote that to run the example that will follow, you’ll need to have Docker installed on your machine. All of the files necessary to run this example can be found in my blog’s Github repo\nI’ll use R for the example I provide in this post, but note that the same principles apply if you’re doing your work in Python, or in any other programming language.\n\n\n\nLet’s imagine we want to print “Hello from Docker!” from R. First, make a new directory called docker-example (or whatever you want to call it):\nmkdir docker-example && cd docker-example\nAnd then we might do something like the following:\n## Dockerfile\n\nFROM rocker/r-ver:4.2.0\n\nCMD [\"Rscript\", \"-e\", \"'Hello from Docker!'\"]\nIf you paste that into a file called Dockerfile, you can then run:\ndocker build --tag example .\nWhich will build the Docker image by running each command you’ve specified. Going line by line, those commands are:\n\nUse the rocker/r-ver:4.2.0 image as the base image. In Docker, base images are useful because they come with things (such as the R language) pre-installed, so you don’t need to install them yourself. rocker/r-ver:4.2.0 ships with R version 4.2.0 pre-installed, which means you can run R as you would on your local.\nAfter declaring the base image, we specify a command to run when docker run is invoked. This command is simple – it just prints Hello from Docker!.\n\nOnce the build has completed, you can:\ndocker run example\nand you should see:\n[1] \"Hello from Docker!\"\nTada 🎉! You just ran R in a Docker container. And since you have your code running in Docker, you could now run the same code on any other machine that supports Docker."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-06-03-docker/docker.html#more-complicated-builds",
    "href": "posts/series/doing-data-science/2023-06-03-docker/docker.html#more-complicated-builds",
    "title": "A Gentle Introduction to Docker",
    "section": "More Complicated Builds",
    "text": "More Complicated Builds\nOf course, this example was trivial. In the real world, our projects are much more complex. They have dependencies, they rely on environment variables, they have scripts that need to be run, and so on.\n\nCopying Files\nLet’s start with running a script instead of running R from the command line as we have been.\nCreate an R script called example.R that looks like this:\n## example.R\n\nprint(\"Hello from Docker!\")\nAnd then you can update the Dockerfile by adding a COPY command to copy the script into your image, as follows.\n## Dockerfile\n\nFROM rocker/r-ver:4.2.0\n\nCOPY example.R example.R\n\nCMD [\"Rscript\", \"example.R\"]\nThe COPY command tells Docker that you want to take example.R and put it into your image at /example.R. You can also specify a file path in the image, but I’m just putting the files I copy in at the root.\nFinally, let’s build and run our Docker image again:\ndocker build --tag example .\n\ndocker run example\nAmazing! You can see in the build logs that the example.R script was copied into the image:\n =&gt; [2/3] COPY example.R example.R\nand then running the image gives the same result as before:\n[1] \"Hello from Docker!\"\n\n\nInstalling Dependencies\nYou’ll generally also need to install dependencies, which you can do using the RUN command. Let’s update the Dockerfile to install glue.\n## Dockerfile\n\nFROM rocker/r-ver:4.2.0\n\nCOPY example.R example.R\n\nRUN Rscript -e \"install.packages('glue')\"\n\nCMD [\"Rscript\", \"example.R\"]\nNow, the third step in the build installs glue. And to show it works, we’ll use glue to do a bit of string interpolation, printing the R version that’s running from the R_VERSION environment variable. Update example.R as follows:\n## example.R\n\nlibrary(glue)\n\nprint(glue('Hello from Docker! I am running R version {Sys.getenv(\"R_VERSION\")}'))\nBuilding and running again should give you some new output. First, you should see glue installing in the build logs:\n =&gt; [3/3] RUN Rscript -e \"install.packages('glue')\"\nAnd once you run the image, you should see:\nHello from Docker! I am running R version 4.2.0                                                                  \nWoohoo! 🥳🥳\n\n\nUsing renv\nBut as I wrote about in my last post, having global dependency installs is usually a bad idea. So we probably don’t want to have an install.packages() as a RUN step in the Dockerfile. Instead, let’s use renv to manage our dependencies.\nFrom the command line, run:\nRscript -e \"renv::init()\"\nSince you already are using glue in your project, this will generate a lockfile that looks something like this:\n{\n  \"R\": {\n    \"Version\": \"4.2.0\",\n    \"Repositories\": [\n      {\n        \"Name\": \"CRAN\",\n        \"URL\": \"https://cloud.r-project.org\"\n      }\n    ]\n  },\n  \"Packages\": {\n    \"glue\": {\n      \"Package\": \"glue\",\n      \"Version\": \"1.6.2\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"CRAN\",\n      \"Requirements\": [\n        \"R\",\n        \"methods\"\n      ],\n      \"Hash\": \"4f2596dfb05dac67b9dc558e5c6fba2e\"\n    },\n    \"renv\": {\n      \"Package\": \"renv\",\n      \"Version\": \"0.17.3\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"CRAN\",\n      \"Requirements\": [\n        \"utils\"\n      ],\n      \"Hash\": \"4543b8cd233ae25c6aba8548be9e747e\"\n    }\n  }\n}\n\n\n\n\n\n\nIt’s important to keep the version of R you’re running in your Docker containers in sync with what you have on local. I’m using 4.2.0 in my Docker image, which I defined with FROM rocker/r-ver:4.2.0, and that version is the same version that’s recorded in my renv.lock file. In Python, you might use a tool like pyenv for managing Python versions.\n\n\n\nNow that we have renv set up, we can update the Dockerfile a bit more:\n## Dockerfile\n\nFROM rocker/r-ver:4.2.0\n\nCOPY example.R example.R\nCOPY renv /renv\nCOPY .Rprofile .Rprofile\nCOPY renv.lock renv.lock\n\nRUN Rscript -e \"renv::restore()\"\n\nCMD [\"Rscript\", \"example.R\"]\nNow, we’re copying all of the renv scaffolding into the image. And instead of running install.packages(...), we’ve replaced that line with renv::restore() which will look at the lockfile and install packages as they’re defined. Rebuilding and running the image again will give you the same result as before.\nNow that we have a script running in Docker and are using renv to declare and install dependencies, let’s move on to…\n\n\nEnvironment Variables\nSometimes we need environment variables like a Github token or a database URL, either to install our dependencies or to run our code. Depending on when the variable will be used, we can either specify it at build time (as a build arg) or a run time. Generally, it’s a good idea to only specify build args that you really need at build time.\n\nBuild Time Config\nFor instance, if your build requires downloading a package from a private Github repository (for which you need to have a GITHUB_PAT set), then you would specify your GITHUB_PAT as a build arg. Let’s try that:\n## Dockerfile\n\nFROM rocker/r-ver:4.2.0\n\nARG GITHUB_PAT\n\n## Note: don't actually do this\n## It's just for the sake of example\nRUN echo \"$GITHUB_PAT\"\n\nCOPY example.R example.R\nCOPY renv /renv\nCOPY .Rprofile .Rprofile\nCOPY renv.lock renv.lock\n\nRUN Rscript -e \"renv::restore()\"\n\nCMD [\"Rscript\", \"example.R\"]\nNow, the second line adds a build arg using ARG. Next, run the build:\ndocker build --tag example --build-arg GITHUB_PAT=foobar .\nYou should see the following in the logs:\n =&gt; [2/7] RUN echo \"foobar\" \nThis means that your variable GITHUB_PAT has been successfully set, and can be used at build time for whatever it’s needed for.\n\n\n\n\n\n\nThis is just an example, but it’s important that you don’t expose secrets in your build like I’ve done here. If you’re using (e.g.) a token as a build arg, make sure it’s not printed in plain text to your logs.\n\n\n\n\n\nRuntime Config\nOther times, you want config to be available at container runtime. For instance, if you’re running a web app, you might not need to be able to connect to your production database when you’re building the image that houses your app. But you need to be able to connect when the container boots up. To achieve this, use --env (or --env-file). We’ll update our example.R a bit to show how this works.\n## example.R\n\nlibrary(glue)\n\nprint(glue('Github PAT: {Sys.getenv(\"GITHUB_PAT\")}'))\n\nprint(glue('Database URL: {Sys.getenv(\"DATABASE_URL\")}'))\n\nprint(glue('Hello from Docker! I am running R version {Sys.getenv(\"R_VERSION\")}.'))\nAnd then, let’s rebuild:\ndocker build --tag example --build-arg GITHUB_PAT=foobar .\nand now we’ll run our image, but this time with the --env flag:\ndocker run --env DATABASE_URL=loremipsum example\nThis tells Docker that you want to pass the environment variable DATABASE_URL=loremipsum into the container running your example image when the container boots up.\nAnd after running, you’ll see something like this:\nGithub PAT: \nDatabase URL: loremipsum\nHello from Docker! I am running R version 4.2.0.\nThere are a few things to note here.\n\nThe GITHUB_PAT that you set as a build arg is no longer accessible at runtime. It’s only accessible at build time.\nThe DATABASE_URL we provided with the --env flag is now accessible as an environment variable named DATABASE_URL\n\n\n\n\n\n\n\nVery often, container orchestration platforms like Heroku, Digital Ocean, AWS Batch, etc. will allow you to specify environment variables via their CLI or UI, which they will then inject into your container for you when it boots up."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-06-03-docker/docker.html#advanced-topics",
    "href": "posts/series/doing-data-science/2023-06-03-docker/docker.html#advanced-topics",
    "title": "A Gentle Introduction to Docker",
    "section": "Advanced Topics",
    "text": "Advanced Topics\nThis post is intended to be a gentle introduction to Docker, but there’s a lot that’s missing. I’d like to quickly address a couple more topics that have been helpful to me and our team as we’ve relied more and more heavily on Docker.\n\nCustom Base Images\nYou might have noticed that your builds take longer as they do more. For instance, the glue install, even though glue is extremely lightweight, takes a few seconds. If you have many dependencies (R dependencies, C dependencies, etc.), building an image for your project can get prohibitively slow. For us in the past, builds have taken over an hour just restoring the dependencies recorded in the lockfile.\nA convenient way around this is to install some “base” dependencies that you’ll update rarely and use often into a base image, which you then push to a repository like Docker Hub and then use as your base image in the FROM ... line of your Dockerfile for any particular project. This prevents you from needing to install the same, unchanged dependencies over and over again.\nWe’ve had a lot of success using this strategy on a few particular fronts:\n\nMaking sure we’re using the same version of R everywhere is simple if we define the R version in one place with a FROM rocker/r-ver:4.2.0 in our base image (which is called collegevine/r-prod-base), and then we use FROM collegevine/r-prod-base as the base image for all of our other Docker builds.\nInstalling Linux dependencies, such as curl, unzip, etc. which we’re happy keeping on a single version can happen once in the base image, and then every downstream image can rely on those same dependencies.\nInstalling CLIs like the AWS CLI, which again, really doesn’t need to happen on every build.\n\n\n\nCI / CD\nThe other time-saving strategy we’ve greatly benefited from is aggressive caching R packages in our CI / CD process. renv has great docs on using it within a CI / CD rig which I would highly recommend.\nAt a high level, what we do is renv::restore() in the CI itself (before running the docker build ...), which installs all of the packages our project needs. Then we COPY the cache of packages into our image, so that they’re available inside of the image. This means we don’t need to reinstall every dependency on every build, and has probably sped up our image build times by 100x."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-06-03-docker/docker.html#wrapping-up",
    "href": "posts/series/doing-data-science/2023-06-03-docker/docker.html#wrapping-up",
    "title": "A Gentle Introduction to Docker",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nI hope this post has demystified Docker a bit and helped clarify some of the basics of Docker and how it’s used. At the highest level, Docker lets you package up your code so that it can be run anywhere, whether that’s on your machine, on the machine of a coworker, in a CI/CD tool, on a cloud server like an EC2 box, or anywhere else. All you need to do is build and push your image!"
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-06-workflow-orchestration/workflow-orchestration.html",
    "href": "posts/series/doing-data-science/2023-05-06-workflow-orchestration/workflow-orchestration.html",
    "title": "Workflow Orchestration",
    "section": "",
    "text": "This post is part of a series called The Missing Semester of Your DS Education."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-06-workflow-orchestration/workflow-orchestration.html#introduction",
    "href": "posts/series/doing-data-science/2023-05-06-workflow-orchestration/workflow-orchestration.html#introduction",
    "title": "Workflow Orchestration",
    "section": "Introduction",
    "text": "Introduction\nWhen I first started my current job, I installed an app called Amphetamine on my machine. As an editorial sidebar: I’d highly recommend Amphetamine. It’s a great app.\nBut anyways, the point is that the reason I had installed Amphetamine was to keep my machine alive at night as I was running some code to train a model or do some analysis that took seven or eight hours locally. My strategy – which I thought was the norm at the time, and was a habit I had brought over from my previous data science role – was to kick off a job manually, tell Amphetamine to keep my machine awake, plug it in, turn the brightness down, and go to sleep. In the morning, I could wake up and see my results."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-06-workflow-orchestration/workflow-orchestration.html#headaches",
    "href": "posts/series/doing-data-science/2023-05-06-workflow-orchestration/workflow-orchestration.html#headaches",
    "title": "Workflow Orchestration",
    "section": "Headaches",
    "text": "Headaches\nI imagine that this pattern is fairly common for data scientists. I thought it was totally normal, and it worked well enough (aside from the fact that my machine’s battery was crying out for help, but I digress). Over the past few years, I’ve learned just how much of an anti-pattern this was in reality. There were a number of painful aspects to this strategy that I had cooked up, all of which took me far too long to recognize as being legitimate:\n\nRunning a job on local at night means that there’s no audit trail of past job runs, and no schedule for future ones. Everything is ad-hoc and “memoryless” in some sense. Without the help of other tooling, it’s hard to remember what happened before and what jobs have run and which are yet to be run.\nIf something went wrong, I was the only one to know about it. Since the problem – error or otherwise – would only show up on my machine, none of my teammates would have any idea that anything had broken.\nIf a product (a model, a dashboard requiring some data wrangling to happen, etc.) needed to be updated for a production setting, the only way to go about that was for me to put down my work, open my code to do the updates, and kick off the job. Unfortunately, these jobs would often eat lots of my machine’s compute resources, leaving me unproductive for the remainder of the day while something ran.\nWhat I could build and run was severely constrained by the compute resources of my machine, which is pretty beefy but not invincible."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-06-workflow-orchestration/workflow-orchestration.html#workflow-orchestration-and-mlops",
    "href": "posts/series/doing-data-science/2023-05-06-workflow-orchestration/workflow-orchestration.html#workflow-orchestration-and-mlops",
    "title": "Workflow Orchestration",
    "section": "Workflow Orchestration and MLOps",
    "text": "Workflow Orchestration and MLOps\nAs it turns out, there are many, many tools that provide the proverbial ibuprofen for these headaches, if you will. Broadly speaking, they fall into two often-overlapping categories: Workflow orchestration tools and MLOps tools. This post will cover workflow orchestration, since workflow orchestration tools are a core part of most every data stack and are more common than MLOps tools, as they’re used for data science, data engineering, and more.\nThere are lots of workflow orchestrators on the market, and there’s a wide range of options in terms of capabilities they provide, whether they’re self-hosted, open-source tools or managed services, and much more. A few popular names in the space are Airflow, Luigi, Dagster, Argo, and Prefect. These are all fantastic tools that come with their own pros and cons, but at their core they all seek to achieve a similar goal: Letting you and your team run jobs in a sane way.\nThe long and short of workflow orchestration tools is that they provide tooling to help you run code – often in different languages and performing disparate tasks – in any way you want, by providing high-level APIs for triggering your jobs. For instance, we often run R code using Docker containers running in individual pods on our Kubernetes cluster, while we might trigger a bash task to just run on the leader node of our cluster without spinning up a new container. Even if that was a lot of technical jargon, the main takeaway is simple: Workflow orchestration tools let you run your code in many ways and in many places, which is incredibly powerful. You can run jobs locally (on your tool of choice’s compute) or you can farm them off to an external source of compute (like Batch or Lambda, and many, many more) to have them run in a galaxy far, far away. The workflow orchestrator will handle the triggering of the job, the monitoring of the job and listening for it to complete successfully or with errors, and will handle the alerting, deciding whether or not to continue running the next steps in the job based on what happened in the previous ones, and so on. All of these tools are highly configurable to your needs."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-06-workflow-orchestration/workflow-orchestration.html#airflow",
    "href": "posts/series/doing-data-science/2023-05-06-workflow-orchestration/workflow-orchestration.html#airflow",
    "title": "Workflow Orchestration",
    "section": "Airflow",
    "text": "Airflow\nOur team uses Airflow (Astronomer in particular, which has a very helpful team and does a great job of managing complicated things like Kubernetes and authentication for us), so that’s what I’ll discuss here. And this time, it’ll be concise.\nAirflow solves a few key problems for us:\n\nIt lets us run jobs on schedules that we define in the code. We can define any schedule we want, and our jobs will magically be run on time, in the cloud. In addition, if we want to manually trigger a job, all we have to do is press the play button.\nIt lets us modularize our jobs, so that if a step fails, we can retry or restart from the failure as opposed to from the start, which often saves lots of time.\nIt provides great visibility into the jobs themselves. In particular, we know when they fail, we know how long steps are taking to complete, and so on. Airflow makes it easy to track and audit what’s happening inside of our jobs.\nIt lets us trigger jobs in many places, such as running retraining jobs for our ML models on AWS Batch. Batch lets us spin up a machine that matches some compute requirements we have, run whatever code we need to on it, and shut down once it finishes. This is hugely valuable, since some of our jobs are memory heavy, and others require lots of cores to parallelize across, and so on. Airflow’s BatchOperator lets us configure the compute resources we need for each individual step (task) in our job, which is extremely flexible.\nAnd much, much more.\n\nNote that these points are solutions to the headaches listed above. In particular:\n\nAirflow lets us track past runs of jobs, so it’s easy to audit the history of any particular job and schedule future jobs as we please.\nAirflow can send notifications of job failures to (e.g.) Slack, which lets our team know that something is broken and starts our triage process.\nAirflow lets us very easily run our jobs on third-party (AWS, in our case) compute, which results in something of a “set it and forget it” process for running jobs: We press play, and wait for the job to finish. And in the meantime, we continue doing whatever else we were working on with minimal disruption.\nSince Airflow lets us easily run jobs on AWS compute and we can dynamically set compute requirements, we can spin up a specific machine – an EC2 box in our case – that’s well-suited to the needs of our jobs. We have some memory intensive jobs that we run on big R4 instances, which provide lots of RAM. You might also need a GPU for a job that trains a deep learning model, in which case you’d configure your compute requirements to include a GPU, and Batch could spin up a P3 instance (with a GPU). Instead of being limited by the compute resources of our local machines, we now have easy access to the entire AWS fleet of EC2 instance types."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-06-workflow-orchestration/workflow-orchestration.html#wrapping-up",
    "href": "posts/series/doing-data-science/2023-05-06-workflow-orchestration/workflow-orchestration.html#wrapping-up",
    "title": "Workflow Orchestration",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nThere was admittedly a lot of technical jargon in this post. But the main takeaway is this: Having a true workflow orchestration tool like Airflow makes it simple for you to run your analytical code (or any other code!) in a sane way. Workflow orchestrators help you run your code on certain schedules, provide lots of visibility into individual runs of your code, help you farm off your code to third-party compute, alert you when things go wrong, and so much more. So please, please shut off your machine at night instead of training your models on it. If you can, you should use a workflow orchestrator. It’s good for your machine’s health, your team’s productivity, and your sanity."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-06-20-how-can-others-use-my-model/how-can-others-use-my-model.html",
    "href": "posts/series/doing-data-science/2023-06-20-how-can-others-use-my-model/how-can-others-use-my-model.html",
    "title": "How Can Someone Else Use My Model?",
    "section": "",
    "text": "This post is part of a series called The Missing Semester of Your DS Education."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-06-20-how-can-others-use-my-model/how-can-others-use-my-model.html#introduction",
    "href": "posts/series/doing-data-science/2023-06-20-how-can-others-use-my-model/how-can-others-use-my-model.html#introduction",
    "title": "How Can Someone Else Use My Model?",
    "section": "Introduction",
    "text": "Introduction\nAt this point in this series, I’ve discussed a lot of aspects of putting machine learning into production. I’ve gone over workflow orchestration for retraining and monitoring models, unit testing for ensuring that your code is correct, experiment tracking and model versioning for keeping track of what’s actually running in production at any given time, and dependency management and Docker for packaging up your analytical code.\nThe last of the nuts and bolts that I’ve yet to go through is how other people - teammates, clients, users, etc. - will actually use your model."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-06-20-how-can-others-use-my-model/how-can-others-use-my-model.html#types-of-inference",
    "href": "posts/series/doing-data-science/2023-06-20-how-can-others-use-my-model/how-can-others-use-my-model.html#types-of-inference",
    "title": "How Can Someone Else Use My Model?",
    "section": "Types of Inference",
    "text": "Types of Inference\nIn very broad strokes, there are two types of “inference” (read: ways of making predictions) that you’ll encounter: Online and batch. In short, “online” means that your model is making predictions in real-time and serving those predictions to whoever the user of your model is. For instance, if you have a widget on your website that lets a student increase or decrease their GPA and see how their chances of admission to Carleton College change, then the process of them sliding the slider, the new GPA being sent to your model, and your model returning a new prediction of that student’s chances is an example of online inference.\nBatch inference is what you might think of as “offline” inference: You make predictions for many people, items, etc. at one time and store them somewhere to be accessed by users. For instance, if you had individual pages with information on any given school and wanted to show a few recommended schools in a section entitled “Students who like X school also like” you might be okay with only recomputing those most similar schools once per day, or once per week, or so on. So in that case, you’d have some job - orchestrated by your workflow orchestration tool of choice - that would create the lists of similar schools for each school, and then would store those lists somewhere for them to be ingested and eventually used on your school pages. Then, once the lists are ingested, the recommendations are static until the job next runs to update them again.\nThis post will cover online inference, and a common way that a consumer of your model’s predictions might communicate with that model: A REST API."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-06-20-how-can-others-use-my-model/how-can-others-use-my-model.html#apis-and-rest",
    "href": "posts/series/doing-data-science/2023-06-20-how-can-others-use-my-model/how-can-others-use-my-model.html#apis-and-rest",
    "title": "How Can Someone Else Use My Model?",
    "section": "APIs and REST",
    "text": "APIs and REST\nAn Application Programming Interface (API) is a fancy acronym for something that I might best describe as an agreement between you and somebody else about what they provide and how you will use it. For instance, you might have used a public API like that of the Census for programmatically pulling population statistics, income statistics, and so on, or Twitter for doing the same for tweets.\nThe basic principle is that an API lets you interact with something, such as data (in the case of Twitter and the Census) or a machine learning model, as long as you comply with the API’s “contract.” If a model lives behind an API, you can make a “request” to that API, and assuming that you’ve correctly provided all the necessary fields, added the correct request headers, etc., the API will hold up its end of the bargain and return the prediction you asked for.\nVery often, you’ll hear talk of REST or “RESTful” APIs. At their core, a REST API is just an API that follows REST standards, including having a uniform interface, being stateless, and so on. I’m no expert on REST, so I’ll defer to a great explainer by the AWS team for more details."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-06-20-how-can-others-use-my-model/how-can-others-use-my-model.html#plumber",
    "href": "posts/series/doing-data-science/2023-06-20-how-can-others-use-my-model/how-can-others-use-my-model.html#plumber",
    "title": "How Can Someone Else Use My Model?",
    "section": "Plumber",
    "text": "Plumber\n\n\n\n\n\n\nThe examples in the remainder of this post will use Plumber to demonstrate a simple REST API.\nUnfortunately, I cannot in good faith recommend using Plumber or R itself in any sort of high-load or high-throughput production system for reasons that are beyond the scope of this post, and would strongly encourage considering a Python framework like FastAPI instead.\n\n\n\nIn R, the Plumber package provides a friendly set of tools for building and deploying APIs of your own. All you need in the most simple case is an R script like this:\nlibrary(plumber)\n\npr() %&gt;%\n  pr_get(\n    \"/hello\",\n    function() \"Hello world!\",\n    serializer = serializer_unboxed_json()\n  ) %&gt;%\n  pr_run(port = 9807)\nRunning that R code will open up a Swagger UI that shows the endpoint you defined at /hello that will look like this:\n\n\n\n\n\nYou can use the Swagger UI to make a request to your API and see the result, which is the Hello world! that we expected:\n\n\n\n\n\nYou can also use curl (or httr or similar) to make requests to your API from the command line, from R, or from any other language of your choosing as follows:\ncurl http://127.0.0.1:8080/hello\nThis request will return:\n\"Hello world!\""
  },
  {
    "objectID": "posts/series/doing-data-science/2023-06-20-how-can-others-use-my-model/how-can-others-use-my-model.html#a-more-involved-example",
    "href": "posts/series/doing-data-science/2023-06-20-how-can-others-use-my-model/how-can-others-use-my-model.html#a-more-involved-example",
    "title": "How Can Someone Else Use My Model?",
    "section": "A More Involved Example",
    "text": "A More Involved Example\nNow that we can successfully talk to our API, let’s imagine we wanted to make predictions from a model that predicts someone’s weight given their height. For the purposes of this example, our “model” is just going to take the height and multiply it by pi, which will return a terrible prediction, but a prediction nonetheless. This is actually an important note about APIs: This trivial example - multiplying by pi - is actually the whole point of having an API. From the consumer of an API’s point of view, whatever is happening behind the scenes is just that: Behind the scenes. The client needs no knowledge of how your API is actually going about figuring out what it should be returning to you. All the client knows is that there’s a contract: If they give you something, you process that thing and give them back what they asked for.\nWith this in mind, let’s add a /predict endpoint to our service above to predict someone’s weight based on their height.\nlibrary(plumber)\n\npr() %&gt;%\n  pr_get(\n    \"/hello\",\n    function() \"Hello world!\",\n    serializer = serializer_unboxed_json()\n  ) %&gt;%\n  pr_get(\n    \"/predict\",\n    function(height) as.numeric(height) * pi,\n    serializer = serializer_unboxed_json()\n  ) %&gt;%\n  pr_run(port = 9807)\nNow we can call our /predict endpoint with a query parameter height indicating the height of the person. Let’s call our prediction endpoint with a height of 5:\ncurl \"http://127.0.0.1:9807/predict?height=5\"\n\n15.708\nGreat! It gives back 15.708, which is indeed equal to 5 * pi.\nAgain, the actual computation here isn’t really the point. Multiplying the height by pi is indeed a terrible way to predict someone’s weight, but the key takeaway is that from the client’s point of view, the computation is a black box. As long as you don’t make any changes to how the client is supposed to interact with your API - these types of changes are called “breaking” changes, and you should be very, very careful making them - you can arbitrarily change the guts of your API and have the client be none the wiser. This means that you could multiply the height by e instead of by pi, or you could swap in a machine learning model and call its predict method from inside of the endpoint. For instance, you could do something like this:\nlibrary(plumber)\n\n## NOTE: \n## This assumes you have a model whose\n## predict method takes a single-column\n## dataframe (with `height` as the column).\n## living at `path/to/model.rds`\nmodel &lt;- readRDS(\"path/to/model.rds\")\n\npr() %&gt;%\n  pr_get(\n    \"/hello\",\n    function() \"Hello world!\",\n    serializer = serializer_unboxed_json()\n  ) %&gt;%\n  pr_get(\n    \"/predict\",\n    function(height) predict(model, newdata = data.frame(height = as.numeric(height))),\n    serializer = serializer_unboxed_json()\n  ) %&gt;%\n  pr_run(port = 9807)\nIn this example, we first load our model into memory at API boot time, and then it’s accessible to our /predict endpoint whenever it’s called by clients. You can extend this basic setup to be arbitrarily complex: Many models, many endpoints, helper code, tests, etc. but the basic premise is always the same: Declare a way for an outside client to use your model by providing an API contract, and then respond to requests for predictions from your model based on whatever inputs you need by taking those inputs, doing any data munging or feature engineering you need to do, running the prediction, doing post-processing or anything else, and returning the response to the client."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-06-20-how-can-others-use-my-model/how-can-others-use-my-model.html#deployment",
    "href": "posts/series/doing-data-science/2023-06-20-how-can-others-use-my-model/how-can-others-use-my-model.html#deployment",
    "title": "How Can Someone Else Use My Model?",
    "section": "Deployment",
    "text": "Deployment\n“But wait!” you think. “pr_run on my local machine is only accessible via localhost. What about if someone outside of my machine wants to access my model?”\nGreat question! You need to deploy your API somewhere outside of your local machine and accessible over the internet so that clients can use your model. There are a number of ways to achieve this, and discussing individual hosting platforms and how to get your code running on them is a bit of a rabbit hole beyond the scope of this post. There are a number of ways of deploying your code into production systems. One thing you can do is wrap up your API in Docker, and then ship it basically anywhere that runs Docker containers (read: basically anywhere). But how you go about shipping your code to production is ultimately a decision that needs to be made based on your team and the problem you’re solving, among other factors. There’s certainly no single right way to go about it.\n\n\n\n\n\n\nFor a bit more on this, I wrote about Docker in my last post, and Plumber has great documentation on how to use Docker to deploy a Plumber API.\n\n\n\nIf you do ultimately decide to use Docker, you’ll have a solution that works roughly the same whether you’re working in Plumber, Flask, FastAPI, or another framework. You’d build your API, package it up in a Docker image, and ship that image to some place out in the world that can run said image for you. Then, once it’s there, you can call it just as you would any public-facing API!"
  },
  {
    "objectID": "posts/series/doing-data-science/2023-06-20-how-can-others-use-my-model/how-can-others-use-my-model.html#wrapping-up",
    "href": "posts/series/doing-data-science/2023-06-20-how-can-others-use-my-model/how-can-others-use-my-model.html#wrapping-up",
    "title": "How Can Someone Else Use My Model?",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nThis post was the last nuts and bolts, mechanical post in this series about doing data science. And it felt like a good place to wrap up: Going over how models actually get used by consumers in the real world at a very high level.\nThe deployment, monitoring, and performance of APIs in production systems has been written about and done over and over again by lots of people far more qualified than I am to educate others on it. Notably, virtually every software developer working on the back end of any website or similar service has worked in production systems that are far more complicated than anything I’ve worked in, so I’ll defer lots of the technical details and best practices to them in favor of giving a hopefully helpful, minimal example here about how to achieve a similar goal, but for serving predictions from machine learning models.\nAt this point, I’ve discussed most of the end-to-end process that we follow for shipping ML products: Starting with the basics of how we structure, test, and review code, to how we run that code in Airflow to train and monitor models, to how we use Docker to deploy APIs in production. There’s lots I’ve missed and didn’t cover, but my hope was that by this point, I’d have achieved one of my main goals in writing this series: To give aspiring data scientists a brief, gentle introduction to some of the tools and processes that we lean on to operationalize our data science work."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-01-library-code/library-code.html",
    "href": "posts/series/doing-data-science/2023-04-01-library-code/library-code.html",
    "title": "Writing Internal Libraries for Analytics Work",
    "section": "",
    "text": "This post is part of a series called The Missing Semester of Your DS Education."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-01-library-code/library-code.html#introduction",
    "href": "posts/series/doing-data-science/2023-04-01-library-code/library-code.html#introduction",
    "title": "Writing Internal Libraries for Analytics Work",
    "section": "Introduction",
    "text": "Introduction\nBy definition, library code is code that’s written to being reused by programs other than itself that are unrelated to each other. For instance, dplyr (R) and pandas (Python) are common examples of library code: Instead of writing code from scratch to work with tabular data, you might use one of those two fantastic libraries. And you get some additional benefits from using them:\n\nThose libraries are well-documented, so it’s easy to figure out how to use them.\nThey’re well-tested, so you (presumably) know that bugs are less likely than if you were to try to write the same functionality from scratch.\nThey’re (relatively) performant.\nThey’re widely used, so it’s easy to find answers to questions and get help from the communities using them."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-01-library-code/library-code.html#a-common-library",
    "href": "posts/series/doing-data-science/2023-04-01-library-code/library-code.html#a-common-library",
    "title": "Writing Internal Libraries for Analytics Work",
    "section": "A Common Library",
    "text": "A Common Library\nAt CollegeVine, we have collegeviner: An R package containing a lot of code that we use very often for all kinds of analytics projects. Some things that live in collegeviner include:\n\nPlot theming code, so that we can consistently theme graphics across all of our work.\nA custom implementation of item-based collaborative filtering, which is core to our school recommendation system.\nDBI and aws.s3 wrappers for connecting to and querying our databases and working with data in S3.\nHelper methods for common math we do, such as converting between odds, log-odds, and probabilities.\nMiscellaneous helper code for things that don’t exist natively in R, such as yeet for removing an item from a list and %notin%, the inverse of the %in% operator.\nAn implementation of the Brier Skill Score, which is a metric we often use for evaluating classification models.\nA lot more!\n\nYou might think of collegeviner as being a common library of things that our team does often, so we don’t need to repeat ourselves or reinvent the wheel.\n\nA Toy Example\nLet’s imagine that you’re setting up a common library (in this example, an R package) for your team. The first thing you might want to do is have some logic to help your team connect to your data warehouse. For this example, let’s just imagine that “warehouse” is your local Postgres instance. Then, you might write a method for your library called connect_to_dwh like this:\n\nlibrary(DBI)\nlibrary(rlang)\nlibrary(httr)\nlibrary(RPostgres)\n\nconnect_to_dwh &lt;- function(url = Sys.getenv(\"DATA_WAREHOUSE_URL\")) {\n  if (url == \"\") abort(\"You must specify a dwh URL.\")\n\n  check_installed(pkg = \"httr\")\n  check_installed(pkg = \"RPostgres\")\n\n  db_params &lt;- parse_url(url)\n\n  db_drv &lt;- Postgres()\n  db_user &lt;- db_params$username\n  db_password &lt;- db_params$password\n  db_host &lt;- db_params$hostname\n  db_port &lt;- db_params$port %||% 5432\n  db_name &lt;- db_params$path\n\n  dbConnect(\n    db_drv,\n    dbname = db_name,\n    host = db_host,\n    port = db_port,\n    user = db_user,\n    password = db_password\n  )\n}\n\nNow you have a single function that your whole team can share to connect to your data warehouse, assuming that they can provide the connection string. Let’s test out how the workflow might look for querying data now.\n\nConnecting\n\nSys.setenv(DATA_WAREHOUSE_URL = \"postgresql://localhost:5432/postgres\")\n\nconn &lt;- connect_to_dwh()\n\nAnd that’s it! You’re connected. You can now query away using dbGetQuery(), a custom wrapper, dbplyr, or any other method of choice.\n\n\nQuerying\n\n## Put some data into the warehouse for example purposes\ndbWriteTable(conn, Id(schema = \"blog\", table = \"iris\"), janitor::clean_names(iris))\n\nresult &lt;- dbGetQuery(\n  conn = conn,\n  \"\n  SELECT species, MAX(sepal_length) AS max_sepal_length\n  FROM blog.iris\n  GROUP BY species\n  ORDER BY 2 DESC\n  \"\n)\n\npretty_print(result)\n\n\n\n\nspecies\nmax_sepal_length\n\n\n\n\nvirginica\n7.9\n\n\nversicolor\n7.0\n\n\nsetosa\n5.8\n\n\n\n\n\n\n\n\n\nTesting\nIt’s also important to test your code. testthat makes writing unit tests for your new connect_to_dwh function very simple.\n\nlibrary(testthat)\n\ntest_that(\"Connecting works as expected\", {\n  ## This should error because the URL is empty\n  expect_error(\n    connect_to_dwh(\"\"),\n    \"You must specify a dwh URL\"\n  )\n  \n  conn &lt;- connect_to_dwh()\n\n  ## Should return a PqConnection object\n  expect_s4_class(conn, \"PqConnection\")\n  \n  ## Should be able to query an example table\n  expect_equal(\n    dbGetQuery(conn, \"SELECT COUNT(*) FROM blog.iris\")$count,\n    150\n  )\n})\n\nTest passed 🎉\n\n\n\n\nVersioning\nLastly, it’s important to version your code. Semantic Versioning (SemVer) is a very common standard for versioning library code. In R specifically, you can read about versioning in Chapter 22 of R Packages.\nIn our toy example, this means that if you change how the logic of your connect_to_dwh function works, you should change the version of your package so that your users (your teammates) don’t get blindsided by your change. Incrementing your package’s version shows your teammates that something has changed in your library, and they can update their code to rely on the latest version (if they wish), or continue using the current version they’re on, or anything else.\n\n\n\n\n\n\nNote that being able to control which version of a library your code is using requires some manner of managing dependencies. In R, I would highly recommend renv. In Python, I like Poetry."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-01-library-code/library-code.html#one-library-per-model",
    "href": "posts/series/doing-data-science/2023-04-01-library-code/library-code.html#one-library-per-model",
    "title": "Writing Internal Libraries for Analytics Work",
    "section": "One Library Per Model",
    "text": "One Library Per Model\nIn addition to a common library for sharing code that’s very often used across the data org, our team has also gotten into the habit of having a library per ML model in production. This definition can be a bit flexible (both in terms of what “ML model” means, and also what “production” means), but the basic principle should be the same: ML in production requires at least some training logic and some monitoring logic. It’s a good idea to share code between those two things. Let’s consider another simple example.\n\nIris Species Prediction\nLet’s imagine that we work for a florist. On our website, we have a service where someone can provide some measurements about an iris (either a setosa or a virginica), as we’ll tell them which of the two we think it is. We know we’ll want to retrain the model periodically as we get more data, and we’ll also want to monitor how our model performs out-of-sample. Both training the model and monitoring will require some shared logic: loading raw data, doing feature engineering, and making predictions. So it would make sense to have those two jobs rely on a single library, as opposed to needing to repeat the logic. Let’s write that library here.\n\nFetching the Data\nFirst, let’s write a method to fetch the raw data from our data warehouse. In practice, it probably makes sense to factor out the SQL here into individual SQL scripts, but for this example I’ll just include the SQL directly as a string.\n\nfetch_raw_data &lt;- function(conn) {\n  dbGetQuery(\n    conn = conn,\n    \"\n    SELECT *\n    FROM blog.iris\n    WHERE species IN ('setosa', 'virginica')\n    \"\n  )\n}\n\n\n\nFeature Engineering\nNext, let’s write some methods to create features.\n\ncreate_sepal_length_feature &lt;- function(sepal_length) {\n  sepal_length + rnorm(n = length(sepal_length))\n}\n\ncreate_petal_width_feature &lt;- function(petal_width) {\n  petal_width + rgamma(n = length(petal_width), shape = 2)\n}\n\nAnd then we can write a function to take our raw data, run the feature engineering steps, and return the features.\n\ncreate_features &lt;- function(raw_data) {\n  sepal_length &lt;- create_sepal_length_feature(raw_data$sepal_length)\n  petal_width &lt;- create_petal_width_feature(raw_data$petal_width)\n  is_setosa &lt;- raw_data$species == \"setosa\"\n  \n  data.frame(\n    sepal_length,\n    petal_width,\n    is_setosa\n  )\n}\n\n\n\nModel Fitting and Prediction\nNext, let’s write methods to fit the model and make predictions.\n\nfit_model &lt;- function(features) {\n  formula &lt;- is_setosa ~ sepal_length + petal_width\n\n  ## Dynamically extract the variables in the formula\n  ## so we don't need to repeat ourselves\n  predictors &lt;- as.character(labels(terms(formula)))\n  target &lt;- as.character(formula[[2]])\n  \n  ## Very basic error handling\n  if (!all(c(predictors, target) %in% colnames(features))) {\n    abort(\"Some required columns were missing from `features`\")\n  }\n  \n  model &lt;- glm(\n    formula,\n    data = features,\n    family = binomial\n  )\n  \n  class(model) &lt;- c(\"irises\", class(model))\n  \n  model\n}\n\npredict.irises &lt;- function(object, newdata = NULL, ...) {\n  probs &lt;- predict.glm(\n    object,\n    newdata,\n    type = \"response\"\n  )\n  \n  ## If the predicted probability is &gt; 50%,\n  ## return `true`, else return `false`\n  probs &gt; 0.50\n}\n\n\n\nModel Evaluation\nAnd finally, let’s add a function to compute the model’s accuracy on some evaluation data.\n\ncompute_accuracy &lt;- function(prediction, is_setosa) {\n  100 * sum(prediction == is_setosa) / length(prediction)\n}\n\n\n\nTesting\nIt’s important to note that all of the methods above can and should be unit tested in the same way we tested our helper for connecting to the database. Testing is a great way to ensure the correctness of your code and make it more maintainable by making it easier to refactor in the future, and putting all of your modeling logic into a library like this makes it very easy to test. For instance, here’s how you might write a couple of unit tests for the petal width feature.\n\ntest_that(\"Petal width feature is created correctly\", {\n  ## The feature should be positive even when the\n  ## petal width is zero, since we're adding gamma\n  ## random noise.\n  expect_gt(create_petal_width_feature(0), 0)\n  \n  ## It should be extremely unlikely that a single \n  ## draw from a gamma(2) is &gt;10, which means this\n  ## feature should be &lt; 10 when the input is 0 in \n  ## the vast majority of cases.\n  ##\n  ## NOTE: This is by definition a brittle test, and\n  ## I wouldn't recommend writing tests that are\n  ## probabilistic like this in practice unless\n  ## you really need to. If you do, this will\n  ## fail _some_ of the time, at random, even\n  ## if \"some\" is a very small percentage.\n  purrr::walk(\n    rep(0, 100), \n    function(x) {\n      expect_lt(\n        create_petal_width_feature(x),\n        10\n      )\n    } \n  )\n})\n\nTest passed 🥇\n\n\n\n\n\nA Retraining Job\nGreat! Now that we have all of that library code written, we can package it up into a retraining job. A very simple training job might look like this:\nFirst, connect to the data warehouse\n\nconn &lt;- connect_to_dwh()\n\nNext, fetch the raw data from the warehouse that we need to train the model.\n\nraw &lt;- fetch_raw_data(conn)\n\npretty_print(head(raw))\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n\n\n\n\n\nNext, create the features from the raw data by running the feature engineering pipeline.\n\nfeatures &lt;- create_features(raw)\n\npretty_print(head(features))\n\n\n\n\nsepal_length\npetal_width\nis_setosa\n\n\n\n\n3.418\n2.724\nTRUE\n\n\n5.585\n2.579\nTRUE\n\n\n4.362\n0.979\nTRUE\n\n\n4.587\n1.677\nTRUE\n\n\n3.476\n2.926\nTRUE\n\n\n4.282\n2.732\nTRUE\n\n\n\n\n\n\n\nThen fit a model using the features.\n\nmodel &lt;- fit_model(features)\n\npretty_print(coef(model))\n\n\n\n\n\nx\n\n\n\n\n(Intercept)\n11.167\n\n\nsepal_length\n-1.564\n\n\npetal_width\n-0.674\n\n\n\n\n\n\n\nFinally, evaluate the performance of the model by making predictions and computing the accuracy of those predictions.\n\npreds &lt;- predict(model)\naccuracy &lt;- compute_accuracy(preds, features$is_setosa)\n\ncat(paste0(\"Model accuracy is \", accuracy, \"%\"))\n\nModel accuracy is 87%\n\n\nAnd that’s it – you have a simple retraining job. This is a very minimal example, but this general framework is very flexible and modular, and it makes up the foundation for how we write our retraining jobs at CollegeVine. You can plug and play all different kinds of feature engineering logic, logic to fetch raw data, metrics, etc. We also use MLFlow for versioning models and tracking experiments, so our retraining jobs have lots of logging of artifacts, parameters, and metrics to our MLFlow instance.\n\n\nA Monitoring Job\nNext, let’s imagine we want to monitor out-of-sample performance of the model. Let’s modify the table with our raw data in the database for this, just for the sake of an example.\n\ntransaction_result &lt;- dbWithTransaction(\n  conn = conn,\n  {\n    dbExecute(\n      conn = conn,\n      \"\n      ALTER TABLE blog.iris\n      ADD COLUMN created_at TIMESTAMP\n      \"\n    )\n    \n    dbExecute(\n      conn = conn,\n      \"\n      UPDATE blog.iris\n      SET created_at = NOW() - random() * INTERVAL '2 weeks'\n      \"\n    )\n  }\n)\n\nGreat, and now let’s make one or two small modifications to our code from above that pulled the raw data from the data warehouse.\n\nfetch_raw_data &lt;- function(conn, created_after = '1970-01-01 00:00:00') {\n  dbGetQuery(\n    conn = conn,\n    sprintf(\n      \"\n      SELECT *\n      FROM blog.iris\n      WHERE \n        species IN ('setosa', 'virginica')\n        AND created_at &gt; '%s'\n      \",\n      created_after\n    )\n  )\n}\n\nAll we’ve done here is added the ability to specify a created_at date to use as the cutoff point, where we’d only include records in our raw data that were created after said point. In practice, this lets us filter our raw data down to only records that were created after the model was trained (the out-of-sample data).\n\n## In practice, this would be set at training time and \"frozen\"\n## possibly by logging the value as a parameter in the MLFlow run\nmodel_trained_at &lt;- median(fetch_raw_data(conn)$created_at)\n\nAnd now that we’ve artificially created a trained_at date for the model, we can run our monitoring job. It’s quite simple, and very similar to the retraining job. All we do here is pull raw data that has been created since the model was trained, run the feature engineering pipeline, make predictions, and compute the accuracy of the model out-of-sample.\n\nraw &lt;- fetch_raw_data(conn, created_after = model_trained_at)\n\nfeatures &lt;- create_features(raw)\npredictions &lt;- predict(model, features)\n\naccuracy &lt;- compute_accuracy(predictions, features$is_setosa)\n\ncat(paste0(\"Out-of-sample accuracy is \", accuracy, \"%\"))\n\nOut-of-sample accuracy is 82%"
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-01-library-code/library-code.html#tying-it-together",
    "href": "posts/series/doing-data-science/2023-04-01-library-code/library-code.html#tying-it-together",
    "title": "Writing Internal Libraries for Analytics Work",
    "section": "Tying it Together",
    "text": "Tying it Together\nThe key piece to notice is how much we’re leveraging our library code in both the retraining and monitoring job. In both cases, we’re doing some very similar things – pulling raw data, creating features, making predictions, computing accuracy – so it makes a lot of sense that we’d want to reuse the code for those two jobs.\nThere might also be more use cases for the code: More retraining or monitoring jobs, REST APIs, ETL jobs, etc. The more times you need to rely on the same logic, the more benefit you’ll derive from having a single source of truth for all of the logic for your modeling process.\nIt also might be useful to separate this library from the common library proposed at the start. There are important tradeoffs to consider: On one hand, a single library might be convenient for having all of your logic in a single place. But on the other hand, as your library grows in scope, it’ll necessarily have a bigger footprint, rely on more dependencies, etc. which will make its use and maintenance more difficult. A happy middle ground for us has been having a single library per “model” or use case. For instance, at CollegeVine we have a package called mlchancing for our chancing model and a separate package called schoolrecommendr for school recommendations and affinity scoring. Keeping these separate has made it easier to iterate on each model individually while also not being a maintenance or ramp-up headache.\nIt’s my view that models and other analytics work that is in production is software, and should be treated as such. If a model is going to be shipped to production, it at the very least needs to be tested, documented, versioned, and put through some kind of CI/CD process. It’d be even better if it’s monitored automatically so that the data scientists working on it can be notified quickly if things start going wrong. Ultimately, writing library code for your modeling work is very well-suited to meeting all of these expectations. And it also just makes everyone’s lives easier by not needing to reinvent the wheel all the time."
  },
  {
    "objectID": "my-three-favorite.html",
    "href": "my-three-favorite.html",
    "title": "My Three Favorite, Alphabetically",
    "section": "",
    "text": "ALGORITHMS: Bogo Sort, Logistic Regression, Metropolis-Hastings\nARTISTS (Musical, According to Spotify): Bad Bunny, Imagine Dragons, Taylor Swift\nAUTHORS: Fredrik Backman, Khaled Hosseini, Sally Rooney\nBEERS: Heady Topper, King Sue, Very Green\nBOOKS (Fiction): The Kite Runner, Neverwhere, A Storm of Swords\nBOOKS (Non-Fiction): Algorithms to Live By, The Signal and the Noise, Factfulness\nCITIES (American): Bozeman, Cambridge, New York City\nCITIES (European): Barcelona, Budapest, Stockholm\nCITIES (Other): Medellín, Oaxaca, Vancouver\nCOLLEGE COURSES: Advanced Algorithms, Mathematical Structures, Price Theory\nCONDIMENTS: Calabrian Chiles, Preserved Lemons, Secret Aardvark Hot Sauce\nCUISINES (Couldn’t pick three): Italian, Japanese, Lebanese, Mexican\nDATA STRUCTURES: Graph, Skip List, Tibble\nECONOMIC SUBFIELDS: Labor Economics, Monetary Theory, Urban Economics\nFICTIONAL CHARACTERS: Elaine Benes, Oberyn Martell, Samwise Gamgee\nFOODS: Bananas, Maitake Mushrooms, Peanut Butter\nKITCHEN UTENSILS: 7” Santoku Knife, Tasting Spoon, Tongs\nMOVIES: Birdman, Inside Out, Moonrise Kingdom\nNON-CITY PLACES: Acadia National Park, Baseball Hall of Fame, Little Cottonwood Canyon\nPARADIGMS: Bayesian, Behavioral, Functional\nPIECES OF COOKWARE: 3-Quart Saute Pan, Cast Iron Skillet, Dutch Oven\nR PACKAGES: brms, dplyr, purrr\nSERIOUSEATS RECIPES: Halal Cart Chicken, Pozole Verde with Chicken, Red Wine-Braised Short Ribs\nSKI MOUNTAINS: Alta, Big Sky, Jackson Hole\nSPIRITS: Fernet, Islay Scotch, Mezcal\nSPORTS TO WATCH: Baseball, College Football, Soccer\nSUBREDDITS: AdvancedRunning, MaleLivingSpace, UnexpectedFactorial\nTV SHOWS: Atlanta, Chef’s Table, Seinfeld\nWEBSITES: Fangraphs, FiveThirtyEight, Serious Eats"
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-13-experiment-tracking/experiment-tracking.html",
    "href": "posts/series/doing-data-science/2023-05-13-experiment-tracking/experiment-tracking.html",
    "title": "Experiment Tracking and Model Versioning",
    "section": "",
    "text": "This post is part of a series called The Missing Semester of Your DS Education."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-13-experiment-tracking/experiment-tracking.html#introduction",
    "href": "posts/series/doing-data-science/2023-05-13-experiment-tracking/experiment-tracking.html#introduction",
    "title": "Experiment Tracking and Model Versioning",
    "section": "Introduction",
    "text": "Introduction\nI’ve started the past few posts by saying things along the lines of “When I first started my current job…” or “It wasn’t until my second data science job…” and going on about how it took me too long to learn something important. But not today! Today, I’ll tell you a story. But first, some background context.\nLet’s imagine that you work for a company whose website provides college guidance to high school students. You might have a page where a student can input their profile – maybe their academic information like their GPA or their SAT scores, some demographic information, or a list of their extracurricular activities – and then a second page where they can view information about different schools they might be interested in. A killer feature of that second page is the student being able to see their chances of being accepted at different schools based on the information in their profile. At CollegeVine we call this “Chancing,” and it’s a core part of our student experience.\nWhen we first started using machine learning for Chancing, we had a number of architectural questions to answer. Of course, there were questions about how the model would be deployed, how it would serve predictions to users, if we could cache predictions, how fast the code had to be, and so on. But this post will focus on the problem that, at first glance, I thought was the simplest: What version of the model is running in production? Over time, we’ve taken a few approaches to solving this problem. And so begins the story."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-13-experiment-tracking/experiment-tracking.html#model-versioning",
    "href": "posts/series/doing-data-science/2023-05-13-experiment-tracking/experiment-tracking.html#model-versioning",
    "title": "Experiment Tracking and Model Versioning",
    "section": "Model Versioning",
    "text": "Model Versioning\nAt first, we used an incrementing counter as the version of our chancing model, so we would identify different versions as v1, v2, and so on. Our assumption initially was that what was most important was being able to update a model (i.e. increment the version) and roll back to a previous model (i.e. decrement), and this versioning scheme allowed us to do that easily. But we quickly ran into some headaches. Notably, we’d find ourselves asking “Okay, Chancing v5 is in production. But when was it trained? And when did we ship it?” And so we soon outgrew our initial approach.\nAfter our first approach failed, our next idea was to use a timestamp like 20230514123456 (12:34 and 56 seconds on May 14th, 2023) as the model version. The timestamp would correspond to when the model was trained – we were assuming we’d ship right after training – and would still have all of the same properties as the incrementing counter from before (letting us upgrade and roll back easily) while also encoding extra information. We viewed this change as a Pareto Improvement.\n\n\n\n\n\n\nIt’s important to mention at this point that all of our modeling code is checked into Git. This means that we can, roughly speaking, see what the state of the world was at the time the model was trained.\n\n\n\nSo now that we’re able to not only upgrade and downgrade model versions quickly, but also know when the model was trained, it’s problem solved. Right?\nWrong. As it turns out, there was another pressing issue. Finding the state of the code when the model was trained was actually non-trivial, because, as I wrote in my previous post on internal libraries, our modeling code was living in a library. But our retraining job was loading a specific version of that library, which meant that we’d need to look through our Git history to find the state of the retraining job when the model was trained, and then find the version of the library the code was using, and then dig through our Git history again to uncover the logic in the library (ultimately the logic training the model) at the time. This is certainly possible, but it’s not trivial.\nAnd so with that, we tried our next idea: Appending the library version to the timestamp, to get a model version like 20230514123456-1-2-3, which would correspond to the model being trained at 12:34 and 56 seconds on May 14th, 2023, using library version 1.2.3. This was another Pareto Improvement: Now we could upgrade and downgrade, we knew when the model was trained, and we knew which library version the model was trained on. Amazing!\nBut at this point, I presume you’re starting to realize that this approach didn’t work either. And so were we. This was when we began to look for an off-the-shelf tool for this deceivingly challenging problem."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-13-experiment-tracking/experiment-tracking.html#enter-mlflow",
    "href": "posts/series/doing-data-science/2023-05-13-experiment-tracking/experiment-tracking.html#enter-mlflow",
    "title": "Experiment Tracking and Model Versioning",
    "section": "Enter MLFlow",
    "text": "Enter MLFlow\nAs it turns out, we’re not the only ones to have this problem. At the highest level, we needed to figure out a way to version our models such that it was easy to determine what actually went into them – the data, the hyperparameters we evaluated and which we selected, the features, and the rest of the code itself. We also needed a way to differentiate one model version from another. In particular, what made two different training runs of the same model different? Did the data change? Did the features change? Lastly, we wanted to be able to track a few other important aspects of the ML lifecycle:\n\nWhen the model was in production from and to.\nAny other parameters to the model, in addition to the hyperparameters, that were used on that training run.\nModel diagnostics, like AUC or the Brier Skill Score, calibration plots, and miscellaneous sanity checks.\nThe data the model was trained on.\n\nTo accomplish all of these goals and more, we turned to MLFlow. MLFlow lets us catalog whatever we want about our model training runs. We save artifacts (a fancy word for files) to MLFlow containing our training data, the models we fit, the results of hyperparameter tuning, a bunch of diagnostic plots, test cases we dynamically generate, and so on. In addition, we log parameters like what high school graduation years we included in our data, or what version of our R package – mlchancing – was used in training the model. We log lots of metrics like AUC, the Brier Skill Score, and so on to keep track of the performance of the model at training time. We also log metrics like the mean and standard deviation of each feature in our model at training time, so we can evaluate data drift over time.\nIn addition to tracking models, metrics, artifacts, and so on, MLFlow also lets us create a new model version for a training run. When we create a model version, we can mark that version as either having no status, being in staging, being in production, or being archived. These statuses let us track what model is in production at any given time, and the model versions link back to the run that the model was trained on, so we can see all of the metrics, artifacts, etc. listed above by just clicking into the training run for any model version in the MLFlow UI.\nLastly, MLFlow lets us log a Git commit as an attribute of the training run, which means we can click directly from the MLFlow UI to the state of our modeling code in GitHub at the time that our model was trained, which lets us more easily track down exactly what the state of the code was at the time.\nAnd here concludes my story. Since adopting MLFlow, our model versioning headaches have more or less subsided. We’ve been running MLFlow in production for about a year and a half now, and it’s made running experiments on our models, tracking training runs, comparing metrics across different ideas we have, and keeping tabs on what’s in production at any given point simple. It’s not a perfect tool by any means, but it’s solved most of our most painful problems."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-13-experiment-tracking/experiment-tracking.html#experiment-tracking-model-registries-and-mlops",
    "href": "posts/series/doing-data-science/2023-05-13-experiment-tracking/experiment-tracking.html#experiment-tracking-model-registries-and-mlops",
    "title": "Experiment Tracking and Model Versioning",
    "section": "Experiment Tracking, Model Registries, and MLOps",
    "text": "Experiment Tracking, Model Registries, and MLOps\nOf course, MLFlow isn’t the only tool out there. Broadly speaking, tools like MLFlow are often referred to as “experiment tracking” tools. Others in this category include Weights & Biases, Neptune, and Comet. Experiment tracking tools let you version different experiments you run, and store metadata like the training data, parameters, etc. on those experiments and training runs. These tools generally also provide a “model registry,” which tends to be the component that handles the versioning of models.\n\n\n\n\n\n\nAs an important aside: There’s a whole world of tools like these out there that make up the field of MLOps. Over the past few years, MLOps has been exploding as more and more companies face pains like ours when it comes to putting models into production. These pain points include everything from versioning to experimenting to deployment, so it’s been very exciting to see all of the awesome new tooling that’s being introduced every week.\nThis also means that this post will likely become stale quickly."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-13-experiment-tracking/experiment-tracking.html#wrapping-up",
    "href": "posts/series/doing-data-science/2023-05-13-experiment-tracking/experiment-tracking.html#wrapping-up",
    "title": "Experiment Tracking and Model Versioning",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nI’d like to wrap this post up with an important lesson I learned from our model versioning saga: If you’re having a technical problem like model versioning that seems simple but is proving to be difficult, it’s often a good idea to see what others are doing to solve that problem. Nowadays, I’d realize that model versioning must be a common problem, and think to myself “Someone must have solved this problem. What do they do?” After all, every company providing any kind of machine learning product must have model versioning issues.\nSo in hindsight, we could’ve come across the world of MLOps far sooner had we taken a step back to consider the fact that we must not be the only ones solving this problem. But we didn’t do that, and versioning became a thorn in our side instead. Hopefully our mistakes will help you take the step back that I wish I had.\nHappy experimenting!"
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html",
    "href": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html",
    "title": "Unit Testing Analytics Code",
    "section": "",
    "text": "This post is part of a series called The Missing Semester of Your DS Education."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#introduction",
    "href": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#introduction",
    "title": "Unit Testing Analytics Code",
    "section": "Introduction",
    "text": "Introduction\nUnit testing was a concept I had never even heard of before I started my second data science job. It never came up in any of my college statistics or computer science courses. It never came up in any of my data science internships. It never came up in my first data science job.\nIn conversations I have with friends – and, broadly, conversations with basically anyone doing data science or analytics – I face lots of pushback when it comes to unit testing. Usually the objections come in the form of either not knowing why you might test, since the code is just so simple and straightforward that nothing could go wrong, or not understanding the value added. In my opinion, both of these objections come from the same place. At first glance, it seems like some combination of blissful ignorance about what could go wrong and overconfidence in one’s own ability or in their code’s correctness, but I think that the objections actually come from something deeper. In my opinion, it’s the unfamiliarity of testing. It’s not something that’s commonly taught to people involved in analytics, and so it feels new. That can be scary."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#whats-a-unit-test",
    "href": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#whats-a-unit-test",
    "title": "Unit Testing Analytics Code",
    "section": "What’s A Unit Test?",
    "text": "What’s A Unit Test?\nFirst thing’s first: What’s a unit test? It’s actually really simple! Unit testing tests the smallest possible components of your code for correctness and what I would define as “good behavior.” In analytics, you might test things like feature engineering steps, metric definitions, or data wrangling code. The basic idea of a unit test is that you take a function you’ve written, and you’d make up some inputs to the function and then check if your code produces the outputs you’d expect. In the simplest case, you might test the identity function as follows:\n\nlibrary(testthat)\n\nidentity &lt;- function(x) {\n  x\n}\n\ntest_that(\"The identity returns the input\", {\n  expect_identical(1, identity(1))\n  expect_identical(\"foo\", identity(\"foo\"))\n  expect_identical(1:100, identity(1:100))\n  expect_identical(\n    lm(iris$Sepal.Length ~ iris$Sepal.Width), \n    identity(lm(iris$Sepal.Length ~ iris$Sepal.Width))\n  )\n})\n\nTest passed 🥳\n\n\nAll we’re doing here is checking that for some given input, our function returns the expected (correct) output. You can also test that your function returns an error or warning, returns nothing, returns some output, and much more. I’d highly recommend looking at the documentation for testthat or pytest to get a sense for what and how to test."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#case-study-feature-engineering",
    "href": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#case-study-feature-engineering",
    "title": "Unit Testing Analytics Code",
    "section": "Case Study: Feature Engineering",
    "text": "Case Study: Feature Engineering\nData scientists very often write feature engineering code. And as it turns out, feature engineering is a very common place where bugs can pop up unbeknownst to the code’s author.\n\n\n\n\n\n\nIt’s important to note that data scientists are usually writing Python or R, which are both dynamically typed, interpreted languages. Unit testing is doubly valuable in these types of languages, since you don’t get the benefits of a compiler and static type checks to catch issues in your code. In languages like Python and R, anything can go wrong in your code. And you often won’t find out about issues until runtime, or, depending on the nature of the bug, even later (if ever).\n\n\n\n\nAn Example\nLet’s write some example code to create a simple feature to use in a hypothetical machine learning model downstream.\n\ncreate_feature &lt;- function(x) {\n  (x - mean(x)) / sd(x)\n}\n\nThis might look familiar: It’s a function that takes a variable x and standardizes it. Now I’ll generate some data to show how it works.\n\n## 1000 draws from a Normal(25, 5)\nraw &lt;- rnorm(1000, mean = 25, sd = 5)\n\nWe can plot a histogram of our data to show what it looks like:\n\nhist(raw, breaks = 20)\n\n\n\n\n\n\n\n\nNow, let’s run raw through our feature engineering function.\n\nstd &lt;- create_feature(raw)\n\nhist(std, breaks = 20)\n\n\n\n\n\n\n\n\nGreat! Now std, the new variable that we created from standardizing raw, looks like it follows a standard normal distribution.\n\n\nTrivial Examples, Unexpected Results\nNow that we’ve written a function to create a feature, let’s use that feature engineering step on a couple of real-world examples. Keep in mind that as data scientists, we’re often working with messy data – it could include missing values, outliers, data of an incorrect type, etc. and there are often very few guarantees about what our data will look like in practice. These next few examples show how things could go wrong (often very quietly) in our seemingly correct feature engineering step we wrote above.\n\nMissings\nLet’s do the simplest thing first: What happens when we have missing values in our data?\n\ncreate_feature(c(1, 2, 3, NA_real_))\n\n[1] NA NA NA NA\n\n\nIf you’re familiar with R, this should be expected. And you’re probably thinking to yourself that we just need to set na.rm = TRUE, and you’d be right! But this brings me to the first major point I’d like to make on how things can go wrong.\n\n\n\n\n\n\nWhen you’re writing your code, it’s easy to forget things like adding na.rm = TRUE when the data you’re working with doesn’t appear to need it. It’s probably not your default behavior to remember to always set the flag to tell R to remove NA values, since if you’re not working with any of them, why would you remember to do that? Expecting yourself to remember to do something like this is a recipe for very brittle, error-prone code.\n\n\n\nLet’s fix this bug the naïve way.\n\ncreate_feature &lt;- function(x) {\n  (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)\n}\n\nGreat, and now let’s test it.\n\ncreate_feature(c(1, 2, 3, NA_real_))\n\n[1] -1  0  1 NA\n\n\nLooks good. Now all of our non-missing values have been correctly transformed.\n\n\nZero Standard Deviation\nHow about another non-trivial bug: A standard deviation of zero. Consider a toy example: Predicting whether or not someone plays basketball.\nIn this example, let’s say we have both men and women in our training data, and we want to use the height of each person to predict whether that person plays basketball. It probably doesn’t make sense to standardize the heights without grouping, since we don’t want to end up with a distribution of heights where most of the women are below average and most of the men are above average. It’d be much smarter to standardize within group, which would then produce a measure of height relative to the “competition” in some sense. Let’s give that a shot on a trivial data set where every man is the same height.\n\nlibrary(tibble)\nlibrary(dplyr)\n\ndata &lt;- tibble(\n  player_id = 1:6,\n  sex = c(\"M\", \"M\", \"M\", \"F\", \"F\", \"F\"),\n  height = c(77, 77, 77, 70, 71, 72)\n)\n\ndata %&gt;%\n  group_by(sex) %&gt;%\n  mutate(\n    height_std = create_feature(height)\n  ) %&gt;%\n  ungroup() %&gt;%\n  pretty_print()\n\n\n\n\nplayer_id\nsex\nheight\nheight_std\n\n\n\n\n1\nM\n77\nNaN\n\n\n2\nM\n77\nNaN\n\n\n3\nM\n77\nNaN\n\n\n4\nF\n70\n-1\n\n\n5\nF\n71\n0\n\n\n6\nF\n72\n1\n\n\n\n\n\n\n\nSo what happened here? In the male group, all of the heights were the same. This resulted in a standard deviation of zero when we went to standardize our height variable, which meant dividing by zero. So we get NaN back – Not a Number.\nThis example feels trivial, but in practice it’s not. If your data had hundreds or thousands of groups, it’s not all that unlikely to have a situation like this. And note that R doesn’t throw any kind of warning or error, it just quietly returns you NaN when you’re expecting a real number.\n\n\n\nInfinity\nR also has a built in value for infinity: Inf. What happens if we try to build our feature when we had an Inf in our data?\n\ncreate_feature(c(1, 2, 3, Inf))\n\n[1] NaN NaN NaN NaN\n\n\nA bunch of NaN. This seems trivial too, but imagine the following example, which you very well might run into in the real world: Computing GDP per capita.\n\ndata &lt;- tibble(\n  gdp = c(1000, 2000, 3000),\n  population = c(0, 50, 100)\n)\n\ndata %&gt;%\n  mutate(\n    gdp_per_capita = gdp / population\n  ) %&gt;%\n  mutate(\n    gdp_per_capita_std = create_feature(gdp_per_capita)\n  ) %&gt;%\n  pretty_print()\n\n\n\n\ngdp\npopulation\ngdp_per_capita\ngdp_per_capita_std\n\n\n\n\n1000\n0\nInf\nNaN\n\n\n2000\n50\n40\nNaN\n\n\n3000\n100\n30\nNaN\n\n\n\n\n\n\n\nR doesn’t yell at you for dividing by zero unlike Python for instance, or virtually any other self-respecting programming language. This means that if you accidentally divide by zero somewhere in your data pipeline, you could very well end up with a bunch of NaN if you standardize. And working with real-world data means that dividing by zero happens fairly often, generally because of small issues in the data we’re working with.\n\nOne Row Per Group\nBack to grouping: It’s also pretty likely when working with real-world data that you might have a group with only one row. Let’s see what happens in that case.\n\ndata &lt;- tibble(\n  group = c(1, 2, 2, 3, 3),\n  value = 1:5\n)\n\ndata %&gt;%\n  group_by(group) %&gt;%\n  mutate(\n    value_std = create_feature(value)\n  ) %&gt;%\n  ungroup() %&gt;%\n  pretty_print()\n\n\n\n\ngroup\nvalue\nvalue_std\n\n\n\n\n1\n1\nNA\n\n\n2\n2\n-0.7071068\n\n\n2\n3\n0.7071068\n\n\n3\n4\n-0.7071068\n\n\n3\n5\n0.7071068\n\n\n\n\n\n\n\nOnce again, same issue. A single-row group returns NA, since the standard deviation of a single number isn’t defined.\n\n\n\nLessons\nWhat have we learned here?\nIn short, there are many ways for things to quietly go wrong in your data pipelines, especially in a language like R. Even with a function as simple as standardization, it’s easy to cook up all kinds of possible corner cases or other issues that would cause your function to return any number of unpredictable results. And when working with real-world data, some of these quirks are inevitable. It’s virtually impossible that you’d never run into any of the issues enumerated above when working with messy data for any amount of time. And these examples were about as simple as they could be. In the real world, analytical code is often far more complicated and data far messier than this, which compounds the likelihood of issues like these."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#enter-testing",
    "href": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#enter-testing",
    "title": "Unit Testing Analytics Code",
    "section": "Enter Testing",
    "text": "Enter Testing\nAnd with all of that: Back to testing. Testing is the only way to know that your code is actually doing what you think it’s doing, and writing tests is a great way to make guarantees about the ability of your code to handle some of these issues that we’ve discussed. Writing tests also lets you ensure that your implementation is correct, and it lets you refactor your code more easily by loudly alerting you if you’ve broken something, which should make you much more confident in the correctness of your implementation.\n\n\n\n\n\n\nIf you’re not familiar with it, this is a good point to introduce Test-Driven Development. It’s not something I would always recommend, but at least being familiar with it might make for a good starting point when it comes to learning how to think about writing tests for your code\n\n\n\n\nTesting Our Standardization Function\nAnd with that, let’s write some tests. Imagine that we knew that we wanted to write our feature engineering function to do standardization. We also know that we want to avoid some of the bugs that we introduced in the examples above. One possible option for handling those cases where our function will do something unexpected would be to throw an error if we get unexpected inputs. That’s what I’ll illustrate here – it’d let the user learn about the issue quickly and debug. There are many options for how to handle these issues though. You might also fall back on a default value when your code will return a NA or NaN, for instance. Now, on to the tests.\n\ntest &lt;- function() {\n  test_that(\"Standardized variable is transformed correctly\", {\n    random_normal &lt;- rnorm(1000, 10, 5)\n    \n    ## Expect that the mean of the transformed data is within\n    ## 0.005 of zero\n    expect_lt(\n      abs(mean(create_feature(random_normal))),\n      0.005\n    )\n    \n    ## Expect that the stddev of the transformed data is within\n    ## 0.005 of 1\n    expect_lt(\n      abs(1 - sd(create_feature(random_normal))),\n      0.005\n    )\n    \n    ## Expect that the math is done correctly\n    expect_identical(\n      create_feature(c(1,2,3)),\n      c(-1, 0, 1)\n    )\n  })\n  \n  test_that(\"Inifinity causes an error\", {\n    expect_error(\n      create_feature(c(1, 2, 3 / 0)),\n      \"`x` must not contain any infinite values\"\n    )\n  })\n  \n  test_that(\"Zero stddev causes an error\", {\n    expect_error(\n      create_feature(c(1, 1, 1)),\n      \"`x` must have a non-zero standard deviation\"\n    )\n  })\n  \n  test_that(\"Length one causes an error\", {\n    expect_error(\n      create_feature(c(1)),\n      \"`x` must have more than one unique element\"\n    )\n  })\n}\n\nAnd now let’s run our test suite.\n\ntest()\n\nTest passed 🥇\n── Failure ('&lt;text&gt;:27:5'): Inifinity causes an error ──────────────────────────\n`create_feature(c(1, 2, 3/0))` did not throw an error.\n\n\nError in `reporter$stop_if_needed()`:\n! Test failed\n\n\nUnsurprisingly, we get some failures. So now let’s refactor our feature engineering function to pass our tests.\n\nlibrary(rlang)\n\ncreate_feature &lt;- function(x) {\n  \n  mu &lt;- mean(x, na.rm = TRUE)\n  sigma &lt;- sd(x, na.rm = TRUE)\n\n  if (isTRUE(sigma == 0)) abort(\"`x` must have a non-zero standard deviation.\")\n  if (isTRUE(length(unique(x)) == 1L)) abort(\"`x` must have more than one unique element.\")\n  if (isTRUE(any(is.nan(x)))) abort(\"`x` must not contain any `NaN` values.\")\n  if (isTRUE(any(is.infinite(x)))) abort(\"`x` must not contain any infinite values.\")\n\n  (x - mu) / sigma\n}\n\ntest()\n\nTest passed 😀\nTest passed 😸\nTest passed 🎉\nTest passed 🥇\n\n\nWoohoo! Our tests all passed. Now, if the user of our function tries to do something like only providing a single value for x, they get an informative error back.\n\ncreate_feature(c(1))\n\nError in `create_feature()`:\n! `x` must have more than one unique element.\n\n\nIn a less trivial example, consider our grouped computation from before:\n\ndata &lt;- tibble(\n  group = c(1, 2, 2, 3, 3),\n  value = 1:5\n)\n\ndata %&gt;%\n  group_by(group) %&gt;%\n  mutate(\n    value_std = create_feature(value)\n  ) %&gt;%\n  ungroup() %&gt;%\n  pretty_print()\n\nError in `mutate()`:\nℹ In argument: `value_std = create_feature(value)`.\nℹ In group 1: `group = 1`.\nCaused by error in `create_feature()`:\n! `x` must have more than one unique element.\n\n\nNot only do we get a helpful error, but in this particular case dplyr also adds helpful context: It tells us which step failed and which group it failed on so that we can effectively debug."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#refactoring",
    "href": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#refactoring",
    "title": "Unit Testing Analytics Code",
    "section": "Refactoring",
    "text": "Refactoring\nNow that we have a test suite, we can also refactor our function and be much more confident that we haven’t broken anything. Let’s do an overly complicated refactor.\n\ncreate_feature &lt;- function(x) {\n  \n  ## Compute the mean as the sum of the non-null elements \n  ## divided by the number of non-null elements\n  mu &lt;- sum(x, na.rm = TRUE) / length(na.omit(x))\n  sigma &lt;- sd(x, na.rm = TRUE)\n\n  if (isTRUE(sigma == 0)) abort(\"`x` must have a non-zero standard deviation.\")\n  if (isTRUE(length(unique(x)) == 1L)) abort(\"`x` must have more than one unique element.\")\n  if (isTRUE(any(is.nan(x)))) abort(\"`x` must not contain any `NaN` values.\")\n  if (isTRUE(any(is.infinite(x)))) abort(\"`x` must not contain any infinite values.\")\n\n  (x - mu) / sigma\n}\n\nAnd now let’s run our tests again.\n\ntest()\n\nTest passed 🌈\nTest passed 🥇\nTest passed 🌈\nTest passed 🥇\n\n\nAwesome! We just did a simple refactor and our tests passed, so I feel good about the correctness of that refactor. This was a very simple example, but you could imagine arbitrarily complex refactors. The value of a test suite increases exponentially as the complexity of the code increases."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#wrapping-up",
    "href": "posts/series/doing-data-science/2023-04-03-unit-testing/unit-testing.html#wrapping-up",
    "title": "Unit Testing Analytics Code",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nThere are a few important takeaways from this post.\nFirst and most importantly, it’s important to test your code. There are so many ways that things can go wrong, and writing tests is the only way to really be confident that nothing is. This is especially true in dynamically typed, interpreted languages like R or Python, where data scientists don’t get the benefits of a compiler or a static type system to catch bugs in their code.\nSecond, analytical code that’s well-written should be easy to test. Ultimately, lots of the code we write for analytics work takes some data, does something to it, and returns some new data. That type of logic makes for a great use case for unit testing: If your code doesn’t produce the values you expect, it’s incorrect.\nLastly, there are many tools in Python (pytest, unittest, etc.) and R (testthat) to make unit testing as simple as writing a few lines of logic and an assertion or two. And then you get all the benefits of having well-tested code, such as the newly-found ease of refactoring said code without being concerned about breaking it.\nHappy testing, and enjoy the peace of mind!"
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-14-code-review/code-review.html",
    "href": "posts/series/doing-data-science/2023-04-14-code-review/code-review.html",
    "title": "Pull Requests, Code Review, and The Art of Requesting Changes",
    "section": "",
    "text": "This post is part of a series called The Missing Semester of Your DS Education."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-14-code-review/code-review.html#introduction",
    "href": "posts/series/doing-data-science/2023-04-14-code-review/code-review.html#introduction",
    "title": "Pull Requests, Code Review, and The Art of Requesting Changes",
    "section": "Introduction",
    "text": "Introduction\nIn my last post on unit testing analytics code, I started off by saying that I had never heard of unit testing until my second data science job. Not in college, not in internships, and not in my first data science job. The same is true for pull requests and code review – they were completely new to me.\nThis, in my view, is a very unfortunate commonality in analytics teams. All too often, code will go unreviewed or kept out of any kind of version control system entirely. Ultimately, this practice, albeit common, is detrimental to the teams practicing it.\nSince then, code review – both doing code review for others and getting feedback from others on my code – has grown to become one of my favorite parts of my job. It’s also a mission-critical process for any team that strives to maintain a high-quality codebase. But more importantly, it’s an valuable social exercise, and an fun way for contributors to come together as a team to collaborate, learn from each other, and improve their skills.\n\n\n\n\n\n\nI’d also like to note that the Tidyverse team just published a great series on code review, which I’d highly recommend reading."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-14-code-review/code-review.html#but-first-git",
    "href": "posts/series/doing-data-science/2023-04-14-code-review/code-review.html#but-first-git",
    "title": "Pull Requests, Code Review, and The Art of Requesting Changes",
    "section": "But First: Git",
    "text": "But First: Git\nThere are a ton of introductions to Git online, all of which are far better than anything I could write here. If you’re not already at least somewhat familiar with Git, I’d recommend starting with a post like one of these:\n\nFrom GitHub\nFrom Atlassian\nFrom Git itself\n\nThat said, what I’ll discuss here doesn’t require any particular Git knowledge."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-14-code-review/code-review.html#on-code-review",
    "href": "posts/series/doing-data-science/2023-04-14-code-review/code-review.html#on-code-review",
    "title": "Pull Requests, Code Review, and The Art of Requesting Changes",
    "section": "On Code Review",
    "text": "On Code Review\nI wasn’t sure where to start this post. I considered discussing Git itself, but I decided it would be better to leave it to the experts. Then I thought about discussing the function of pull requests, but that didn’t seem particularly valuable either. So I figured I might just jump into the meat of it: Why code review is valuable, and why analytics teams should be doing more of it than they are.\n\nWhy Do We Review Code?\nPeople and teams have wildly differing opinions about what the main objective of code review is. The following is the opinion of the Tidyverse team:\n\nThe primary purpose of code review is to ensure that the overall “code health” of our packages is improving over time.\n\nAnother common opinion on the main purpose of code review is to catch bugs in code before they hit production.\nIn my opinion, these are both important objectives of code review to be sure, but are not the main objective. My team’s view is that the main objective of code review is knowledge transfer between teammates. On our team – with “team” encompassing both data and engineering – we talk often about the bus factor of our work. A toned-down explanation of the bus factor is that it represents the number of people who need to go on vacation, or be out sick, or leave for a new job for a project to come to a grinding halt because else nobody on the team knows how a particular system works. We’ve been bitten multiple times by projects with a bus factor of one, meaning that if one person were to leave, that project would be left in limbo.\nAnalytics work is no different than engineering work in this respect: If one data scientist lives in a dark room for two years building a novel model and ships it to production without anyone else knowing how it works, how to maintain it, and how to monitor it, then that project is risky. If that data scientist were to leave, there would be a model in production that nobody else had experience working with, which could result in any number of issues. In steps code review to help us solve this problem.\n\n\nOther Reasons To Review Code\nAside from knowledge transfer, code review is also a valuable opportunity to do quality control. In no particular order, all of the following (and more) can be suggested in code review:\n\nBug fixes\nMaintainability improvements, such as improving the readability of the code, adding comments, adding tests, etc.\nPerformance improvements\nSecurity improvements\n\nAll of these are important aspects of maintaining a codebase over the long-term, and, in my experience, are far too often overlooked by analytics teams. In particular, a past team of mine – as well as many others, from what I’ve read and heard in talking to others in analytical roles – had a bad habit of data scientists being siloed and not having any eyes on their work until things were “ready” to be shipped to production. In my opinion, data products are software and should be treated as such. This means that if our models will be shipped to production, then it’s imperative that the code underlying them is written in a way that’s understandable by others, safe, correct, and, to whatever extent necessary, performant. Code review helps achieve all of these goals.\nLastly, code review is a social exercise. It’s a great way for a team to work together, especially in the world of remote work. In getting feedback from teammates, data scientists get to flex their collaboration muscle and, ideally, this type of collaboration allows the whole to be more than the sum of its parts.\n\n\nAn Example Review\nSince code review is unfamiliar to a lot of us in the analytics world, I’ll give an example of how I might do a review. Our team often works with data from external sources like the Census for doing all kinds of analysis. Let’s imagine reviewing the following code that a hypothetical teammate has put in to determine the five counties in Arizona with the highest percentage of Native American residents:\n\nlibrary(httr)\nlibrary(purrr)\nlibrary(dplyr)\n\nvariables &lt;- c(\n  \"NAME\",\n  \"B01001A_001E\",\n  \"B01001B_001E\",\n  \"B01001C_001E\",\n  \"B01001D_001E\",\n  \"B01001E_001E\",\n  \"B01001F_001E\",\n  \"B01001G_001E\"\n)\n\nresponse &lt;- GET(\n  \"https://api.census.gov/data/2019/acs/acs5\",\n  query = list(\n    get = paste(variables, collapse = \",\"),\n    \"for\" = \"tract:*\",\n    \"in\" = \"state:04\",\n    key = secret::get_secret(\"CENSUS_API_KEY\", vault = sprintf(\"%s/posts/config\", Sys.getenv(\"SITE_ROOT_DIR\")))\n  )\n) %&gt;%\n  content()\n\ncols &lt;- response[[1]]\nresponse &lt;- response[-1]\n\nresponse %&gt;%\n  map_dfr(\n    ~ set_names(.x, cols)\n  ) %&gt;%\n  janitor::clean_names() %&gt;%\n  mutate(\n    across(\n      -c(name, tract, county, state),\n      as.numeric\n    )\n  ) %&gt;%\n  group_by(county) %&gt;%\n  summarize(\n    na_pct = sum(b01001c_001e) / sum(rowSums(across(where(is.numeric))))\n  ) %&gt;%\n  slice_max(\n    order_by = na_pct,\n    n = 5\n  ) %&gt;%\n  pretty_print()\n\n\n\n\ncounty\nna_pct\n\n\n\n\n001\n0.7377187\n\n\n017\n0.4465544\n\n\n005\n0.2654699\n\n\n007\n0.1630934\n\n\n012\n0.1495215\n\n\n\n\n\n\n\nAt first glance, this code looks pretty involved and poorly written to me. In the real world, we often write complicated code that relies on hyper-specific domain knowledge about the problem being solved in order to really understand it. As I mentioned above, our team often finds ourselves working with external data sources, such as the Census data shown here. And these sources usually have their own internal codes or identifiers that denote what the data you’re actually looking at is about. For instance, the series B01001C_001E used above represents the total Native American population in a region. In the Census, it’s corresponding “concept” is defined as the following:\n\nB01001C_001E: SEX BY AGE (AMERICAN INDIAN AND ALASKA NATIVE ALONE)\n\nObviously, these codes are not comprehensible for the layman. There’s method to the madness, but we so often work with gibberish codes that don’t correspond to anything intuitive and make code review and maintenance dramatically harder. The challenge here is that in order to effectively review this code, the reviewer not only needs to review what the code is trying to accomplish in some high-level sense, but also needs to understand the data itself and its schema in order to understand how the author is trying to accomplish their goal. Unfortunately, the code’s author has not done a good job of helping the reviewer (me) understand the code.\nNot only do I not have any context on the Census codes, but there’s no documentation linked (or written out) to help me find what the codes mean, there’s no documentation of the API response to explain why they’re seemingly removing the first item in the response’s body and setting it as the names of the columns in my final data frame, and there’s no explanation of what na_pct means, which in the context of this work means “percentage Native American” but could also reasonably mean something like “percentage of the data that’s null”.\nAnd this is exactly where the value of code review comes in. We often like to say that you’re not writing code for yourself right now. Instead, you should be writing it for a future you in six months or a year who won’t remember all of the intricacies of this problem anymore. In short, you want your code to be understandable to future you, and, by extension, your teammates. But code authors, just like anyone else working on a project of any sort, are often so deep in the weeds of their work and so familiar with all of the ins and outs that it’s easy to forget just how confusing their work can be at times. Code review is a great opportunity for others to ask for clarification, which is the knowledge transfer I mentioned before.\nIf a teammate put in these changes, I’d request a few improvements:\n\nWhat do the Census codes mean?\nWhat does na_pct mean?\nWhat does the response from the Census API look like?\nWhat does state:04 mean?\n\nAfter requesting clarification on all of those points, I’d hope the code would ultimately end up looking something like this:\n\n## Data from the Census ACS (American Community Survey)\n##\n## ACS 2019 5-year data\n## B01001A_001E: Total White alone population\n## B01001B_001E: Total Black or African American alone population\n## B01001C_001E: Total American Indian and Alaska Native alone population\n## B01001D_001E: Total Asian alone population\n## B01001E_001E: Total Native Hawaiian and Other Pacific Islander alone population\n## B01001F_001E: Total Some other race alone population\n## B01001G_001E: Total Two or more races population\n##\n## Codes of the form B01001X_001E\n## correspond to \"Total X population\" where X\n## is a race as defined by the Census. For instance,\n## B01001A_001E: Total White alone population\nvariables &lt;- c(\n  white_alone = \"B01001A_001E\",\n  black = \"B01001B_001E\",\n  native_american = \"B01001C_001E\",\n  asian = \"B01001D_001E\",\n  hawaiian_pacific_islander = \"B01001E_001E\",\n  other_alone = \"B01001F_001E\",\n  multi_racial = \"B01001G_001E\"\n)\n\nresponse &lt;- GET(\n  \"https://api.census.gov/data/2019/acs/acs5\",\n  query = list(\n    get = paste(variables, collapse = \",\"),\n    \"for\" = \"tract:*\",\n    \"in\" = \"state:04\",\n    key = secret::get_secret(\"CENSUS_API_KEY\", vault = sprintf(\"%s/posts/config\", Sys.getenv(\"SITE_ROOT_DIR\")))\n  )\n) %&gt;%\n  content()\n\n## The first item in the JSON response body is a vector\n## of the names of the fields returned.\n## For this query, we get: \n##\n##   * B01001A_001E\n##   * B01001B_001E\n##   * B01001C_001E\n##   * B01001D_001E\n##   * B01001E_001E\n##   * B01001F_001E\n##   * B01001G_001E\n##   * state\n##   * county\n##   * tract\n##\n## Every other record contains data corresponding to these fields.\n## The values in the population fields (e.g. B01001A_001E) are integers\n## representing the total population for that racial group in that tract.\n## The state, county, and tract are all FIPS codes.\ncols &lt;- response[[1]]\nresponse &lt;- response[-1]\n\nresponse %&gt;%\n  map_dfr(\n    ~ set_names(.x, cols)\n  ) %&gt;%\n  janitor::clean_names() %&gt;%\n  mutate(\n    across(\n      all_of(tolower(unname(variables))),\n      as.numeric\n    )\n  ) %&gt;%\n  group_by(county) %&gt;%\n  summarize(\n    percent_native_american = sum(b01001c_001e) / sum(rowSums(across(all_of(tolower(unname(variables))))))\n  ) %&gt;%\n  slice_max(\n    order_by = percent_native_american,\n    n = 5\n  ) %&gt;%\n  pretty_print()\n\n\n\n\ncounty\npercent_native_american\n\n\n\n\n001\n0.7377187\n\n\n017\n0.4465544\n\n\n005\n0.2654699\n\n\n007\n0.1630934\n\n\n012\n0.1495215\n\n\n\n\n\n\n\nNow that the code has been rewritten, future readers of the code have some much needed context. The author has explained what the data looks like when we get it back from the API, has explained what each of the codes corresponds to, and has renamed a variable to be more helpful. Ultimately, these changes will make it easier for readers to understand what this code is trying to accomplish and work with it."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-04-14-code-review/code-review.html#wrapping-up",
    "href": "posts/series/doing-data-science/2023-04-14-code-review/code-review.html#wrapping-up",
    "title": "Pull Requests, Code Review, and The Art of Requesting Changes",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nThe key takeaway here should be that in doing this review, we’ve established some shared sense of how this code works between multiple people instead of just one (the author) understanding it. In addition, the reviewer helped the author rework their code to make it easier for anyone else on the team to understand. This means that when we need to use this code in the future, it’ll be written in a way that makes it easier for that future person to work with, which will ultimately lead to them being able to ship more quickly and confidently, and needing to reinvent the wheel less often.\nA popular saying is that code is read ten times more than it’s written since we often need to understand existing code to author new code. Reviews are a great way to make sure that code is written in a maintainable, understandable way so that when it’s read all of those many times in the future, it’s able to be understood effortlessly."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-27-renv-dependency-management/renv-dependency-management.html",
    "href": "posts/series/doing-data-science/2023-05-27-renv-dependency-management/renv-dependency-management.html",
    "title": "Dependency Management",
    "section": "",
    "text": "This post is part of a series called The Missing Semester of Your DS Education."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-27-renv-dependency-management/renv-dependency-management.html#introduction",
    "href": "posts/series/doing-data-science/2023-05-27-renv-dependency-management/renv-dependency-management.html#introduction",
    "title": "Dependency Management",
    "section": "Introduction",
    "text": "Introduction\nWhen I was first learning to program, I’d face problems that would require (or at least were just made easier by using) library code. For instance, I learned about the Tidyverse the hard way my first summer in college when I was interning at a FinTech firm doing lots of data wrangling, because I had just finished spending the summer very hackily implementing group_by and summarize by hand using lots and lots of layers of nested for loops. That experience taught me an important lesson: I realized that thinking “Someone else must have solved this problem before” and then seeing what they’ve done is a very practical way to solve problems.\nAnd with that realization (that I could use libraries for data wrangling), I started writing lots of lines of code like this one:\ninstall.packages(\"tidyverse\")\nor the not-quite-equivalent Python:\npip install pandas\nAnd as soon as I began doing that, I started running into one of the most common, if not the most common, source of headaches for new programmers: Dependency conflicts. I was getting errors like these:\n&gt; Error: package or namespace load failed for ‘foo’ in loadNamespace(j &lt;- i[[1L]], \n&gt; c(lib.loc, .libPaths()), versionCheck = vI[[j]]):\n    namespace ‘bar’ 0.6-1 is being loaded, but &gt;= 0.8 is required"
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-27-renv-dependency-management/renv-dependency-management.html#whats-the-problem",
    "href": "posts/series/doing-data-science/2023-05-27-renv-dependency-management/renv-dependency-management.html#whats-the-problem",
    "title": "Dependency Management",
    "section": "What’s the Problem?",
    "text": "What’s the Problem?\nAt the time, I saw these types of errors and was frustrated. I didn’t understand them – why is a package requiring a specific version of another package? And I also thought there was a quick fix: I’d just install the required version of the dependency and everything would work again.\nUntil I inevitably ran into the same problem again soon after, while trying to install some other dependency.\nAt its core, I was experiencing my first foray into what might be considered a semi-pro version of dependency hell. I thought I could “solve” my problem of incompatible dependencies with duct tape – installing the correct version in order to fix the immediate error – but that doesn’t actually work in practice, since the same issue is bound to come up again with another version of a different library, or in a different project, or on another day."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-27-renv-dependency-management/renv-dependency-management.html#dependency-resolution",
    "href": "posts/series/doing-data-science/2023-05-27-renv-dependency-management/renv-dependency-management.html#dependency-resolution",
    "title": "Dependency Management",
    "section": "Dependency Resolution",
    "text": "Dependency Resolution\nIn practice, this is a dependency management problem. And the basic story goes as follows.\nYou start on your programming journey, and you want to use some great library – take dplyr, for example – for some of your work. So you install.packages(\"dplyr\") and it installs the most recent version. Then you decide you’re interested in working with spatial data, so you install sf with install.packages(\"sf\"). Next, you realize you need to work with Census data (what else are you going to be making maps of, after all?), so you install.packages(\"tidycensus\") and with those three packages, you do your project.\nBut then next week you have a homework assignment, which, entirely hypothetically, requires you to install some niche package like rethinking, so, of course, you install.packages(\"rethinking\") and you do your homework.\nAnd then later that night, you get curious about seeing what the Elon Jet Tracker has been up to on Twitter, and so you install rtweet: install.packages(\"rtweet\"), but this time, you get one of the errors above about a namespace clash.\nAnd so now, you’re at a block in the road. You can either upgrade or downgrade the dependency that’s clashing and risk one of your other projects breaking, or you can try to get around the problem another way, such as using an old version of rtweet or not using rtweet at all.\nThis is the point at which former you – and former me – should have been thinking “someone must have solved this problem.”"
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-27-renv-dependency-management/renv-dependency-management.html#declaring-your-dependencies",
    "href": "posts/series/doing-data-science/2023-05-27-renv-dependency-management/renv-dependency-management.html#declaring-your-dependencies",
    "title": "Dependency Management",
    "section": "Declaring Your Dependencies",
    "text": "Declaring Your Dependencies\nThe root cause of the errors in situations like these is that we have a dependency leakage problem. In the example I gave above, there were three separate “projects”: Your Census data analysis, your homework, and your keeping tabs on Elon’s jet. Those three projects don’t need to know about each other, and it’s actually a problem that they do. The issue is that in using install.packages(\"...\") (or pip install ...) everywhere, you’re installing all of these packages globally on your machine. This means that every project you’re working on needs to use the same dependencies, even when the projects are separate.\nAbove, I proposed the solution to this problem that I used to use, which was to just install the version of the package that was needed by the package I was trying to install, and continue on until I was inevitably frustrated by the issue once again, generally sooner rather than later. But there’s a battle-hardened way of solving this problem: Declaring your dependencies, and using a dependency isolation tool.\nIn R, there’s the great renv for this. In Python, I personally like Poetry, but Conda, pipenv, or just a plain old virtualenv would work just fine too. The key is that you want to be declaring the dependencies that your project needs in some kind of file (such as an DESCRIPTION file in R, or a pyproject.toml or a requirements.txt in Python, or a Gemfile in Ruby, and so on), and then preferably having a dependency manager like renv or poetry resolve those dependencies and save the result into a lockfile, like a poetry.lock or an renv.lock. Then, when you want to work on your project, or homework, or whatever, you restore the dependencies as they’re recorded in that lockfile. This means that whenever you want to run a project, you know exactly what versions of every dependency need to be installed. And at the same time, if you want to add a new dependency, your dependency manager can do its best to resolve conflicts between that new dependency and all of the other libraries in your lockfile."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-27-renv-dependency-management/renv-dependency-management.html#dependency-isolation",
    "href": "posts/series/doing-data-science/2023-05-27-renv-dependency-management/renv-dependency-management.html#dependency-isolation",
    "title": "Dependency Management",
    "section": "Dependency Isolation",
    "text": "Dependency Isolation\nThe other key piece to the puzzle is dependency isolation, which is the leakage problem from before. In an ideal world, the dependencies for your project should be “isolated”, meaning that they’re only installed in your project environment (the R project, the virtualenv, etc.) and not globally on your machine.\nLet’s take an example in R to see how dependency isolation works. Start by initializing a very simple R project by running the command below in a terminal.\nmkdir renv-test && \\\n  cd renv-test && \\\n  echo \"library(example)\" &gt;&gt; test.R && \\\n  Rscript -e \"install.packages('renv', repos='http://cran.us.r-project.org'); renv::init()\" && \\\n  Rscript -e \"renv::install('mrkaye97/mrkaye97.github.io:posts/series/doing-data-science/2023-05-27-renv-dependency-management/example'); renv::snapshot()\" && \\\n  cat renv.lock\nYou should see some R output about installing packages, and then you should see something like this:\n{\n  \"R\": {\n    \"Version\": \"4.1.2\",\n    \"Repositories\": [\n      {\n        \"Name\": \"CRAN\",\n        \"URL\": \"https://cloud.r-project.org\"\n      }\n    ]\n  },\n  \"Packages\": {\n    \"example\": {\n      \"Package\": \"example\",\n      \"Version\": \"0.1.0\",\n      \"Source\": \"GitHub\",\n      \"RemoteType\": \"github\",\n      \"RemoteHost\": \"api.github.com\",\n      \"RemoteUsername\": \"mrkaye97\",\n      \"RemoteRepo\": \"mrkaye97.github.io\",\n      \"RemoteSubdir\": \"posts/series/doing-data-science/2023-05-27-renv-dependency-management/example\",\n      \"RemoteRef\": \"master\",\n      \"RemoteSha\": \"a72bb805e175c77e7bb8a2f4fb11780b76807d4d\",\n      \"Hash\": \"22d8e981dd94e2fab693636781631008\"\n    },\n    \"renv\": {\n      \"Package\": \"renv\",\n      \"Version\": \"0.17.3\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"CRAN\",\n      \"Requirements\": [\n        \"utils\"\n      ],\n      \"Hash\": \"4543b8cd233ae25c6aba8548be9e747e\"\n    }\n  }\n}\nThis last bit is the renv lockfile. It’s where all of the dependencies of your project are enumerated. Here, the project is trivial. We’re just installing example, which has no dependencies, so it’s the only thing (in addition to renv itself) that’s recorded in the lockfile. But this also generalizes to arbitrarily complicated projects and sets of dependencies. As you add more packages, the lockfile grows, but you can simply run renv::restore() to restore all of the dependencies for your project.\nNext, if you run the following, you should be able to use the package just fine:\nRscript -e \"library(example); hello()\"\nAnd you’ll see:\n[1] \"Hello, world!\"\nBut then, try this:\ncd .. && \\\n  Rscript -e \"library(example); hello()\"\nUnless you have another package installed globally called example that does the same thing as my example package, you’ll see:\nError in library(example) : there is no package called ‘example’\nExecution halted\nBut this time, this error is a feature, not a bug! This is an example of dependency isolation. When you’re inside of your renv-test project, you install the example package, record it in the lockfile, and can use it just fine. But as soon as you’re no longer inside of that project, this package is no longer available. This means that if you, for instance, start a new project that needs a different version of example, you don’t need to worry about this renv-test project being corrupted. The two projects are isolated from each other, so they can rely on separate sets of dependencies with no leakage from one to the other.\n\n\n\n\n\n\nAs an aside, tools like renv and pip are also usually good at dependency resolution, meaning they can figure out which versions of the dependencies of the packages you want to use you should install in order to make sure that everything is compatible. If you’re interested, you can read about how pip does dependency resolution on their site."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-27-renv-dependency-management/renv-dependency-management.html#other-resources",
    "href": "posts/series/doing-data-science/2023-05-27-renv-dependency-management/renv-dependency-management.html#other-resources",
    "title": "Dependency Management",
    "section": "Other Resources",
    "text": "Other Resources\nThere are tons of amazing resources on managing dependencies. This is a problem that every software team needs to manage, so virtually every programming language will have at least one, if not many widely-used dependency management tools. Since this is such a common problem, there’s also a lot written (and spoken!) about it. A few resources I particularly like are David Aja’s RStudio::conf 2022 talk about renv (which I was enthusiastically in the audience for!) and something he mentions, which is The 12-Factor App, which our team’s Head of Engineering recommended to me very soon after starting at CollegeVine."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-05-27-renv-dependency-management/renv-dependency-management.html#wrapping-up",
    "href": "posts/series/doing-data-science/2023-05-27-renv-dependency-management/renv-dependency-management.html#wrapping-up",
    "title": "Dependency Management",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nDavid Aja’s talk on this topic was called “You Should Be Using renv,” and you should be. In the short run, it might feel like setting up the scaffolding for renv or poetry or whatever your tool of choice is adds too much overhead. But if you’re feeling that way, it’s important to keep in mind that it’s inevitable that you end up running into dependency issues, and it’ll be much harder to untangle them once you’re already in deep than it will be to build good habits from the get-go.\nYou should be using renv."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-08-03-doing-data-science/doing-data-science.html",
    "href": "posts/series/doing-data-science/2023-08-03-doing-data-science/doing-data-science.html",
    "title": "On Doing Data Science",
    "section": "",
    "text": "This post is part of a series called The Missing Semester of Your DS Education."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-08-03-doing-data-science/doing-data-science.html#introduction",
    "href": "posts/series/doing-data-science/2023-08-03-doing-data-science/doing-data-science.html#introduction",
    "title": "On Doing Data Science",
    "section": "Introduction",
    "text": "Introduction\nThis post is something I’ve wanted to write for a long time, but never quite knew how to. I wasn’t sure where to start, and it was bound to turn into a rambling jumble of not-so-useful things I’ve learned along the way.\nBut to some extent, this post is the encapsulation of this series I’ve just written: The Missing Semester of Your DS Education. And so I thought: What better time than now?"
  },
  {
    "objectID": "posts/series/doing-data-science/2023-08-03-doing-data-science/doing-data-science.html#a-harmless-interview-question",
    "href": "posts/series/doing-data-science/2023-08-03-doing-data-science/doing-data-science.html#a-harmless-interview-question",
    "title": "On Doing Data Science",
    "section": "A Harmless Interview Question",
    "text": "A Harmless Interview Question\nThis whole series, and this post in particular, stems from an interview question that I got when I was interviewing for what ultimately was my first data science job: Doing analytics work in the Baltimore Orioles front office. I was in a room surrounded by something like six data scientists who would ultimately become my teammates and one of them asked me the following (paraphrased) question: “What are the steps that you go through when doing an analytics project?”\nI immediately thought I knew the answer, and confidently jumped into some explanation about how first there’s exploratory data analysis (EDA) to figure out what’s what in your data. Once you’ve finished your EDA, you do the feature engineering, and then you do the model fitting - of course not forgetting about cross validation! And finally, you do diagnostics on the model, like checking the accuracy or the MSE. I went on to emphasize how important those diagnostics were - and how smart and knowledgeable I was - because I was convinced that most people wouldn’t go the extra yard and would just stop at fitting the model! I went on about how I’d look into the results myself, and then get other people to look into them as well, and so on, until we were sure that everything was just right.\nWith hindsight and experience, I’ve realized that the answer I gave is probably a very common one coming from a junior data person, especially one just out of college or graduate school, who has done lots of learning about data science in the classroom or in a research setting. And if you’ve read the other posts in this series, you probably have a hunch for what this one is going to be about: What nuance was my answer missing?"
  },
  {
    "objectID": "posts/series/doing-data-science/2023-08-03-doing-data-science/doing-data-science.html#this-series",
    "href": "posts/series/doing-data-science/2023-08-03-doing-data-science/doing-data-science.html#this-series",
    "title": "On Doing Data Science",
    "section": "This Series",
    "text": "This Series\nTo a large degree, this series has made up a number of the pieces of the answer to that harmless interview question. When I was first starting out in data science, I wasn’t aware of any of these lesser-discussed but extremely valuable aspects of doing data science professionally. My answer had no mention of deploying my model to production so it could be used by others. It made no mention of testing the code. It made no mention of retraining the model using some kind of orchestration tool. It made no mention of monitoring the performance of the model out-of-sample to learn more about how it was doing on new data.\nI overlooked all of these aspects of the process because, at the time, I didn’t know about any of them. They weren’t part of my data scientific vocabulary. I had been taught lots in school and in internships about writing code, doing statistical tests, and the ins and outs of building machine learning models, but I had no exposure to the practical side of things."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-08-03-doing-data-science/doing-data-science.html#lessons-learned-along-the-way",
    "href": "posts/series/doing-data-science/2023-08-03-doing-data-science/doing-data-science.html#lessons-learned-along-the-way",
    "title": "On Doing Data Science",
    "section": "Lessons Learned Along the Way",
    "text": "Lessons Learned Along the Way\nIn addition to the other posts in this series which have gotten into some of the engineering nuts and bolts of shipping data products, I wanted to make part of this post a bit of a love letter to things I’ve learned along the way, especially by working in data at a start up for the past three years. There are lots of technical things that nobody taught me about doing data science, but there are also a few habits and soft skills I’ve picked up over the past few years from shipping data products that I think have been extremely important in my growth as a data scientist, and I’d like to share a few of them here.\n\nDelivering Business Value\nFirst, a meta-principle: Employees, in broad strokes, exist to deliver value for a business. This is true for data scientists just as it is for everyone else. And importantly: Products that are not being used are not currently delivering value.\nOften, I’ll see questions asked on r/DataScience or in person by junior data scientists and analysts asking what skills are the most important for them to learn. Generally, these questions focus on in the weeds technical skills and overlook the single most important skill, which is a bias towards delivering value. Of course, this is a soft skill.\nAnd with that in mind, a few additional principles I’ve learned and benefitted greatly from following along the way.\n\n\nKeep It Simple, Stupid\nOr Occam’s Razor, or whatever else you want to call it. In short: Opt for simple solutions over complicated ones.\nIn my answer to the interview question about the process I’d follow for completing an analytical project, I was focused on machine learning. I was also very much focused on getting the model just right. I wanted to avoid any footguns and make sure I covered all of my bases. This meant building out a whole model, a feature engineering pipeline, pulling other people in to do diagnostics, tweaking things, and doing it over. All of that takes time.\nThe “model” that I was so confidently going on about could have been something much simpler. Sometimes, the answer is to build a dashboard. Sometimes it’s so dump the results of a SQL query to a CSV. Sometimes it’s to use a heuristic. Sometimes it makes sense to use a simple logistic regression model.\nA lot of the time, we refer to these as “80% solutions,” meaning that they get you 80% of the way there. You might think of this as a twist on the 80-20 Rule, and interpret it as meaning that 20% of the effort can often get you 80% of the way to the optimal solution. Often, 80% is good enough.\n\n\nShip Fast, Learn, and Iterate\nA classic mistake that startups make that often causes them to fail is spending all the time and money they have trying to build the perfect product, only to watch it go unused once they release it. Instead, veterans like the friendly faces at Y Combinator generally advise startups to deliver a minimum viable product (MVP) and iterate. Generally, “minimum viable” means “bad.” The point is to put something, even something bad, in front of users to collect feedback from them.\nIn startup world, feedback from real users is like gold. The reality is that no matter how perfect you think your recommender system is, or how convincingly human-like your LLM’s response can be, or how accurate your NBA model’s predictions are, and so on, you cannot know if users will like your data product until it is in front of them. This means that in general, it’s a good idea to put a bad product in front of users quickly, in order to prove out the concept. And the inverse is also generally true: It’s a bad idea to tweak and tweak and tweak something to perfection in a dark room for six months without anyone seeing it. Of course, dedicated research projects and extremely high-stakes projects might be exceptions to this rule, but in general, the only way to find out if your project is something people actually want is to ship it.\nAfter you ship, you do two more things. First, you learn. What’s working? What’s not? Next, you iterate.\nA friend and coworker of mine likes describing the process of building a product as first building a skateboard, then building a bike, then building a car. The alternative would be trying to build the car from the get-go. You probably have a hunch that the car will be the best solution to your problem, but you can get the skateboard working more quickly, learn from mistakes you made in building it, and maybe, just maybe, it’ll end up getting you where you want to go. The skateboard in this analogy is very similar to the dashboard proposed above when discussing Occam’s Razor. In answering the interview question, I should have explained how to build a skateboard well as opposed to explaining how to half-ass the build of a car.\n\n\nLearning from Developers\nThe last major lesson I’ve learned in my past few years shipping data products is that as you’re working on analytical problems, you’ll eventually face obstacles that are new to you. For instance: How can I let someone else use my model? How can I effectively do code review? How can I deploy changes to my model without breaking anything?\nA few years ago, I would have tried to engineer a solution when faced with a problem like these. But now, my thought process usually goes something like: “Someone must have solved this problem before. How did they do it?” Usually, that someone is a software developer.\nDevs have been solving the nitty gritty problems that data people are just having their eyes opened to for decades. And so my usual bias when faced with a problem like “How do you organize your data projects? Do you use Jira?” is to try my best to emulate what the developers on my team do. For this particular case, that means thinking about how the entire product will work up front, and then laying out the work that it’ll take to ship that product in small chunks that can be delivered in less than two days. Sometimes this means making 20 cards (and, subsequently, 20 pull requests). This is a feature, not a bug.\nGenerally speaking, this instinct to ask a developer if they’ve solved a similar problem – or just to ask myself what others have done – has been an extremely high-leverage tool for me. Notably, it’s saved me lots of time and headaches that would have been caused by hacking together bad solutions to problems that others have already solved.\nSo in general: It’s rare that problems are genuinely novel, and asking myself what others have done when faced with similar ones has saved me lots of time and effort."
  },
  {
    "objectID": "posts/series/doing-data-science/2023-08-03-doing-data-science/doing-data-science.html#wrapping-up",
    "href": "posts/series/doing-data-science/2023-08-03-doing-data-science/doing-data-science.html#wrapping-up",
    "title": "On Doing Data Science",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nAnd with that, we’re at the end of my series on things you probably didn’t learn about data science before starting in the field. My goal in writing this series – and this post, in particular – was to share some tidbits about things I wish I’d learned earlier on and how I like to think about solving analytical problems, in an approachable way.\nThe goal of these posts was not to be in the technical weeds or to be exhaustive in any way. Instead, my hope is that this series will give data scientists – both present and future – a few more words in their vocabulary, and that some day, when faced with a dependency hell problem or a data pipeline problem they might think to use a tool like Poetry or remember the words “Workflow Orchestration.” I hope that I’ve shared enough here to point someone in the right direction or to give them a jumping off point, or to help fill in some keywords in a Google search.\nI also hope that some of these lessons – especially the notes on soft skills and ways I think about approaching data science problems – will be helpful to others who are, as I was, flying by the seats of their pants trying to figure out how to make decisions as a data scientist.\nLastly, I wanted to spend a sentence or two to thank everyone who’s picked up these posts, discussed them, and criticized them. It’s been fun to write things that seem to be meaningful and strike a chord, and I hope that some of these posts will have enough staying power to continue making any impact they can on the community."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-10-calling-a-b-tests/index.html",
    "href": "posts/series/a-b-testing/2022-04-10-calling-a-b-tests/index.html",
    "title": "Calling A/B Tests",
    "section": "",
    "text": "In the last post, I gave a bird’s eye level overview of the mechanics of running an A/B test. But at the end, we reached a problem: We had two conversion rates – 20% and 25% – but we didn’t know if the difference between those was really big enough to make a strong claim that the blue underlines were actually performing better than the red ones in some real world sense. If you’re asking yourself whether the five percentage point difference between the two conversion rates is statistically significant, then your head’s in the right place.\nIn this post, we’ll discuss how we can determine whether our test results are statistically significant. But since statistical significance is an often confusing and nebulous topic, we’ll also explore what statistical significance even is (including what p-values are), when it’s important, and when it might not be."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-10-calling-a-b-tests/index.html#statistical-significance",
    "href": "posts/series/a-b-testing/2022-04-10-calling-a-b-tests/index.html#statistical-significance",
    "title": "Calling A/B Tests",
    "section": "Statistical Significance",
    "text": "Statistical Significance\nMisunderstandings about statistical significance run rampant. It’s not a reach for me to say that the majority of the time I hear someone mention that something is “statistically significant” I end up rolling my eyes. But before we get into common mistakes and misunderstandings, we need to first establish what statistical significance actually is.\nIntuitively, if something is statistically significant, it’s unlikely to have happened due to random chance. Not that scary, after all! How unlikely, though, varies wildly depending on the setting. For instance, if we’re running clinical trials to determine if a new drug is capable of curing cancer, then we want it to be very unlikely that we make a consequential mistake and claim that the drug works when it actually doesn’t.\nWe use p-values as the indicator of the likelihood of our result being due to random chance. In this instance, we would run our test using the number of page views and the number of conversions for each group, and depending on how we ran our test we might get a p-value of 0.43% back. What this p-value actually means is that the probability of seeing the difference in conversion rates between groups that we do (five percentage points) due to purely random chance is 0.43%. A p-value threshold of 5% is very common, so in this case we’d call the test for the variant (since 0.43% is below 5%), and we’d assert that this difference in conversion rates is statistically significant."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-10-calling-a-b-tests/index.html#eye-rolling",
    "href": "posts/series/a-b-testing/2022-04-10-calling-a-b-tests/index.html#eye-rolling",
    "title": "Calling A/B Tests",
    "section": "Eye Rolling",
    "text": "Eye Rolling\nBack to my eye rolling: I often roll my eyes when someone claims that something is statistically significant for two reasons.\nFirst and foremost: Something being statistically significant does not mean that thing is significant. Often we get so hung up on things being statistically significant that we forget that lifting some metric by 0.0001% isn’t practically significant, since it won’t make any difference in the end. If 0.0001% more people read my blog posts, what do I care? That’s something like 1 extra person every hundred years (optimistically).\nSecondly, I often roll my eyes because of the number of choices and assumptions that need to be made along the way, many of which tend to be difficult to defend. One choice, as previously mentioned, is the p-value threshold (alpha) that you choose. In some instances, we want to be very confident that we’re not leaning into results that are the result of random chance, and so we might use a lower threshold. In other cases, we might be okay with taking on more risk of a false positive result in order to run our tests faster and mitigate the risk of a false negative (saying something does not help when it actually does).\nAnother thing that will affect the results we see is the type of test we’re running: one-tailed or two-tailed. Often, online calculators like this one will use two-tailed tests by default because they’re more conservative. But in my opinion, using a two-tailed test doesn’t actually make any sense. Here’s why: A two-tailed test checks if the conversion rates of the variant and the control are not equal, which means that we can get a statistically significant result if the variant is significantly worse than the control, in addition to if it’s significantly better. But in A/B testing, we’re only going to call a test for the variant when it’s significantly better, so why do we care about the case where it’s worse? We want to test the hypothesis that the variant is significantly better than the control, not that it’s not equal, and that’s what a one-tailed test does. If you use two-tailed tests, it’ll be harder to get significant results without any real benefits.\n\nYet another consideration is how the statistical test was actually conducted. For instance, if you use a Chi-square test with Yates’s continuity correction (the default in R, although a little controversial among statisticians), you’ll end up with higher (more conservative) p-values than if you don’t correct, which is why the p-value I just reported is higher than the one you’d get from most online calculators that don’t use the correction.\n\nFinally, and most importantly, is that the mechanics of running the test actually affect the chance that you are reporting a false positive result. For example, if you were to run the test described in the past few posts and calculate the p-values every time a new user visited the page and call the test for the variant the first time it were significant, you’d have just blown up the chances of a false positive result."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-10-calling-a-b-tests/index.html#a-common-mistake",
    "href": "posts/series/a-b-testing/2022-04-10-calling-a-b-tests/index.html#a-common-mistake",
    "title": "Calling A/B Tests",
    "section": "A Common Mistake",
    "text": "A Common Mistake\nThe most common mistake I see that’s made by people running A/B tests is using the “call it when it’s significant” heuristic. As I mentioned before, checking in on your test often and calling it for the variant the first time you get a significant p-value is a huge problem because the false positive rate of your test compounds the more you check on it. The reason for this is a statistical concept called multiple testing, and there’s an XKCD comic about it!\nSo we want to avoid checking the test all the time, but this raises another problem: If we can’t check our test all the time, how do we know when to call it? And this is where test planning comes in. There are a number of online test planners (which generally make shoddy assumptions, like that you’re running a two-tailed test when you should be running a one-tailed one instead) like this one that take a few parameters and tell you how long to run your test for. And these planners are great! The idea is that if you can plan your test in advance, given that you know your baseline conversion rate and can specify how big of a lift you’re shooting for, then all you have to do is wait until you hit the sample size number that the calculator gives you back. Once you hit it, you check in on your test, run your p-value calculation, and call the test.\nSo, problem solved, right? Well, not quite. Because while we’ve solved the multiple testing problem where we blow up our false positive rate by checking the test all the time, now we have a new issue: We have to wait until we hit some (potentially big) sample size before we can call our test, and that’s problematic for teams that want to iterate quickly.\nThe next post in this series is the punch line. It’ll discuss sequential testing, which is the methodology that makes up the guts of how we run A/B tests at CollegeVine. Sequential testing solves the problem of needing to wait until you hit a final sample size to call your test without making any sacrifices on the rigor front, which means you can call your tests quickly and reliably."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-17-sequential-testing/index.html",
    "href": "posts/series/a-b-testing/2022-04-17-sequential-testing/index.html",
    "title": "Sequential Testing",
    "section": "",
    "text": "The last post proposed a solution to the multiple testing problem that often invalidates A/B test results test planning. The idea is to calculate the sample sizes you need for your test in advance, and then wait for your control and variant groups to hit those sample sizes in order to call the test. This approach is a significant methodological improvement from the “call it when it’s significant” heuristic: It prevents you from compounding the false positive rate of your test by checking on it all the time.\nBut there’s a different issue with planning the test in advance and running it until the end: It’s slow. I use “slow” to mean “slower than it needs to be,” in the sense that you will likely end up waiting too long to call a test for the variant when you could’ve made the call earlier. This waiting around is expensive – the difference between running a test for a day or two and a week or two matters a lot for the teams and businesses running the tests. Often, this big of a time difference can have massive effects on metrics, revenue, learnings, etc., so teams benefit from being able to call their tests faster without sacrificing any statistical rigor.\nBut how do we do that without checking in on the test all the time? Enter sequential testing."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-17-sequential-testing/index.html#introduction",
    "href": "posts/series/a-b-testing/2022-04-17-sequential-testing/index.html#introduction",
    "title": "Sequential Testing",
    "section": "",
    "text": "The last post proposed a solution to the multiple testing problem that often invalidates A/B test results test planning. The idea is to calculate the sample sizes you need for your test in advance, and then wait for your control and variant groups to hit those sample sizes in order to call the test. This approach is a significant methodological improvement from the “call it when it’s significant” heuristic: It prevents you from compounding the false positive rate of your test by checking on it all the time.\nBut there’s a different issue with planning the test in advance and running it until the end: It’s slow. I use “slow” to mean “slower than it needs to be,” in the sense that you will likely end up waiting too long to call a test for the variant when you could’ve made the call earlier. This waiting around is expensive – the difference between running a test for a day or two and a week or two matters a lot for the teams and businesses running the tests. Often, this big of a time difference can have massive effects on metrics, revenue, learnings, etc., so teams benefit from being able to call their tests faster without sacrificing any statistical rigor.\nBut how do we do that without checking in on the test all the time? Enter sequential testing."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-17-sequential-testing/index.html#sequential-testing",
    "href": "posts/series/a-b-testing/2022-04-17-sequential-testing/index.html#sequential-testing",
    "title": "Sequential Testing",
    "section": "Sequential Testing",
    "text": "Sequential Testing\nSequential testing is a method for running experiments that allows us to evaluate the results of the test we’re running along the way instead of waiting to hit a pre-determined sample size. Intuitively, you might think about sequential testing like this: If early on in my test I see a massive lift in my metric, I should be able to use a lower p-value than the one I set at the start of my test to call it. It’s earlier, hence the lower p-value, but the intuitive idea is that the metric lift is so big that the p-value we’d see would be smaller than some yet undetermined p-value threshold, such that we could call the test.\nIn A/B testing world, this boils down to building checkpoints into our tests. For instance, imagine you have a test that you’re expecting to take six days to hit the final sample size that you need. If you build in three checkpoints, then you can check in on your test on day two, day four, and day six (the end of the test). On day two, if the p-value for your test is lower than the pre-determined day two p-value needed, you call the test. If it’s not, you move on to day four and repeat. Once you get to day six, if the test is still insignificant you call it for the control and end the test.\nThis gives us the best of both worlds: We have a setup where we can call the test on day two if the lift is big enough, but we can do so without inflating the false positive rate of our test. In practice, this means often being able to call tests in half, a third, a quarter, etc. of the time it’d otherwise take, which is hugely valuable for the team running the test.\n\nStatistical note: There are a number of ways to determine what p-value to use at each checkpoint when planning the test. We use the R package rpact for planning tests, and we plan our tests using the O’Brien-Fleming method (with alpha spending). This results in p-value thresholds that increase over time and asymptote to a value slightly less than the initial alpha you specified, depending on the number of checkpoints you build into your test. Another popular method is Pocock’s approach."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-17-sequential-testing/index.html#in-practice",
    "href": "posts/series/a-b-testing/2022-04-17-sequential-testing/index.html#in-practice",
    "title": "Sequential Testing",
    "section": "In practice",
    "text": "In practice\nSo how does this work in practice? We build an internal tool that lets you plan a test given a few inputs:\n\nThe alpha level (we generally use 20%, since we’re not particularly afraid of false positives and want to be able to run tests quickly)\nThe power (we generally use 95%, since we don’t want to take on many false negatives)\nThe minimum detectable effect\nThe baseline conversion rate\nThe expected number of users entering the funnel per day\nThe number of checkpoints to build in\nThe split of the test (is it 50/50?)\nThe number of variants (is it a true A/B test? Are there multiple variants being tested?)\n\nWith those inputs, we generate a test plan which you can save, tie to a JIRA card, and send to Slack. Then all you need to do is turn on your test and wait for it to hit the first checkpoint. Once it does, you evaluate the test to get a p-value, compare it to the p-value threshold that the test plan provided at the first checkpoint, and call the test if it’s significant. If it’s not, you keep running the test up to the next checkpoint and do the same thing, and so on."
  },
  {
    "objectID": "posts/series/a-b-testing/2022-04-17-sequential-testing/index.html#the-bottom-line",
    "href": "posts/series/a-b-testing/2022-04-17-sequential-testing/index.html#the-bottom-line",
    "title": "Sequential Testing",
    "section": "The Bottom Line",
    "text": "The Bottom Line\nThe main takeaway from this post is that sequential testing lets us solve two huge problems in A/B testing simultaneously: It lets us run our tests fast, and it lets us do it without sacrificing any statistical rigor. Too often, I see teams committing atrocities against statistics in the name of moving fast when they don’t need to be – using sequential designs for your A/B tests lets you control the false positive and false negative rates of your A/B tests while also allowing you to make calls on those tests as quickly as possible, which is hugely valuable.\nAnd with that, we’ve concluded a four-part series on A/B testing! Hopefully you found this interesting and useful, and have taken something away that will be beneficial for your own work. Or, if I’m lucky, maybe you’re even considering overhauling how you run A/B tests."
  },
  {
    "objectID": "posts/2021-06-08-working-with-your-fitbit-data-in-r/index.html#introduction",
    "href": "posts/2021-06-08-working-with-your-fitbit-data-in-r/index.html#introduction",
    "title": "Working With Your Fitbit Data in R",
    "section": "Introduction",
    "text": "Introduction\nfitbitr 0.1.0 is now available on CRAN! You can install it with\ninstall.packages(\"fitbitr\")\nor you can get the latest dev version with\n## install.packages(\"devtools\")\ndevtools::install_github(\"mrkaye97/fitbitr\")\nfitbitr makes it easy to pull your Fitbit data into R and use it for whatever interests you: personal projects, visualization, medical purposes, etc.\nThis post shows how you might use fitbitr to pull and visualize some of your data."
  },
  {
    "objectID": "posts/2021-06-08-working-with-your-fitbit-data-in-r/index.html#sleep",
    "href": "posts/2021-06-08-working-with-your-fitbit-data-in-r/index.html#sleep",
    "title": "Working With Your Fitbit Data in R",
    "section": "Sleep",
    "text": "Sleep\nFirst, you should either generate a new token with generate_token() or load a cached token with load_cached_token().\n\nlibrary(fitbitr)\nlibrary(lubridate)\nlibrary(tidyverse)\n\n## Dates to use throughout post\nstart &lt;- as_date(\"2020-01-01\")\nend &lt;- as_date(\"2021-10-18\")\n\ngenerate_fitbitr_token()\n\nAnd then you can start pulling your data!\n\nsleep &lt;- get_sleep_summary(\n  start_date = end - months(3),\n  end_date = end\n)\n\nhead(sleep)\n\n\n\n\nlog_id\ndate\nstart_time\nend_time\nduration\nefficiency\nminutes_to_fall_asleep\nminutes_asleep\nminutes_awake\nminutes_after_wakeup\ntime_in_bed\n\n\n\n\n34207402675\n2021-10-18\n2021-10-17 23:01:00\n2021-10-18 06:30:00\n26940000\n91\n0\n391\n58\n0\n449\n\n\n34193579435\n2021-10-17\n2021-10-16 23:03:30\n2021-10-17 08:11:00\n32820000\n95\n0\n472\n75\n4\n547\n\n\n34183584553\n2021-10-16\n2021-10-15 22:46:30\n2021-10-16 06:57:30\n29460000\n94\n0\n424\n67\n0\n491\n\n\n34174304493\n2021-10-15\n2021-10-14 23:50:00\n2021-10-15 08:20:30\n30600000\n94\n0\n438\n72\n0\n510\n\n\n34159751655\n2021-10-14\n2021-10-13 23:34:00\n2021-10-14 09:18:00\n35040000\n98\n0\n524\n60\n0\n584\n\n\n34146865838\n2021-10-13\n2021-10-12 23:50:00\n2021-10-13 08:32:30\n31320000\n94\n0\n461\n61\n1\n522\n\n\n\n\n\n\n\nOnce you’ve loaded some data, you can visualize it!\n\nlibrary(zoo)\nlibrary(scales)\nlibrary(ggthemes)\n\nsleep &lt;- sleep %&gt;%\n  mutate(\n   date = as_date(date),\n   start_time = as_datetime(start_time),\n   end_time = as_datetime(end_time),\n   sh = ifelse(hour(start_time) &lt; 8, hour(start_time) + 24, hour(start_time)), #create numeric times\n   sm = minute(start_time),\n   st = sh + sm/60,\n   eh = hour(end_time),\n   em = minute(end_time),\n   et = eh + em/60,\n   mst = rollmean(st, 7, fill = NA), #create moving averages\n   met = rollmean(et, 7, fill = NA),\n   year = year(start_time)\n)\n\nsleep %&gt;%\n    ggplot(aes(x = date)) +\n    geom_line(aes(y = et), color = 'coral', alpha = .3, na.rm = T) +\n    geom_line(aes(y = st), color = 'dodgerblue', alpha = .3, na.rm = T) +\n    geom_line(aes(y = met), color = 'coral', na.rm = T) +\n    geom_line(aes(y = mst), color = 'dodgerblue', na.rm = T) +\n    scale_y_continuous(\n      breaks = seq(0, 30, 2),\n      labels = trans_format(\n        function(x) ifelse(x &gt; 23, x - 24, x), \n        format = scales::comma_format(suffix = \":00\", accuracy = 1)\n      )\n    ) +\n    labs(x = \"Date\", y = 'Time') +\n    theme_fivethirtyeight() +\n    scale_x_date(date_breaks = '1 month', date_labels = '%b', expand = c(0, 0)) +\n    facet_grid(. ~ year, space = 'free', scales = 'free_x', switch = 'x') +\n    theme(panel.spacing.x = unit(0,\"line\"), strip.placement = \"outside\")\n\n\n\n\n\n\n\n\nThis bit of code makes a nicely formatted plot of the times you went to sleep and woke up over the past three months. You can also use fitbitr to expand the time window with a little help from purrr (the Fitbit API rate limits you, so you can’t request data for infinitely long windows in a single request).\n\n## Pull three months of data\nsleep &lt;- map_dfr(\n  3:0,\n  ~ sleep_summary(\n    end - months(.x), \n    end - months(.x) + months(1)\n  )\n)\n\nAfter pulling the data, we can use the same code again to visualize it.\n\nsleep &lt;- sleep %&gt;%\n  mutate(\n   date = as_date(date),\n   start_time = as_datetime(start_time),\n   end_time = as_datetime(end_time),\n   sh = ifelse(hour(start_time) &lt; 8, hour(start_time) + 24, hour(start_time)), #create numeric times\n   sm = minute(start_time),\n   st = sh + sm/60,\n   eh = hour(end_time),\n   em = minute(end_time),\n   et = eh + em/60,\n   mst = rollmean(st, 7, fill = NA), #create moving averages\n   met = rollmean(et, 7, fill = NA),\n   year = year(start_time)\n) %&gt;%\n  distinct()\n\nsleep %&gt;%\n    ggplot(aes(x = date)) +\n    geom_line(aes(y = et), color = 'coral', alpha = .3, na.rm = T) +\n    geom_line(aes(y = st), color = 'dodgerblue', alpha = .3, na.rm = T) +\n    geom_line(aes(y = met), color = 'coral', na.rm = T) +\n    geom_line(aes(y = mst), color = 'dodgerblue', na.rm = T) +\n    scale_y_continuous(\n      breaks = seq(0, 30, 2),\n      labels = trans_format(\n        function(x) ifelse(x &gt; 23, x - 24, x), \n        format = scales::comma_format(suffix = \":00\", accuracy = 1)\n      )\n    ) +\n    labs(x = \"Date\", y = 'Time') +\n    theme_fivethirtyeight() +\n  scale_x_date(date_breaks = '1 month', date_labels = '%b', expand = c(0, 0)) +\n  facet_grid(. ~ year, space = 'free', scales = 'free_x', switch = 'x') +\n  theme(panel.spacing.x = unit(0,\"line\"), strip.placement = \"outside\")"
  },
  {
    "objectID": "posts/2021-06-08-working-with-your-fitbit-data-in-r/index.html#heart-rate-and-steps",
    "href": "posts/2021-06-08-working-with-your-fitbit-data-in-r/index.html#heart-rate-and-steps",
    "title": "Working With Your Fitbit Data in R",
    "section": "Heart Rate and Steps",
    "text": "Heart Rate and Steps\nYou can also pull your heart rate data with fitbitr. Maybe we’re curious about seeing how the number of minutes spent in the “fat burn,” “cardio,” and “peak” zones correlates with the number of steps taken that day. Let’s find out!\n\nhr &lt;- map_dfr(\n  3:0,\n  ~ heart_rate_zones(\n    end - months(.x), \n    end - months(.x) + months(1)\n  )\n)\n\nsteps &lt;- map_dfr(\n  3:0,\n  ~ steps(\n    end - months(.x), \n    end - months(.x) + months(1)\n  )\n)\n\nFirst, we can examine the heart rate data:\n\nhead(hr)\n\n\n\n\ndate\nzone\nmin_hr\nmax_hr\nminutes_in_zone\ncalories_out\n\n\n\n\n2021-07-18\nOut of Range\n30\n113\n1440\n2530.16460\n\n\n2021-07-18\nFat Burn\n113\n141\n0\n0.00000\n\n\n2021-07-18\nCardio\n141\n176\n0\n0.00000\n\n\n2021-07-18\nPeak\n176\n220\n0\n0.00000\n\n\n2021-07-19\nOut of Range\n30\n113\n1408\n2689.45124\n\n\n2021-07-19\nFat Burn\n113\n141\n9\n86.59917\n\n\n\n\n\n\n\nand the steps data:\n\nhead(steps)\n\n\n\n\ndate\nsteps\n\n\n\n\n2021-07-18\n5620\n\n\n2021-07-19\n7537\n\n\n2021-07-20\n5513\n\n\n2021-07-21\n9014\n\n\n2021-07-22\n10883\n\n\n2021-07-23\n2975\n\n\n\n\n\n\n\nNow, let’s plot them against each other.\n\ndf &lt;- hr %&gt;%\n  filter(zone != \"Out of Range\") %&gt;%\n  group_by(date) %&gt;%\n  summarize(total_minutes = sum(minutes_in_zone), .groups = \"drop\") %&gt;%\n  inner_join(steps, by = \"date\")\n  \ndf %&gt;%\n  mutate(steps = as.numeric(steps)) %&gt;%\n  filter(log(total_minutes) &gt; 1) %&gt;%\n  ggplot(\n    aes(\n      steps,\n      total_minutes\n    )\n  ) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F) +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\n\n\n\n\nOr maybe it’d be interesting to predict your zone minutes from your steps:\n\npredictions &lt;- df %&gt;%\n  mutate(steps = as.numeric(steps)) %&gt;%\n  lm(total_minutes ~ steps, data = .) %&gt;%\n  broom::tidy() %&gt;%\n  mutate(across(where(is.numeric), round, 5))\n\nhead(predictions)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n23.09761\n5.77502\n3.99957\n0.00011\n\n\nsteps\n0.00252\n0.00056\n4.52922\n0.00001"
  },
  {
    "objectID": "posts/2021-06-08-working-with-your-fitbit-data-in-r/index.html#wrapping-up",
    "href": "posts/2021-06-08-working-with-your-fitbit-data-in-r/index.html#wrapping-up",
    "title": "Working With Your Fitbit Data in R",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nAnd that’s it! Hopefully this helped show how fitbitr makes pulling your data easy, and gets you curious about the insights you can glean from your own data. The Fitbit API gives you access to so much interesting information about yourself, your habits, your fitness, and so much more, and fitbitr is just meant to be a door into that gold mine."
  },
  {
    "objectID": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html",
    "href": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html",
    "title": "Deploying MLFlow on Heroku (with Heroku Postgres, S3, and nginx for basic auth)",
    "section": "",
    "text": "Disclaimer: I followed this guide to setting up MLFlow on Heroku initially. However, there were certain aspects of it that are either outdated or do not work, so this post remedies those issues."
  },
  {
    "objectID": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#mlflow",
    "href": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#mlflow",
    "title": "Deploying MLFlow on Heroku (with Heroku Postgres, S3, and nginx for basic auth)",
    "section": "MLFlow",
    "text": "MLFlow\nMLFlow is an open source tool for the entire machine learning lifecycle. It lets you create and experiment with models, write notes and descriptions, track parameters, metrics, and artifacts, and deploy to production all through an easy-to-use API and an intuitive UI. You can make calls to a running MLFlow service through the R API, the Python API, the Java API, or via the command line (cURL) or your favorite language by interacting with the REST API."
  },
  {
    "objectID": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#overview",
    "href": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#overview",
    "title": "Deploying MLFlow on Heroku (with Heroku Postgres, S3, and nginx for basic auth)",
    "section": "Overview",
    "text": "Overview\nMLFlow is surprisingly easy to set up and deploy to Heroku. There are a few steps to follow to get everything running: 1. Dockerize an MLFlow instance 2. Set up an artifact store 3. Set up a database 4. Secure the instance with basic auth"
  },
  {
    "objectID": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#prerequisites",
    "href": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#prerequisites",
    "title": "Deploying MLFlow on Heroku (with Heroku Postgres, S3, and nginx for basic auth)",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis guide assumes you already have AWS (specifically S3) set up. It also assumes some knowledge of shell scripting, Docker, and Heroku."
  },
  {
    "objectID": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#creating-a-heroku-app",
    "href": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#creating-a-heroku-app",
    "title": "Deploying MLFlow on Heroku (with Heroku Postgres, S3, and nginx for basic auth)",
    "section": "Creating a Heroku App",
    "text": "Creating a Heroku App\nFirst, let’s create a Heroku app from the CLI. For the purposes of this example, I’m going to call the app my-mlflow-example. You’ll need to choose your own app name.\nheroku create my-mlflow-example\nNext, let’s attach a Heroku Postgres instance.\nheroku addons:create heroku-postgresql:hobby-dev --app my-mlflow-example\nCreating this hobby-dev Heroku Postgres instance will also automatically set the DATABASE_URL environment variable in your app’s configuration.\nNext, we’ll need some other environment variables to be set. You can set your config like this:\nheroku config:set \\\n  S3_URI=s3://YOUR-S3-URI \\\n  AWS_SECRET_ACCESS_KEY=YOUR-SECRET \\\n  AWS_ACCESS_KEY_ID=YOUR-KEY \\\n  AWS_DEFAULT_REGION=YOUR-REGION \\\n  MLFLOW_TRACKING_USERNAME=YOUR-USERNAME \\\n  MLFLOW_TRACKING_PASSWORD=YOUR-PASSWORD \\\n  --app my-mlflow-example\nYou can repeat fewer lines of code by creating a .env file that looks like this:\nS3_URI=s3://YOUR-S3-URI AWS_SECRET_ACCESS_KEY=YOUR-SECRET ...\nand then using heroku config:set .env --app my-mlflow-example.\nGreat! At this point, your Heroku app should be all set up. Now all we need to do is create the Docker image that will run MLFlow."
  },
  {
    "objectID": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#setup",
    "href": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#setup",
    "title": "Deploying MLFlow on Heroku (with Heroku Postgres, S3, and nginx for basic auth)",
    "section": "Setup",
    "text": "Setup\nFirst thing’s first, let’s create a folder called my-mlflow-example where we’ll store all the files we’ll need. I’ll do that with:\nmkdir my-mlflow-example && cd my-mlflow-example\nNext, let’s create a few files we’ll need:\ntouch Dockerfile run.sh requirements.txt nginx.conf_template\nYou’ll also want to make your run.sh script executable.\nchmod +x run.sh\nGreat, now we’ve to all the files we’ll need to deploy our MLFlow instance!\n\nThe Dockerfile\nThe Dockerfile will contain all of the library installs and files you need to run your MLFlow instance. The Dockerfile you’ll need to write for MLFlow should look something like this:\nFROM continuumio/miniconda3\n\n## Copy files into the image\nCOPY run.sh run.sh\nCOPY requirements.txt requirements.txt\nCOPY nginx.conf_template /etc/nginx/sites-available/default/nginx.conf_template\n\n## Install Postgres\nRUN apt-get -y update && \\\n  apt-get -y upgrade && \\\n  apt-get install -y postgresql\n\n## Install nginx and dependencies\nRUN apt-get -y update && \\\n  apt-get install -y make vim \\\n  automake gcc g++ subversion \\\n  musl-dev nginx gettext apache2-utils\n\n## Install pip and dependencies\nRUN conda install -c anaconda pip && \\\n  pip install --upgrade pip && \\\n  pip install -r requirements.txt && \\\n  conda update -n base -c defaults conda && \\\n  conda env list && \\\n  pip freeze list\n\n## Run your `run.sh` script on container boot\nCMD ./run.sh\n\n\nThe requirements.txt File\nNext, copy the following lines into your requirements.txt:\nmlflow\npsycopg2-binary\nboto3\nThis file will tell pip which libraries to install in your docker image in the RUN pip install -r requirements.txt line above.\n\n\nThe nginx Template\nNext, we’ll create a template for the nginx.conf file that we’ll eventually use in the container for basic auth. One important issue here: This file is a template because Heroku randomly assigns a port on dyno start, which means we can’t hard code any ports for nginx (since we don’t know them ahead of time). Instead of hard-coding, we specify a couple of placeholders: $HEROKU_PORT and $MLFLOW_PORT. We’ll replace these with the proper ports on container startup.\nYour nginx.conf_template should look like this:\nevents {}\nhttp {\n  server {\n        listen $HEROKU_PORT;\n\n        access_log /var/log/nginx/reverse-access.log;\n        error_log /var/log/nginx/reverse-error.log;\n\n        location / {\n            auth_basic \"Restricted Content\";\n            auth_basic_user_file /etc/nginx/.htpasswd;\n\n            proxy_pass                          http://127.0.0.1:$MLFLOW_PORT/;\n            proxy_set_header Host               $host;\n            proxy_set_header X-Real-IP          $remote_addr;\n            proxy_set_header X-Forwarded-For    $proxy_add_x_forwarded_for;\n        }\n    }\n}\n\n\nRun Script\nFinally, let’s create a script, run.sh, which will run when the Heroku dyno starts.\nexport HEROKU_PORT=$(echo \"$PORT\")\nexport MLFLOW_PORT=5000\n\nenvsubst '$HEROKU_PORT,$MLFLOW_PORT' &lt; /etc/nginx/sites-available/default/nginx.conf_template &gt; /etc/nginx/sites-available/default/nginx.conf\n\nhtpasswd -bc /etc/nginx/.htpasswd $MLFLOW_TRACKING_USERNAME $MLFLOW_TRACKING_PASSWORD\n\nkillall nginx\n\nmlflow ui \\\n  --port $MLFLOW_PORT \\\n  --host 127.0.0.1 \\\n  --backend-store-uri $(echo \"$DATABASE_URL\" | sed \"s/postgres/postgresql/\") \\\n  --default-artifact-root $S3_URI &\n\nnginx -g 'daemon off;' -c /etc/nginx/sites-available/default/nginx.conf\nThere are a few things happening here: 1. We set the HEROKU_PORT environment variable from the randomly assigned PORT that Heroku creates on dyno startup 2. We assign MLFLOW_PORT to 5000. The chances that Heroku assigns exactly port 5000 are low. If you want, you can add a few more lines to first check if Heroku assigns 5000 as the port, and if it does, choose any other port instead (e.g. 1000). 3. We substitute the port placeholder variables in our nginx.conf_template file with the actual environment variable values for the ports, so that nginx knows where to listen and direct traffic. 4. We create a new .htpasswd file from the MLFLOW_TRACKING_USERNAME and MLFLOW_TRACKING_PASSWORD environment variables that we set in the Heroku config. This file will be used by nginx to check inputted usernames and passwords against. 5. We start the MLFlow UI in the background, telling it to run on the MLFLOW_PORT variable we created, and pointing it to our Heroku Postgres instance for the backend store and our S3 bucket for artifact storage. Note: By default, Heroku Postgres provides a URL that begins with postgres. This is not compatible with SQLAlchemy, so we substitute postgresql (which is compatible) for postgres. 6. We start nginx for basic auth."
  },
  {
    "objectID": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#deploying",
    "href": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#deploying",
    "title": "Deploying MLFlow on Heroku (with Heroku Postgres, S3, and nginx for basic auth)",
    "section": "Deploying",
    "text": "Deploying\nDeployment is simple, and can be done with a few lines of bash:\nheroku login\nheroku container:login\nheroku container:push web --app my-mlflow-example\nheroku container:release web --app my-mlflow-example\nAnd that’s it! Running those four lines should build your Docker image, push it to Heroku, and release it as your app. Once it releases, Heroku will boot up a dyno and you should be able to go to my-mlflow-example.herokuapp.com and see the MLFlow UI."
  },
  {
    "objectID": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#using-mlflow",
    "href": "posts/2021-12-26-deploying-mlflow-on-heroku-with-heroku-postgres-s3-and-nginx-for-basic-auth/index.html#using-mlflow",
    "title": "Deploying MLFlow on Heroku (with Heroku Postgres, S3, and nginx for basic auth)",
    "section": "Using MLFlow",
    "text": "Using MLFlow\nNow that your instance is deployed, you should have no problem using MLFlow for your whole ML lifecycle. For example, in R you might want to create a new experiment. You could do something like this:\n## install.packages(\"mlflow\")\n\nlibrary(mlflow)\n\n## Set up some environment variables\n## This way, your R session will know where to\n##  look for your MLFlow instance and will have\n##  the proper credentials set up\nSys.setenv(\n  MLFLOW_TRACKING_URI = \"https://my-mlflow-example.herokuapp.com\"\n  MLFLOW_TRACKING_USERNAME = \"YOUR-USERNAME\",\n  MLFLOW_TRACKING_PASSWORD = \"YOUR-PASSWORD\"\n)\n\nmlflow_create_experiment(\"my-first-experiment\")\nAnd with that, you should be able to harness all of the awesome power of MLFlow for all of your ML lifecycle needs!"
  },
  {
    "objectID": "posts/2021-02-07-what-s-new-in-slackr-2-1-0/index.html",
    "href": "posts/2021-02-07-what-s-new-in-slackr-2-1-0/index.html",
    "title": "What’s New in slackr 2.1.0",
    "section": "",
    "text": "slackr 2.1.0+ is live! There are a whole bunch of exciting changes that we (mostly Andrie de Vries and I) have made to improve the package a bunch."
  },
  {
    "objectID": "posts/2021-02-07-what-s-new-in-slackr-2-1-0/index.html#introduction",
    "href": "posts/2021-02-07-what-s-new-in-slackr-2-1-0/index.html#introduction",
    "title": "What’s New in slackr 2.1.0",
    "section": "",
    "text": "slackr 2.1.0+ is live! There are a whole bunch of exciting changes that we (mostly Andrie de Vries and I) have made to improve the package a bunch."
  },
  {
    "objectID": "posts/2021-02-07-what-s-new-in-slackr-2-1-0/index.html#changes",
    "href": "posts/2021-02-07-what-s-new-in-slackr-2-1-0/index.html#changes",
    "title": "What’s New in slackr 2.1.0",
    "section": "Changes",
    "text": "Changes\nHere are some of the things that are new in slackr 2.1.0+. For more info on the package, check out the Github repo and the pkgdown site.\n\nEase of Use Improvements\n\nWe’ve dramatically improved error messaging, so long gone are the days of errors like No 'id' column found in 'x'! Now, error messages should be far more helpful, with some hints about what might be going wrong.\nWe’ve updated the package documentation significantly, so now there’s a far more informative README, some vignettes, and a pkgdown site.\nWe’ve more clearly described the different use cases for slackr, in order to better help users set up slackr in a way that makes sense for them.\n\n\n\nNew Features\n\nWe’ve fixed a bunch of bugs that were preventing things like icon_emoji and username from working, so those are fixed now!\nWe’ve brought back some old functions that were removed in slackr 2.0.0: slackr_history() and slackr_delete(). See the docs for descriptions of what these functions can do.\n\n\n\nBack-End Changes\nWe’ve made a ton of changes for how slackr interacts with the Slack API:\n\nWe now allow paging, which is especially helpful when you have a workspace of more than 1000 channels.\nWe cache requests to get lists of channels and users so that we don’t need to repeat common API calls. This speeds up calls to slackr_***() and limits how often you need to actually hit the API.\nWe’ve gotten rid of a really nasty implementation of channel caching (writing a local cache to the disk) in favor of the method described above.\nWe’ve factored out API calls into a separate function, which makes the package easier to understand and test.\nSpeaking of testing, we’ve implemented a whole bunch of unit tests, and will be working on more.\n\n\n\nDeprecations\n\nWe’ve deprecated a bunch of camel case functions in favor of their snake case counterparts for simplicity. Don’t worry! These are soft-deprecated for now. They won’t go away fully until a future version of slackr\nWe’ve deprecated text_slackr in favor of slackr_msg, since they do basically the same thing."
  },
  {
    "objectID": "posts/2023-03-25-balancing-classes/balancing-classes.html",
    "href": "posts/2023-03-25-balancing-classes/balancing-classes.html",
    "title": "Balancing Classes in Classification Problems",
    "section": "",
    "text": "In my last post I wrote about common classifications metrics and, especially, calibration.\nWith calibration in mind, this post will show why balancing your classes – which is an all-too-common practice when working on classification problems – is generally a bad idea and leads to poorly calibrated models."
  },
  {
    "objectID": "posts/2023-03-25-balancing-classes/balancing-classes.html#introduction",
    "href": "posts/2023-03-25-balancing-classes/balancing-classes.html#introduction",
    "title": "Balancing Classes in Classification Problems",
    "section": "",
    "text": "In my last post I wrote about common classifications metrics and, especially, calibration.\nWith calibration in mind, this post will show why balancing your classes – which is an all-too-common practice when working on classification problems – is generally a bad idea and leads to poorly calibrated models."
  },
  {
    "objectID": "posts/2023-03-25-balancing-classes/balancing-classes.html#some-example-data",
    "href": "posts/2023-03-25-balancing-classes/balancing-classes.html#some-example-data",
    "title": "Balancing Classes in Classification Problems",
    "section": "Some Example Data",
    "text": "Some Example Data\nFor the purposes of this example, I’ll use the Wisconsin breast cancer data. The data is built into the mlbench package in R and scikit-learn in python. You can also get it from the UCI Machine Learning Repository.\nI’ll only be using cl_thickness, which is the indicator for clump thickness.\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(janitor)\nlibrary(purrr)\nlibrary(mlbench)\n\ndata(BreastCancer)\n\ndata &lt;- BreastCancer %&gt;%\n  clean_names() %&gt;%\n  transmute(\n    cl_thickness = as.numeric(cl_thickness), \n    class\n  ) %&gt;%\n  as_tibble()\n\ndata %&gt;%\n  slice_sample(n = 5) %&gt;%\n  pretty_print()\n\n\n\n\ncl_thickness\nclass\n\n\n\n\n5\nbenign\n\n\n5\nbenign\n\n\n4\nbenign\n\n\n6\nmalignant\n\n\n1\nbenign\n\n\n\n\n\n\n\nThe data is imbalanced: There are far more (about 2x) benign tumors than malignant ones in the sample.\n\ndata %&gt;%\n  count(class) %&gt;%\n  mutate(prop = n / sum(n)) %&gt;%\n  pretty_print()\n\n\n\n\nclass\nn\nprop\n\n\n\n\nbenign\n458\n0.6552217\n\n\nmalignant\n241\n0.3447783"
  },
  {
    "objectID": "posts/2023-03-25-balancing-classes/balancing-classes.html#model-fitting",
    "href": "posts/2023-03-25-balancing-classes/balancing-classes.html#model-fitting",
    "title": "Balancing Classes in Classification Problems",
    "section": "Model Fitting",
    "text": "Model Fitting\nWith that class imbalance in mind, let’s get to model fitting. The first thing I’ll do is fit a simple logistic regression model to predict the class (either malignant or benign) from the clump thickness.\nFirst, I’ve written a bit of tidymodels helper code below for reuse later.\n\nlibrary(tidymodels)\n\nfit_model &lt;- function(data, spec) {\n  spec &lt;- set_mode(spec, \"classification\")\n  \n  rec &lt;- recipe(\n    class ~ cl_thickness,\n    data = data\n  )\n  \n  wf &lt;- workflow() %&gt;%\n    add_model(spec) %&gt;%\n    add_recipe(rec)\n  \n  fit(wf, data)\n}\n\npredict_prob &lt;- function(model, data) {\n  predict(model, data, type = \"prob\")$.pred_malignant\n}\n\nNow, I’ll fit a simple logistic regression model by specifying logistic_reg() as the model specification in fit_model().\n\nlibrary(probably)\n\nunbalanced_model &lt;- fit_model(\n  data,\n  logistic_reg()\n)\n\npreds &lt;- tibble(\n  truth = data$class,\n  truth_int = as.integer(data$class) - 1,\n  estimate = predict_prob(unbalanced_model, data)\n)\n\nAnd now we can make a calibration plot of our predictions. Remember, the goal is to have the points on the plot lie roughly along the line y = x. Lying below the line means that our predictions are too high, and above the line means our predictions are too low.\n\ncal_plot_breaks(preds, truth = truth, estimate = estimate, event_level = \"second\")\n\n\n\n\nAwesome! Even with the class imbalance, our model’s probability predictions are well-calibrated. In other words, when we predict that there’s a 25% chance that a tumor is malignant, it’s actually malignant about 25% of the time."
  },
  {
    "objectID": "posts/2023-03-25-balancing-classes/balancing-classes.html#balancing-the-training-data",
    "href": "posts/2023-03-25-balancing-classes/balancing-classes.html#balancing-the-training-data",
    "title": "Balancing Classes in Classification Problems",
    "section": "Balancing the Training Data",
    "text": "Balancing the Training Data\nSo then, what happens if we balance the training data as we’re so often told to do? First, let’s balance by undersampling from the majority class.\n\nminority_class &lt;- data %&gt;%\n  count(class) %&gt;%\n  filter(n == min(n))\n\nbalanced &lt;- data %&gt;%\n  group_split(class, .keep = TRUE) %&gt;%\n  map_dfr(\n    ~ {\n      if (.x$class[1] == minority_class$class) {\n        .x\n      } else {\n        slice_sample(\n          .x,\n          n = minority_class$n,\n          replace = FALSE\n        )\n      }\n    }\n  )\n\nbalanced %&gt;%\n  count(class) %&gt;%\n  pretty_print()\n\n\n\n\nclass\nn\n\n\n\n\nbenign\n241\n\n\nmalignant\n241\n\n\n\n\n\n\n\nNow we have the same number of observations for each class. Let’s go ahead and fit another logistic regression model, but this time on the balanced data.\n\nbalanced_model &lt;- fit_model(balanced, logistic_reg())\n\npreds$balanced_preds &lt;- predict_prob(\n  balanced_model,\n  data\n)\n\ncal_plot_breaks(preds, truth = truth, estimate = balanced_preds, event_level = \"second\")\n\n\n\n\nAll of a sudden, our model is very poorly calibrated. We’re consistently overpredicting the probability of a tumor being malignant. Why is that? Think back to what we just did: We removed a bunch of examples of benign tumors from our training data.\nLet’s think about that from first principles for a minute. If you had no information at all, a reasonable guess for whether or not a tumor is malignant would be the overall proportion of tumors that are malignant. In the unbalanced data, that number was about 34%. But after balancing, it’s now 50%. That means that we’ve just biased our “no-information” prediction upwards by about 16 percentage points (or 50%). And so it shouldn’t be surprising that in our calibration plot above, we see that we’re consistently over-predicting. Our probabilities are too high because the baseline rate at which the true class appears in our training data has just increased significantly.\n\nAn important note which I’ll circle back to later is that this intuition about a baseline guess is directly rated to the intercept term of the logistic regression model you fit."
  },
  {
    "objectID": "posts/2023-03-25-balancing-classes/balancing-classes.html#smote",
    "href": "posts/2023-03-25-balancing-classes/balancing-classes.html#smote",
    "title": "Balancing Classes in Classification Problems",
    "section": "SMOTE",
    "text": "SMOTE\n“But no!” you might be thinking. “Why would you just undersample directly? You’re supposed to use an algorithm like SMOTE to overcome your class imbalance problem.”\nGreat! Let’s see if using SMOTE fixes our calibration issues. I’ll first use SMOTE to intelligently oversample the minority class.\n\nlibrary(themis)\n\nsmote &lt;- recipe(class ~ cl_thickness, data = data) %&gt;%\n  step_smote(class) %&gt;%\n  prep() %&gt;%\n  bake(new_data = NULL)\n\nsmote %&gt;%\n  count(class) %&gt;%\n  pretty_print()\n\n\n\n\nclass\nn\n\n\n\n\nbenign\n458\n\n\nmalignant\n458\n\n\n\n\n\n\n\nNow that we have balanced classes thanks to SMOTE, let’s fit another logistic regresion model and see if it’s any better-calibrated.\n\nsmote_model &lt;- fit_model(smote, logistic_reg())\n\npreds$smote_preds &lt;- predict_prob(\n  smote_model,\n  data\n)\n\ncal_plot_breaks(preds, truth = truth, estimate = smote_preds, event_level = \"second\")\n\n\n\n\nInteresting – same problem. With SMOTE, we still make very similar errors to the ones we made in the case where we naively undersampled from our majority class. But let’s think back to first principles again, because the exact same rationale applies. When we undersampled, we ended up artificially increasing the baseline rate of malignant tumors in our training data, which resulted in predictions that were too high. With SMOTE, we’re doing the exact same thing: We’ve stil rebalanced our data to 50/50, we’ve just done it a fancier way. So of course we’ll have the same problem with overprediction."
  },
  {
    "objectID": "posts/2023-03-25-balancing-classes/balancing-classes.html#a-random-forest",
    "href": "posts/2023-03-25-balancing-classes/balancing-classes.html#a-random-forest",
    "title": "Balancing Classes in Classification Problems",
    "section": "A Random Forest",
    "text": "A Random Forest\n“But no!” you might be thinking. “You need to use a more complicated model like a random forest, because logistic regression won’t pick up on complexities in your data well enough to be well-calibrated.”\nGreat! Let’s try a random forest:\n\nlibrary(ranger)\n\nrf &lt;- fit_model(smote, rand_forest())\n\npreds$rf_preds &lt;- predict_prob(\n  rf,\n  data\n)\n\ncal_plot_breaks(preds, truth = truth, estimate = rf_preds, event_level = \"second\")\n\n\n\n\nSame issue again, albeit not quite as severe. And the same logic holds. In fact, it’s even more straightforward with tree-based models. In a decision tree, you would determine a predicted probability by seeing what proportion of the labels in the leaf node that you end up in based on the features belong to the positive class. But then there’s the same logic as before: We’ve just artifically increased the number of instances of the positive class dramatically, so of course the proportion of labels belonging to the positive class in our leaf nodes will increase."
  },
  {
    "objectID": "posts/2023-03-25-balancing-classes/balancing-classes.html#coefficients",
    "href": "posts/2023-03-25-balancing-classes/balancing-classes.html#coefficients",
    "title": "Balancing Classes in Classification Problems",
    "section": "Coefficients",
    "text": "Coefficients\nLooking at the coefficients of our three models can help understand what’s going on here.\n\nlist(\n  \"Original\" = unbalanced_model, \n  \"Undersampled\" = balanced_model, \n  \"Smote\" = smote_model\n) %&gt;%\n  map(tidy) %&gt;%\n  imap(~ select(.x, \"Term\" = term, !!.y := \"estimate\")) %&gt;%\n  reduce(inner_join, by = \"Term\") %&gt;%\n  pretty_print()\n\n\n\n\nTerm\nOriginal\nUndersampled\nSmote\n\n\n\n\n(Intercept)\n-5.1601677\n-4.4911164\n-4.4639646\n\n\ncl_thickness\n0.9354593\n0.9400247\n0.9173742\n\n\n\n\n\n\n\nIn all three models, the coefficient associated with the clump thickness is very similar. This should make sense intuitively: Our sampling was at random, so the relationship between the clump thickness and whether or not the tumor was malignant shouldn’t change at all.\nThe thing that does change, though, is the intercept term. In both the model where we undersampled from the majority class and in the SMOTE model, the intercept term is significantly higher than it is in the original model on the unbalanced data. This should feel similar intuitively to the idea of the baseline guess from before. As its core, the intercept term in your logistic regression model is the guess you’d make with “no” information (in this particular case, no information means a clump thickness of 0).\nWe can illustrate this more clearly with three intercept-only models:\n\nunbalanced_intercept_only &lt;-  glm(class ~ 1, data = data, family = binomial)\nundersampled_intecept_only &lt;- glm(class ~ 1, data = balanced, family = binomial)\nsmote_intercept_only &lt;-       glm(class ~ 1, data = smote, family = binomial)\n\nNow, let’s compare the intercept coefficients of these three models on a probability (read: not a log-odds) scale.\n\nconvert_log_odds_to_probability &lt;- function(x) {\n  odds &lt;- exp(x)\n  odds / (1 + odds)\n}\n\nunbalanced_intercept &lt;-   coef(unbalanced_intercept_only)\nundersampled_intercept &lt;- coef(undersampled_intecept_only)\nsmote_intercept &lt;-        coef(smote_intercept_only)\n\nintercepts &lt;- tibble(\n  original =     convert_log_odds_to_probability(unbalanced_intercept),\n  undersampled = convert_log_odds_to_probability(undersampled_intercept),\n  smote =        convert_log_odds_to_probability(smote_intercept)\n)\n\npretty_print(intercepts)\n\n\n\n\noriginal\nundersampled\nsmote\n\n\n\n\n0.3447783\n0.5\n0.5\n\n\n\n\n\n\n\nAnd it’s just as we expected: The intercept coefficient in the SMOTE model and the undersampling model are exactly 1/2, which corresponds to the fact that we balanced the classes to be exactly 50/50. And the intercept in the original model with the unbalanced classes is exactly the percentage of the data made up by the true class (malignant)."
  },
  {
    "objectID": "posts/2023-03-25-balancing-classes/balancing-classes.html#when-to-rebalance",
    "href": "posts/2023-03-25-balancing-classes/balancing-classes.html#when-to-rebalance",
    "title": "Balancing Classes in Classification Problems",
    "section": "When To Rebalance",
    "text": "When To Rebalance\nThere are some times where re-balancing the classes in your training data might make sense. One application that comes to mind is if you have strong prior information that your training data is actually biased, and is over-representing one of the two classes.\nFor instance, let’s imagine we have data from 1000 breast cancer patients and we know a priori that about 20% of tumors are malignant, but in the training data, maybe 40% of the tumors we have are malignant. Depending on the long-term goal of the project, it might make sense to undersample from the malignant cases to get the overall rate of tumors being malignant down to around the 20% prior.\nThe rationale behind doing this would be that if you wanted your model to generalize well to future cases (outside of your training set) and you knew that in the broader population about 20% of cases are malignant, your biased training data could very well result in biased predictions out-of-sample even if your predictions look good in-sample.\nAnother case where rebalancing can make sense is if you plan to use a technique like Platt Scaling or Isotonic Regression to re-calibrate your predictions ex-post. These methods are a bit beyond the scope of this post, but they’re both fantastic ways to make sure a model is giving well-calibrated predictions while using classifiers that don’t guarantee calibration, such as tree-based models. For instance, a combination of upsampling with SMOTE, using an often poorly-calibrated classifier like a tree booster such as XGBoost, and then re-calibrating ex-post with a Platt Scaler can result in a rare win-win scenario: Well-calibrated predictions, but also improved performance on normal classification metrics like the F1 score or AUC."
  },
  {
    "objectID": "posts/2023-03-25-balancing-classes/balancing-classes.html#conclusion",
    "href": "posts/2023-03-25-balancing-classes/balancing-classes.html#conclusion",
    "title": "Balancing Classes in Classification Problems",
    "section": "Conclusion",
    "text": "Conclusion\nIn the previous post, I wrote about calibration. In short, I think calibration is the single most important metric in evaluating the performance of classification models.\nAnd so with that in mind, the main takeaway of this post is that you should be very careful about trying to “fix” the “problem” of class imbalances when you’re working on classification problems. If I could summarize the principle that I would follow in just a sentence, it would be that class imbalances often reflect important information about the prevalence of your classes in the real world, and it’s often risky to dismiss that information in the name of having data that’s split equally. In other words, class imbalances are usually not a problem at all. They’re a feature, not a bug."
  },
  {
    "objectID": "posts/2023-07-09-unit-testing-dbt-models/unit-testing-dbt-models.html",
    "href": "posts/2023-07-09-unit-testing-dbt-models/unit-testing-dbt-models.html",
    "title": "Unit Testing dbt Models",
    "section": "",
    "text": "Our team adopted dbt about a year ago, and it’s become an integral part of our data stack. dbt is a major component of the so-called “modern data stack” and has exploded onto the scene in the past few years.\nThe basic gist is this: You use an ELT tool like Fivetran or Airbyte to ingest raw data from any number of sources – think a Postgres database, a Salesforce instance, Segment or a similar analytics platform, or a product management tool like Asana, among many, many others – into your data warehouse (Redshift, Snowflake, etc.). Then, you use dbt to transform the raw data in the warehouse into a format that’s friendly for downstream users like data scientists and analysts, so that they can generate insights more quickly and with a higher degree of confidence.\ndbt is the transformation tool (the “T” in “ELT”) in that stack: It runs SQL inside of your data warehouse and abstracts away the often boilerplate DDL code, so that you can focus on writing transformations as SELECT statements and spend your time on driving business value. dbt also allows you to easily write documentation and tests, write macros for often-reused code, and so on. The testing bit of dbt is what this post will focus on. In short, dbt sells itself as a tool that lets you bring software engineering best practices to ELT pipelines.\n\n\n\n\n\n\nIf you’re not familiar with dbt, I would recommend reading some of their documentation before continuing this post. It will not cover many dbt basics."
  },
  {
    "objectID": "posts/2023-07-09-unit-testing-dbt-models/unit-testing-dbt-models.html#introduction",
    "href": "posts/2023-07-09-unit-testing-dbt-models/unit-testing-dbt-models.html#introduction",
    "title": "Unit Testing dbt Models",
    "section": "",
    "text": "Our team adopted dbt about a year ago, and it’s become an integral part of our data stack. dbt is a major component of the so-called “modern data stack” and has exploded onto the scene in the past few years.\nThe basic gist is this: You use an ELT tool like Fivetran or Airbyte to ingest raw data from any number of sources – think a Postgres database, a Salesforce instance, Segment or a similar analytics platform, or a product management tool like Asana, among many, many others – into your data warehouse (Redshift, Snowflake, etc.). Then, you use dbt to transform the raw data in the warehouse into a format that’s friendly for downstream users like data scientists and analysts, so that they can generate insights more quickly and with a higher degree of confidence.\ndbt is the transformation tool (the “T” in “ELT”) in that stack: It runs SQL inside of your data warehouse and abstracts away the often boilerplate DDL code, so that you can focus on writing transformations as SELECT statements and spend your time on driving business value. dbt also allows you to easily write documentation and tests, write macros for often-reused code, and so on. The testing bit of dbt is what this post will focus on. In short, dbt sells itself as a tool that lets you bring software engineering best practices to ELT pipelines.\n\n\n\n\n\n\nIf you’re not familiar with dbt, I would recommend reading some of their documentation before continuing this post. It will not cover many dbt basics."
  },
  {
    "objectID": "posts/2023-07-09-unit-testing-dbt-models/unit-testing-dbt-models.html#tests-in-dbt",
    "href": "posts/2023-07-09-unit-testing-dbt-models/unit-testing-dbt-models.html#tests-in-dbt",
    "title": "Unit Testing dbt Models",
    "section": "Tests in dbt",
    "text": "Tests in dbt\ndbt has lots of support for tests. It ships with a few – uniqueness, not-null, enums, and “relationships” (which test foreign key relationships) – out of the box. All you need to do is add a line like the following to one of your schema.yml files where a model is defined, as follows:\n## schema.yml\n\nversion: 2\n\nmodels:\n  - name: fct_pageviews\n    columns:\n      - name: event_id\n        tests:\n          - unique\n          - not_null\n      - name: user_id\n        tests:\n          - not_null\n      ...\nThis bit of yaml does a few things: It defines a table called fct_pageviews, that has at least two columns: the event_id and the user_id. The event_id column has both a unique and a not_null test, and the user_id also must be non-null (but not unique, since users can view many pages).\nThese tests are great, since they enforce a number of data quality expectations that we want to make sure are followed when shipping data to production. Since dbt generates a DAG from the tests and models you define, you can run these tests upstream of production and catch any failures before they hit production.\ndbt makes it easy to write arbitrary tests as macros, which you can then add to a yaml file to apply to a model. You can also use a package like dbt-expectations to greatly expand upon dbt’s built-in testing capabilities. In conjunction, all of these testing capabilities should give you lots of confidence that the data you’re shipping to production meets virtually any data quality bar you can set.\nBut this post doesn’t end here, so what’s wrong? It’s simple: These tests, useful as they may be, are not unit tests."
  },
  {
    "objectID": "posts/2023-07-09-unit-testing-dbt-models/unit-testing-dbt-models.html#unit-testing",
    "href": "posts/2023-07-09-unit-testing-dbt-models/unit-testing-dbt-models.html#unit-testing",
    "title": "Unit Testing dbt Models",
    "section": "Unit Testing",
    "text": "Unit Testing\nA unit test is a test that checks the correctness of a single unit of code. Generally, you would try to test the smallest components you can to ensure that each individual component of a larger codebase is doing what is expected of it. For instance, if you have the following method that adds two numbers:\ndef add(x: float, y: float) -&gt; float:\n  return x + y\nYou could write the following tests:\ndef test_add():\n\n  ## Commutativity\n  assert add(1, 2) == add(2, 1)  \n  \n  ## Associativity\n  assert add(1, add(2, 3)) == add(add(1, 2), 3)\n  \n  ## Correctness\n  assert add(2, 2) == 4\n  assert add(2, -10) == -8\n  assert add(2.5, 3.5) == 6\nThese are some very basic tests you might write to ensure that your add method is correctly adding the two numbers you supply it. Of course, these tests are not exhaustive, but you get the idea."
  },
  {
    "objectID": "posts/2023-07-09-unit-testing-dbt-models/unit-testing-dbt-models.html#unit-testing-sql",
    "href": "posts/2023-07-09-unit-testing-dbt-models/unit-testing-dbt-models.html#unit-testing-sql",
    "title": "Unit Testing dbt Models",
    "section": "Unit Testing SQL",
    "text": "Unit Testing SQL\nWriting unit tests for SQL code – in dbt or otherwise – is much less common than writing unit tests for application code, for instance. In fact, until I started working on unit testing our dbt models (the topic of this post), I had never seen any unit tests for SQL logic. There’s also relatively little written about best practices for unit testing in dbt, and I’ve often heard and seen the data quality checks outlined a bit above mistaken for unit tests. This is unfortunate, since they’re two entirely separate aspects to any data pipeline: Unit tests check that your SQL is correct and does what you think it does, and data quality checks ensure that the data flowing through your system meets your expectations. Both of these types of tests should be important parts of your ELT pipelines.\nThe trickiest part of unit testing SQL queries is the actual mechanics of it. Unit testing SQL requires seeding data into a database, then running a query, and then comparing the results of that query on the seed data to some expectations. For instance, consider the following query that selects page views from the previous week using the fct_pageviews table we defined above:\nSELECT event_id, user_id, page_viewed_at\nFROM fct_pageviews\nWHERE page_viewed_at &gt; (GETDATE() - INTERVAL '7 days')\nOf course, this query is trivial. But you might imagine wanting to write a unit test that checks if the oldest page view occurred less than seven days ago. You might write some SQL like this to achieve that:\nWITH query AS (\n  SELECT event_id, user_id, page_viewed_at\n  FROM fct_pageviews\n  WHERE page_viewed_at &gt; (GETDATE() - INTERVAL '7 days')\n)\n\nSELECT MIN(page_viewed_at) &gt; 'some date' AS test_passed\nFROM query\nYou could also, for instance, pull your query result into R and run the assertions from there:\nconn &lt;- DBI::dbConnect(\"some\", \"credentials\")\n\nresult &lt;- DBI::dbGetQuery(\n  conn,\n  \"\n  SELECT event_id, user_id, page_viewed_at\n  FROM fct_pageviews\n  WHERE page_viewed_at &gt; (GETDATE() - INTERVAL '7 days')\n  \"\n)\n\ncheckmate::assert_true(min(result$page_viewed_at) &gt; \"some date\")\nBut now you need to determine how to actually run the assertion and what to do if it fails. Not only that, but you probably don’t want to be running unit tests against production data. It would be better, for instance, to seed some data into your database to run the test against. Some seed data might look like this:\n\n\n\n\nevent_id\nuser_id\npage_viewed_at\n\n\n\n\n1\n1\n2023-07-01 23:59:59\n\n\n2\n1\n2023-07-02 00:01:01\n\n\n\n\nThen, after running the SQL, you might expect the following:\n\n\n\n\nevent_id\nuser_id\npage_viewed_at\n\n\n\n\n1\n1\n2023-07-01 23:59:59\n\n\n\n\nIf that’s your expectation, you could write assertions like:\n## assume x is a data frame with the result of the query\n\nassert len(x) == 1\nassert x[\"page_viewed_at\"][0] == \"2023-07-01 23:59:59\"\n\n## and so on\nThis approach could actually work quite well! If you have a query to test, you can strategically create seed data for individual test cases to test different bits of the logic.\nSo now, the question is: How do we actually implement and orchestrate these types of tests within dbt?"
  },
  {
    "objectID": "posts/2023-07-09-unit-testing-dbt-models/unit-testing-dbt-models.html#dbt-pytest",
    "href": "posts/2023-07-09-unit-testing-dbt-models/unit-testing-dbt-models.html#dbt-pytest",
    "title": "Unit Testing dbt Models",
    "section": "dbt + pytest",
    "text": "dbt + pytest\ndbt actually has a lesser-known feature that does exactly what we want, which leverages pytest fixtures. The long-and-short of it is this: dbt provides a framework for adapter developers to test their adapters, and we can jerry-rig the same framework to let us test our models!\n\nBackground\nBefore getting into the nuts and bolts, there’s some important context to include here.\nWe use Redshift as our data warehouse. But for testing, we don’t want to make a ton of round trips to a Redshift cluster to set up the tests, seed raw data, run transformations and queries, and clean up at the end. Doing all of that would dramatically slow down the testing process, and it’d also be expensive. Unfortunately, we also can’t run Redshift locally (like you can MySQL, for instance). So our hands are tied.\nOr are they? Redshift, fortunately, is a fork of Postgres 8, and shares a lot of user-facing features with Postgres, even if the guts of how it works are completely different. This, in combination with the handy cross-database macros that dbt provides let us solve our performance problems from above. Instead of using Redshift for our unit tests, we use Postgres instead. We can run Postgres locally, it’s easy to spin up and down on local and in a CI/CD environment, and it’s fast: Our test suite runs some 100x faster against a local Postgres instance than it does against Redshift. This is a massive win.\nIn short: We have a bash script that runs our unit tests, and it does four things:\n\nSets up environment variables that we need for running Postgres and dbt\nStart up Postgres in Docker with docker run postgres ...\nRun our unit test suite with poetry run pytest ...\nSpin down Postgres with docker stop ...\n\nAnd that’s it! We can run the exact same script on local and in CI.\n\n\n\n\n\n\nWe run Postgres in Docker as follows:\ndocker run \\\n    --name dbt_test_postgres \\\n    -p $TEST_DB_PORT:5432 \\\n    -e POSTGRES_PASSWORD=$TEST_DB_PASSWORD \\\n    -e POSTGRES_USER=$TEST_DB_USER \\\n    -e POSTGRES_DB=$TEST_DB_NAME \\\n    -d \\\n    postgres:11\n\n\n\nNow that you have a sense for the infrastructure, on to the tests themselves.\n\n\nOur Framework\nSince the dbt documentation is so great, I would recommend starting there for getting a sense for how to use dbt to test your models. But since it’s not exactly geared towards this particular use case, I’ll start by explaining a bit how we organize unit tests for our dbt models.\nIt’s actually quite simple: At the top level in our dbt project, we have a unit-tests directory. The contents of that directory look like this:\n.\n├── README.md\n├── common\n├── conftest.py\n├── poetry.lock\n├── pyproject.toml\n└── tests\nA few notes:\n\nWe use Poetry for managing dependencies, so our unit testing rig is a very basic Poetry project. We only have a few dependencies defined: pytest, dbt-postgres, pandas, and sqlalchemy.\nWe have the tests themselves living inside of tests/.\nWe have shared helper code living inside of common/.\nWe have a script at the top level of our dbt project that runs our tests, as described above.\n\n\nThe conftest File\nLet’s start with the conftest.py file, which looks like this:\n## conftest.py\n\nimport pytest\nimport os\n\n# Import the standard functional fixtures as a plugin\n# Note: fixtures with session scope need to be local\npytest_plugins = [\"dbt.tests.fixtures.project\"]\n\n# The profile dictionary, used to write out profiles.yml\n# dbt will supply a unique schema per test, so we do not specify \"schema\" here\n@pytest.fixture(scope=\"class\")\ndef dbt_profile_target():\n    return {\n        \"type\": \"postgres\",\n        \"dbname\": os.environ.get(\"TEST_DB_NAME\"),\n        \"threads\": 8,\n        \"host\": \"localhost\",\n        \"port\": int(os.environ.get(\"TEST_DB_PORT\")),\n        \"user\": os.environ.get(\"TEST_DB_USER\"),\n        \"password\": os.environ.get(\"TEST_DB_PASSWORD\")\n    }\nWhat we’re doing here is pretty simple. First, we import the plugin we need for running dbt with pytest, as is recommended in the docs. Next, we add a pytest fixture that represents the profiles.yml you’d find at the root of a dbt project, where we specify connection details to our Postgres test database. And that’s it!\n\n\nThe Tests\nOnce the conftest.py file is set up, the basic process is to add a new test at a path inside of tests/ that matches the location of the corresponding model you’re testing inside of models/. For instance, if you have models/int/int_users.sql, then you would also have tests/int/test_int_users.py. Let’s imagine we have a model called stg_users that our int_users selects from, and one of the transformations we want to do in the intermediate layer is remove any internal users. Maybe our SQL looks like this:\n-- int_users.sql\n\nSELECT *\nFROM {{ ref(\"stg_users\") }}\nWHERE NOT is_internal\nGreat! Now let’s test it.\n## tests/int/test_int_users.py\n\nimport pytest\nimport pandas as pd\n\nfrom dbt.tests.util import run_dbt\n\n## Importing helper code from `common`\n## \n## * `load_sql` that returns a string with the \n##     SQL for a model based on the model's name\n## * `connect_to_test_db` makes a connection to our \n##     Postgres test database so that we can query it\n##     from pandas\n## * `convert_dicts_to_csv` writes a list of Python dictionaries\n##     representing rows in a table to a CSV string.\nfrom common import load_sql, connect_to_test_db, convert_dicts_to_csv\n\nMODEL_NAME = \"int_users\"\n\n\nmock_stg_users = convert_dicts_to_csv([\n    {\"user_id\": 1, \"is_internal\": True, \"created_at\": \"2023-04-13 00:00:00\"},\n    {\"user_id\": 2, \"is_internal\": False, \"created_at\": \"2023-04-14 00:00:00\"},\n    {\"user_id\": 3, \"is_internal\": False, \"created_at\": \"2023-04-15 00:00:00\"}\n])\n\n\nclass TestIntUsers():\n\n    @pytest.fixture(scope=\"class\")\n    def seeds(self):\n        return {\"stg_users\": mock_stg_users}\n\n    @pytest.fixture(scope=\"class\")\n    def models(self):\n        ## See comment in imports for note on this method\n        return {\"actual.sql\": load_sql(MODEL_NAME)}\n\n    @pytest.fixture(scope=\"class\")\n    def actual(self):\n        build_result = run_dbt([\"build\"])\n\n        ## Extract the temporary schema generated by dbt + pytest\n        schema = build_result.results[0].node.schema\n\n        ## See comment in imports for note on this method\n        engine = connect_to_test_db()\n\n        actual = pd.read_sql(\n            sql = f\"SELECT * FROM {schema}.actual ORDER BY user_id\",\n            con = engine\n        )\n\n        return actual\n\n    def test_int_users_dimensions(self, project, actual):\n        assert actual.shape = (2, 3)\n      \n    def test_user_ids(self, project, actual):\n        assert actual[\"user_id\"].to_list() == [1, 2]\nAnd that’s it! Since actual is a Pandas DataFrame, you can write arbitrary assertions using whatever Python logic you please. Then, you just need to run pytest (or poetry run pytest, in our case) to run your test suite, assuming that you have Postgres running in the background already. If you don’t, you’ll need to spin it up first.\n\n\n\nA More Complicated Example\nLet’s get into a more complicated example, since the value of unit testing isn’t in testing trivial cases like above.\n\nSessionization\nA very common task for an analytics engineer is “sessionizing” events. In other words, converting actions that users take – such as viewing pages – into some notion of a session on a site. For example, if a user visits your site both today and tomorrow, you might consider those two visits to be separate sessions.\nBut some user tracking tools don’t give you sessions for free. Instead, you need to create them. Let’s imagine we have a table called stg_pageviews that comes from Segment, which you might use for event tracking. The table has the following columns: event_id, which uniquely identifies each page view, url, which is the URL of the page viewed, anonymous_id, which is Segment’s user ID that works for both anonymous and logged-in users, and timestamp, which is the timestamp of when the user viewed the page. Then, we might create sessions as follows\n-- int_sessions.sql\n\n-- Set the max session idle time, in minutes\n{% set max_session_idle_time_minutes = 30 %}\n\n\n-- First, we figure out how long passes between each (consecutive) pair\n-- of page views for a user.\nWITH pageviews_with_previous AS (\n    SELECT\n        event_id,\n        url,\n        anonymous_id,\n        timestamp,\n        LAG(timestamp, 1) OVER(\n            PARTITION BY anonymous_id\n            ORDER BY timestamp\n        ) AS previous_timestamp\n    FROM {{ ref(\"stg_pageviews\") }}\n),\nnew_session_labels AS (\n    SELECT\n        event_id,\n        url,\n        anonymous_id,\n        timestamp,\n        CASE\n            -- If the previous page was viewed less than `max_session_idle_time_minutes` ago\n            -- then it should be considered part of a new session\n            WHEN {{ datediff(\"previous_timestamp\", \"timestamp\", \"minute\") }} &lt;= {{ max_session_idle_time_minutes }} THEN 0\n            ELSE 1\n        END AS new_session\n    FROM pageviews_with_previous\n),\n\n-- Next, we create the session by taking a cumulative sum of the `new_session` values.\n--\n-- The basic idea is that `new_session` is 1 if {{ max_session_idle_time_minutes }} have\n--  passed since the last page view, and zero otherwise. That means that if you view\n--  three pages one after another in quick succession, the first one will get a value of 1\n--  for `new_session` since it's the first pageview _ever_ for you, and the others\n--  get a value of zero, since not enough time has passed since your last page view.\n--  If you then leave for an hour, come back, and view one page, that page gets a `new_session`\n--  value of one. And then you do it again, and the newest page view also gets a `new_session`\n--  value of one.\n--\n-- Then, when we do the cumulative sum over `new_session`, all of the values for a single \"session\"\n-- are 1 until we hit the fourth row (where there's another `new_session` value of 1) at which point\n-- the cumulative sum becomes 2. And then we hit the next row, and it becomes 3.\n--\n-- Here's an example of how it looks for the example above:\n--\n-- event_id     time     new_session  session_id\n--    1          00:00:00       1           1\n--    2          00:00:10       0           1\n--    3          00:00:20       0           1\n--    4          08:00:00       1           2\n--    5          14:00:00       1           3\nsession_numbers AS (\n    SELECT\n        *,\n        SUM(new_session) OVER(\n            PARTITION BY anonymous_id\n            ORDER BY timestamp\n            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n        ) AS session_number\n    FROM new_session_labels\n)\n\nSELECT\n    {{ dbt_utils.generate_surrogate_key(['anonymous_id', 'session_number']) }} AS session_id,\n    event_id,\n    url,\n    timestamp\nFROM session_numbers\nHopefully the comments in the code help with following along. The goal here at the end of the day is to create a table that we can join back to the page views table with our newly created sessions. Then we can use sessions to easily analyze things like conversion rates, bounce rates, common landing and exit pages, and so on.\nBut this isn’t a blog post about sessionization, it’s about testing. So let’s write some tests!\n## tests/int/test_int_sessions.py\n\nimport pytest\nimport pandas as pd\nfrom dbt.tests.util import run_dbt\nfrom common import load_sql, convert_dicts_to_csv, connect_to_test_db\n\nMODEL_NAME = \"int_sessions\"\n\nmock_stg_pageviews = convert_dicts_to_csv([\n    {\"event_id\": 1, \"url\": \"www.example.com\", \"anonymous_id\": \"foobar\", \"timestamp\": \"2023-04-13 15:00:31\"},\n    {\"event_id\": 2, \"url\": \"www.example.com/foo/bar\", \"anonymous_id\": \"foobar\", \"timestamp\": \"2023-04-13 15:00:41\"},\n    {\"event_id\": 3, \"url\": \"www.example.com/baz\", \"anonymous_id\": \"foobar\", \"timestamp\": \"2023-04-13 15:00:59\"},\n    {\"event_id\": 4, \"url\": \"www.example.com\", \"anonymous_id\": \"foobar\", \"timestamp\": \"2023-04-13 16:00:00\"},\n    {\"event_id\": 5, \"url\": \"www.example.com/baz\", \"anonymous_id\": \"foobar\", \"timestamp\": \"2023-04-13 17:00:00\"},\n    {\"event_id\": 6, \"url\": \"www.example.com/baz\", \"anonymous_id\": \"foobar\", \"timestamp\": \"2023-04-13 17:29:59\"},\n    {\"event_id\": 7, \"url\": \"www.example.com/baz\", \"anonymous_id\": \"foobar\", \"timestamp\": \"2023-04-13 18:00:00\"},\n])\n\nexpected = pd.DataFrame([{\"event_id\": i} for i in range(1, 8)])\n\nactual = load_sql(MODEL_NAME)\n\n\nclass TestIntSessions():\n    @pytest.fixture(scope=\"class\")\n    def seeds(self):\n        return {\n            \"stg_pageviews.csv\": mock_stg_pageviews\n        }\n\n    @pytest.fixture(scope=\"class\")\n    def models(self):\n        return {\n            \"actual.sql\": actual\n        }\n\n    @pytest.fixture(scope=\"class\")\n    def packages(self):\n        return \"\"\"\n        packages:\n          - package: dbt-labs/dbt_utils\n            version: \"1.0.0\"\n        \"\"\"\n\n    @pytest.fixture(scope=\"class\")\n    def actual(self):\n        run_dbt([\"deps\"])\n        build_result = run_dbt([\"build\"])\n\n        schema = build_result.results[0].node.schema\n\n        engine = connect_to_test_db()\n\n        actual = pd.read_sql(\n            sql = f\"SELECT * FROM {schema}.actual ORDER BY event_id\",\n            con = engine\n        )\n\n        return actual\n\n    def extract_session_id(self, actual):\n        return actual[\"session_id\"].tolist()\n\n    def test_event_ids_are_unmodified_by_model(self, project, actual):\n        ## Test that the pageview IDs and the landing pages match our expectations\n        assert actual[\"event_id\"].tolist() == expected[\"event_id\"].tolist()\n\n    def test_first_session_correctly_created(self, project, actual):\n        session_ids = self.extract_session_id(actual)\n\n        ## In the data above, there should be four unique sessions created.\n        ## They should correspond to page views 1-3, page view 4, page views 5-6, and page view 7\n        ## The first three rows should all be the same session\n        assert session_ids[0] == session_ids[1]\n        assert session_ids[1] == session_ids[2]\n        assert session_ids[0] == session_ids[2]\n\n        ## The rest of the rows should be different sessions\n        assert session_ids[0] not in session_ids[3:]\n\n    def test_second_session_one_row(self, project, actual):\n        session_ids = self.extract_session_id(actual)\n\n        ## The fourth row should be its own session\n        assert session_ids[3] not in session_ids[:3]\n        assert session_ids[3] not in session_ids[4:]\n\n    def test_twenty_nine_min_fifty_nine_seconds_later_is_same_session(self, project, actual):\n        session_ids = self.extract_session_id(actual)\n\n        # ## The fifth and sixth rows should be their own session\n        assert session_ids[4] == session_ids[5]\n        assert session_ids[4] not in session_ids[:4]\n        assert session_ids[4] not in session_ids[6:]\n\n    def test_thirty_minutes_and_one_second_later_is_new_session(self, project, actual):\n        session_ids = self.extract_session_id(actual)\n\n        # ## The seventh row should be its own session\n        assert session_ids[6] not in session_ids[:6]\nThat was a lot of code to process, but the basic gist is the same as before: First, we define some “seed” data in stringified CSV (converted from a list of dictionaries) that we load into our database. We then run our dbt model on the seed data, we query the result, and we run assertions against the result to guarantee that our code is actually behaving how we want.\nNote that in the spirit of unit testing, we can get very granular here. For instance, in these tests we’re checking things like individual session IDs for pageviews that occurred one second before and after a specified timestamp being different from each other, and that different users have different session IDs, and so on. This is of course just an example, but you can make this logic as involved as you like. At the end of the day, the goal is to help you sleep at night with the knowledge that your code, which might be feeding into ML models, underpinning business decisions, and so on, is correct."
  },
  {
    "objectID": "posts/2023-07-09-unit-testing-dbt-models/unit-testing-dbt-models.html#wrapping-up",
    "href": "posts/2023-07-09-unit-testing-dbt-models/unit-testing-dbt-models.html#wrapping-up",
    "title": "Unit Testing dbt Models",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nThis was a bit of an in the weeds, technical post. The goal was to shed some light on something that, from what I’ve heard, not many people are talking about, and likely even fewer are actually doing. My hope is that every analytics engineering team will write unit tests for their dbt pipelines, and my goal in writing this post was to make setting up your test suite more approachable. The main takeaway is this: Since dbt plays so nicely with pytest, it should be fast and simple to get your unit test suite off the ground! And once you’re in the habit of writing unit tests, you can have significantly more confidence in the correctness of the SQL that’s running in your dbt builds. After all: dbt sells itself as bringing software engineering practices to data pipelines, and unit testing is maybe the best of those best practices."
  },
  {
    "objectID": "posts/2023-07-09-unit-testing-dbt-models/unit-testing-dbt-models.html#appendix",
    "href": "posts/2023-07-09-unit-testing-dbt-models/unit-testing-dbt-models.html#appendix",
    "title": "Unit Testing dbt Models",
    "section": "Appendix",
    "text": "Appendix\n\ncommon Code\nI’m going to leave some of the code we have in our common module here, for others to copy.\nFirst, connecting to our test database:\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.engine.url import URL\nimport os\n\ndef connect_to_test_db():\n    url = URL.create(\n        \"postgresql\",\n        username = os.environ.get(\"TEST_DB_USER\"),\n        password = os.environ.get(\"TEST_DB_PASSWORD\"),\n        host = \"localhost\",\n        database = os.environ.get(\"TEST_DB_NAME\"),\n        port = int(os.environ.get(\"TEST_DB_PORT\"))\n    )\n\n    return create_engine(url)\nNext, converting a list of dictionaries to a stringified CSV:\nimport pandas as pd\n\ndef convert_dict_to_csv(data):\n    df = pd.DataFrame.from_dict(data)\n    return df.to_csv(index = False)\nAnd finally, loading models and macros. Note that this is much more involved, and there’s certainly a way to do this with a macro that relies on the dbt graph context variable, but that was far more involved than this.\nfrom dbt.tests.util import read_file\nimport glob\n\ndef load_sql(basename):\n    model_sql_name = basename + \".sql\"\n    model_regex = f'../models/**/{model_sql_name}'\n    model_matches = glob.glob(model_regex, recursive=True)\n\n    if not model_matches:\n        raise Exception(\n            f\"\"\"\n            Could not find a model named '{model_sql_name}'.\n            Does the path to your test exactly match the path to the model you're testing?\n\n            For example, to test model\n                'models/staging/foo/bar/baz.sql'\n\n            You would put your test in:\n                'tests/staging/foo/bar/test_baz.sql'\n            \"\"\"\n        )\n\n    if len(model_matches) &gt; 1:\n        raise Exception(\n            f\"\"\"\n            Your path matched multiple models. Did you accidentally create a duplicate model?\n\n            The following paths were matched: {model_matches}\n            \"\"\"\n        )\n\n    return read_file(model_matches[0])"
  },
  {
    "objectID": "posts/2021-01-13-a-gentle-introduction-to-markov-chains-and-mcmc/index.html",
    "href": "posts/2021-01-13-a-gentle-introduction-to-markov-chains-and-mcmc/index.html",
    "title": "A Gentle Introduction to Markov Chains and MCMC",
    "section": "",
    "text": "Introduction\nEvery other Friday at work we have a meeting called All Hands. During the first half of All Hands a member of the team gives a presentation, which is split up into two pieces: A personal presentation – your favorite food, TV shows, books, etc. – and a mini-lesson, which can be about any topic of interest that’s unrelated to work. Yesterday it was my turn, and I gave my mini-lesson on Markov Chains and Markov Chain Monte Carlo. This post memorializes what I covered.\n\n\nMarkov Chains\nFirst, what is a Markov Chain? It’s easiest to break it down into it’s component parts. A Markov Chain is a chain, or sequence of events, that follow the Markov Property. And the Markov Property is pretty intuitive: The Markov Property says that the next state in a sequence (chain) is only dependent on the current state. Statisticians would call this “memorylessness,” and we can write out the property in its true mathematical form below, where \\(X\\) is a random variable and \\(x\\_{t}\\) is the probability distribution that \\(X\\) takes on at time \\(t\\).\n\\[\np(x_{t+1} | x_{t}, x_{t-1}, x_{t-2}, .. x_{0}) = p(x_{t+1} | x_{t})\n\\]\nIn plain English, all this definition is saying is that if you are following the Markov Property, then where you go next is only determined by where you are now, and how you got to where you are has no impact. At the end of my talk, one of my coworkers commented that this property is actually quite beautiful in a real-world sense, and I feel the same way. It certainly could have been the example of a guiding principle that I choose to follow that I used in my personal presentation.\nSo, now that we know what a Markov Chain is, let’s walk through an example. The most commonly seen type of Markov Chain is called a random walk. Simply, a random walk is a Markov Chain where the next state is just determined by the current state plus some random noise. I’ve coded up an example below:\n\n## library(purrr)\n## library(magrittr)\n## library(ggplot2)\n\nrandomly_walk &lt;- function(.ix = c(), n_steps = 100) {\n  results &lt;- numeric(n_steps)\n  for (i in 1:(n_steps-1)) {\n    results[i + 1] &lt;- results[i] + rnorm(1, 0, 1)\n  }\n  \n  return(results)\n}\n\n(\n  random_walk &lt;- randomly_walk() %&gt;%\n    tibble(position = .)\n)\n\n# A tibble: 100 × 1\n   position\n      &lt;dbl&gt;\n 1   0     \n 2  -1.13  \n 3   1.52  \n 4  -0.0912\n 5  -0.348 \n 6  -1.66  \n 7  -0.884 \n 8  -0.491 \n 9   0.179 \n10  -0.812 \n# … with 90 more rows\n\n\nThat table shows a random walk with 100 steps, generated by adding standard normal noise to the position after each step. Let’s plot it and see what it looks like.\n\nrandom_walk %&gt;%\n  rownames_to_column(var = 'time') %&gt;%\n  mutate(time = time %&gt;% as.numeric()) %&gt;%\n  ggplot(aes(time, position)) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nCool! The walk starts at 0, and then jumps around randomly a bunch until \\(t=100\\). It’ll be more interesting once we simulate 20 random walks.\n\nn_steps &lt;- 500\nn_chains &lt;- 20\ntwenty_walks &lt;- map(\n  1:n_chains, \n  randomly_walk,\n  n_steps\n) %&gt;%\n  tibble(position = .) %&gt;%\n  unnest(cols = c(position)) %&gt;%\n  mutate(\n    time = rep(1:n_steps, n_chains),\n    walk = map(1:n_chains, rep, n_steps) %&gt;% \n      unlist() %&gt;% \n      as.factor()\n  )\n\ntwenty_walks %&gt;%\n  ggplot() +\n  aes(time, position, color = walk) +\n  geom_line() +\n  theme_minimal() + \n  theme(legend.position = 'none')\n\n\n\n\n\n\n\n\nSo, what’s going on here? It basically looks how we’d expect. At any given time point, the mean position of the 20 is about zero, but the standard deviation of those positions goes up over time. Specifically, at any given time \\(t\\), the standard deviation of the positions should be roughly equal to \\(\\sqrt t\\), because of how the variance is compounding. Remember, these random walks are Markov Chains because at every time \\(t\\), I defined the position \\(y\\_{t+1}\\) to be \\(y\\_{t} + \\mathcal{N}(0, 1)\\), or the the next position is the current position plus a standard normal noise (i.e. zero-centered with unit variance).\nCool, so now we have an idea of what a Markov Chain is and how a random walk is an example of one. Now, why do we care? What kinds of problems can we solve with Markov Chains? It turns out that one thing we can use them to do is to calculate intractable integrals. What does this mean? Well, remembering back to a calculus class once upon a time, we know if we have some function \\(f(x) = 2x\\), we can integrate that function by following a one of a couple of rules. In this case, that rule is to raise the coefficient in front of the \\(x\\) to turn it into a power, such that the new exponent equals the old one plus one, and the new coefficient equals the old one divided by the new exponent. For \\(f(x)\\), we find \\(F(x) = \\int f(x) = x^{2} + c\\), where \\(c\\) is a constant. However, in many applications, such as Bayesian statistics, we run into functions of hundreds or thousands of parameters that are intractable to integrate. In other words, even really, really powerful calculators can’t integrate them: there are just too many parameters. So, we’re stuck. How do we integrate a function that even a super powerful calculator can’t? In steps Markov Chain Monte Carlo, coming to the rescue.\n\n\nMarkov Chain Monte Carlo\nIt turns out that we can use Markov Chains to approximate the integral in cases where we can’t calculate it directly. This is an incredible powerful discovery, and one that we’ve only been able to really take advantage of in the past twenty or so years, as computing power has grown exponentially. So, how do we actually do it? Let’s frame it as a simple problem that’s isomorphic to the actual problem at hand.\nImagine you are the ruler of an island kingdom, which has four islands. Island 1 has population 1, Island 2 has population 2, Island 3 has population 3, and Island 4 has population 4. And, imagine that you want to spend time on each island proportional to the percentage of the total population of your kingdom that it makes up. In other words, you want to spend 10% of your time on Island 1, and so on. But, you have a problem: You don’t know how to add. Imagine that the only mathematical operation you know how to do is divide. Can you figure out a way to spend your time how you want without being able to calculate the total population of your kingdom?\nMost likely, how you’d solve this problem isn’t immediately obvious, but there are a few brilliant algorithms that help us achieve our goal. One of them is proposed to you by two of your friends, Metropolis and Hastings. I’ve coded up their suggestion below:\n\nrun_rwmh &lt;- function(n_iters = 1000, island_populations = 1:4) {\n  locations &lt;- numeric(n_iters)\n  \n  ## randomly choose an island to start on\n  locations[1] &lt;- sample(island_populations, 1)\n\n  for (i in 1:(n_iters-1)) {\n    \n    ## propose a new island to go to\n    proposal_island &lt;- sample(setdiff(island_populations, locations[i]), 1) \n    \n    ## if that island has more people, always go\n    if (proposal_island &gt; locations[i]) {\n      locations[i + 1] &lt;- proposal_island\n    } else {\n      ## if it has fewer people, flip a coin with probability\n      ##   proportional to the ratio of the populations to\n      ##   decide whether to go or stay\n      acceptance_probability &lt;- proposal_island / locations[i]\n      locations[i + 1] &lt;- \n        sample(\n          c(proposal_island, locations[i]), 1, \n          prob = c(acceptance_probability, 1 - acceptance_probability)\n        )\n    }\n  }\n  return(locations)\n}\n\nHere’s the algorithm your friends propose:\n\nPick a random island to start on.\nOn each day, randomly select a new island to go to (the proposal island).\nDo one of the following, depending on the populations of the islands:\n\nIf the proposal island has more people than the current island, go to the proposal island.\nIf it has fewer people, then flip a coin with probability equal to the proposal island’s population divided by the current island’s. If the coin comes up heads, go to the proposal island.\n\nDo it again a bunch of times.\n\nSo, how does this algorithm perform? Let’s try it out!\n\nrun_rwmh(n_iters = 10) %&gt;%\n  tibble(island = .) %&gt;%\n  group_by(island) %&gt;%\n  summarize(days_spent = n(), .groups = 'drop') %&gt;%\n  mutate(day_proportion = days_spent / sum(days_spent))\n\n# A tibble: 4 × 3\n  island days_spent day_proportion\n   &lt;dbl&gt;      &lt;int&gt;          &lt;dbl&gt;\n1      1          1            0.1\n2      2          1            0.1\n3      3          3            0.3\n4      4          5            0.5\n\n\nUnsurprisingly, with only 10 iterations the algorithm does not perform particularly well. But what about if we give it a lot more time? Let’s try 10,000 iterations.\n\nsome_islands &lt;- run_rwmh(n_iters = 10000) %&gt;%\n  tibble(island = .) %&gt;%\n  group_by(island) %&gt;%\n  summarize(days_spent = n(), .groups = 'drop') %&gt;%\n  mutate(day_proportion = days_spent / sum(days_spent),\n         error_margin = day_proportion / (island / sum(island)) - 1)\n\nmean_error_margin &lt;- mean(some_islands$error_margin)\nsd_error_margin &lt;- sd(some_islands$error_margin)\nsome_islands\n\n# A tibble: 4 × 4\n  island days_spent day_proportion error_margin\n   &lt;dbl&gt;      &lt;int&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n1      1       1000          0.1        0      \n2      2       1958          0.196     -0.0210 \n3      3       3063          0.306      0.0210 \n4      4       3979          0.398     -0.00525\n\n\nMuch better! After 10,000 iterations, we’re spending almost the exact proportion of time on each island that we want to be, as evidenced by the tiny error margins. In addition, the standard deviation of the error margins is 0.01735, which is tiny. That’s awesome! But what about if the system is more complex? Like, what if we had 100 islands?\n\nmore_islands &lt;- run_rwmh(n_iters = 10000, island_populations = 1:100) %&gt;%\n  tibble(island = .) %&gt;%\n  group_by(island) %&gt;%\n  summarize(days_spent = n(), .groups = 'drop') %&gt;%\n  mutate(day_proportion = days_spent / sum(days_spent),\n         error_margin = day_proportion / (island / sum(island)) - 1)\n\nmean_error_margin &lt;- mean(more_islands$error_margin)\nsd_error_margin &lt;- sd(more_islands$error_margin)\nmore_islands\n\n# A tibble: 100 × 4\n   island days_spent day_proportion error_margin\n    &lt;dbl&gt;      &lt;int&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n 1      1          1         0.0001      -0.495 \n 2      2          8         0.0008       1.02  \n 3      3          9         0.0009       0.515 \n 4      4         19         0.0019       1.40  \n 5      5          8         0.0008      -0.192 \n 6      6         13         0.0013       0.0942\n 7      7         14         0.0014       0.0100\n 8      8         11         0.0011      -0.306 \n 9      9         19         0.0019       0.0661\n10     10         28         0.0028       0.414 \n# … with 90 more rows\n\n\nNo problem! Even with the extra islands, the mean error margin is still zero, and the standard deviation of the error margins is 0.23184, which is also small, but not as small as the simpler system. It’s true that a more complex system (i.e. more islands) would mean that we need more iterations to converge in probability to the proportions we’re shooting for, but the algorithm will still work with enough time. Let’s try running it one more time on the complex system, but this time with a million iterations.\n\nmore_iters &lt;- run_rwmh(n_iters = 1000000, island_populations = 1:100) %&gt;%\n  tibble(island = .) %&gt;%\n  group_by(island) %&gt;%\n  summarize(days_spent = n(), .groups = 'drop') %&gt;%\n  mutate(day_proportion = days_spent / sum(days_spent),\n         error_margin = day_proportion / (island / sum(island)) - 1)\n\nmean_error_margin &lt;- mean(more_iters$error_margin)\nsd_error_margin &lt;- sd(more_iters$error_margin)\nmore_iters\n\n# A tibble: 100 × 4\n   island days_spent day_proportion error_margin\n    &lt;dbl&gt;      &lt;int&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n 1      1        199       0.000199      0.00495\n 2      2        432       0.000432      0.0908 \n 3      3        614       0.000614      0.0336 \n 4      4        726       0.000726     -0.0834 \n 5      5        985       0.000985     -0.00515\n 6      6       1157       0.00116      -0.0262 \n 7      7       1409       0.00141       0.0165 \n 8      8       1601       0.00160       0.0106 \n 9      9       1695       0.00170      -0.0489 \n10     10       1952       0.00195      -0.0142 \n# … with 90 more rows\n\n\nLooks like that did the trick! The standard deviation of the error margins fell to 0.02015, just as we expected.\nThis algorithm is called the Metropolis-Hastings Algorithm, and it’s one of many in the class of Markov Chain Monte Carlo algorithms. Some others are the Gibbs Sampler and Hamiltonian Monte Carlo, both of which are frequently used in Bayesian statistics for estimating the parameters of regression models with hundreds of thousands of parameters. In short, these algorithms allow us to solve problems that were literally impossible to solve only two decades ago or so, which is an amazing feat!\n\n\nRecap\n\nMarkov Chains are not that scary! They’re just a memoryless sequence of events, meaning that where you came from doesn’t impact where you go next.\nMarkov Chain Monte Carlo algorithms like the Metropolis-Hastings can be quite simple, and let us solve impossibly hard problems."
  },
  {
    "objectID": "posts/2021-02-11-highlights-from-rstudio-global/index.html",
    "href": "posts/2021-02-11-highlights-from-rstudio-global/index.html",
    "title": "Highlights From rstudio::global",
    "section": "",
    "text": "rstudio::global, this year’s iteration of the annual RStudio conference, was a few weeks ago. Here were some highlights:"
  },
  {
    "objectID": "posts/2021-02-11-highlights-from-rstudio-global/index.html#talks",
    "href": "posts/2021-02-11-highlights-from-rstudio-global/index.html#talks",
    "title": "Highlights From rstudio::global",
    "section": "Talks",
    "text": "Talks\nThere were a few talks I really loved:\n\nUsing R to Up Your Experimentation Game, by Shirbi Ish-Shalom. On experimentation, sequential testing, taking big swings, and being statistically rigorous\nMaintaining the House the Tidyverse Built, by Hadley Wickham. On building and maintaining the Tidyverse, and what package maintenance in the real world is like when you have millions of downloads.\noRganization: How to Make Internal R Packages Part of Your Team, by Emily Riederer. On how using internal packages (like collegeviner at CollegeVine!) can improve your R workflow and make teamwork in R dramatically easier, smoother, and more efficient.\nFairness and Data Science: Failures, Factors, and Futures, by Grant Fleming. On model fairness, bias, and evaluation techniques, and why they’re important to get right."
  },
  {
    "objectID": "posts/2021-02-11-highlights-from-rstudio-global/index.html#cool-new-things",
    "href": "posts/2021-02-11-highlights-from-rstudio-global/index.html#cool-new-things",
    "title": "Highlights From rstudio::global",
    "section": "Cool New Things",
    "text": "Cool New Things\n\nfinetune, Max Kuhn’s new tune-adjacent package, is live (albeit a little buggy)! It has some cool new model tuning algorithms, including racing methods with tune_race_anova() and tune_race_win_loss(), in addition to my personal favorite: tune_sim_anneal() for Simulated Annealing! Link to the talk\nMajor improvements to shiny, including some serious caching upgrades that’ll improve performance dramatically! Link to the talk"
  },
  {
    "objectID": "posts/2021-02-11-highlights-from-rstudio-global/index.html#other-highlights",
    "href": "posts/2021-02-11-highlights-from-rstudio-global/index.html#other-highlights",
    "title": "Highlights From rstudio::global",
    "section": "Other Highlights",
    "text": "Other Highlights\n\nMeeting a bunch of people in the breakout sessions! This year, there were virtual “tables” where you could drag your avatar to “sit down”, and once you were close enough to a table you could hear all of its conversation."
  },
  {
    "objectID": "series.html",
    "href": "series.html",
    "title": "Series",
    "section": "",
    "text": "Heavily inspired by The Missing Semester of Your CS Education, this is a series on all of the aspects of doing data science that you probably didn’t learn in class or from doing research. I’ve learned a lot in the past few years about what it takes to do data science professionally and, frankly, my data science education had some gaping holes. The goal of this series is to help others fill some of those holes (to the best of my ability) by sharing some of the things I’ve learned along the way from great mentors and terrible mistakes.\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\nApr 1, 2023\n\n\nWriting Internal Libraries for Analytics Work\n\n\n14 min\n\n\n\n\nApr 5, 2023\n\n\nUnit Testing Analytics Code\n\n\n14 min\n\n\n\n\nApr 24, 2023\n\n\nPull Requests, Code Review, and The Art of Requesting Changes\n\n\n12 min\n\n\n\n\nMay 6, 2023\n\n\nWorkflow Orchestration\n\n\n8 min\n\n\n\n\nMay 14, 2023\n\n\nExperiment Tracking and Model Versioning\n\n\n9 min\n\n\n\n\nMay 28, 2023\n\n\nDependency Management\n\n\n10 min\n\n\n\n\nJun 6, 2023\n\n\nA Gentle Introduction to Docker\n\n\n12 min\n\n\n\n\nJun 21, 2023\n\n\nHow Can Someone Else Use My Model?\n\n\n10 min\n\n\n\n\nAug 3, 2023\n\n\nOn Doing Data Science\n\n\n10 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "series.html#the-missing-semester-of-your-ds-education",
    "href": "series.html#the-missing-semester-of-your-ds-education",
    "title": "Series",
    "section": "",
    "text": "Heavily inspired by The Missing Semester of Your CS Education, this is a series on all of the aspects of doing data science that you probably didn’t learn in class or from doing research. I’ve learned a lot in the past few years about what it takes to do data science professionally and, frankly, my data science education had some gaping holes. The goal of this series is to help others fill some of those holes (to the best of my ability) by sharing some of the things I’ve learned along the way from great mentors and terrible mistakes.\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\nApr 1, 2023\n\n\nWriting Internal Libraries for Analytics Work\n\n\n14 min\n\n\n\n\nApr 5, 2023\n\n\nUnit Testing Analytics Code\n\n\n14 min\n\n\n\n\nApr 24, 2023\n\n\nPull Requests, Code Review, and The Art of Requesting Changes\n\n\n12 min\n\n\n\n\nMay 6, 2023\n\n\nWorkflow Orchestration\n\n\n8 min\n\n\n\n\nMay 14, 2023\n\n\nExperiment Tracking and Model Versioning\n\n\n9 min\n\n\n\n\nMay 28, 2023\n\n\nDependency Management\n\n\n10 min\n\n\n\n\nJun 6, 2023\n\n\nA Gentle Introduction to Docker\n\n\n12 min\n\n\n\n\nJun 21, 2023\n\n\nHow Can Someone Else Use My Model?\n\n\n10 min\n\n\n\n\nAug 3, 2023\n\n\nOn Doing Data Science\n\n\n10 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "series.html#ab-testing",
    "href": "series.html#ab-testing",
    "title": "Series",
    "section": "A/B Testing",
    "text": "A/B Testing\nA series on common pitfalls in A/B testing and how sequential testing solves many common A/B testing problems.\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\nMar 25, 2022\n\n\nA/B Testing: A Primer\n\n\n4 min\n\n\n\n\nApr 9, 2022\n\n\nRunning A/B Tests\n\n\n5 min\n\n\n\n\nApr 10, 2022\n\n\nCalling A/B Tests\n\n\n7 min\n\n\n\n\nApr 17, 2022\n\n\nSequential Testing\n\n\n5 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "series.html#all-posts",
    "href": "series.html#all-posts",
    "title": "Series",
    "section": "All Posts",
    "text": "All Posts\nTo see all of my blog posts, go to the posts page"
  },
  {
    "objectID": "blogroll.html",
    "href": "blogroll.html",
    "title": "Blogroll",
    "section": "",
    "text": "R-Bloggers"
  },
  {
    "objectID": "posts/2023-10-12-jobcrawler/jobcrawler.html",
    "href": "posts/2023-10-12-jobcrawler/jobcrawler.html",
    "title": "Introducing Jobcrawler",
    "section": "",
    "text": "I’ve been working on Jobcrawler for the past six months or so, and I’m proud of where it’s at and thought it might be a good time to share it with all of you (cough, potential beta users). This post will be a brief intro to what the app does, why I built it, a couple things I’ve learned along the way, and some roadmap items for the future. As a disclaimer: I’m still not an engineer, so please don’t read through my largely GPT-generated React code."
  },
  {
    "objectID": "posts/2023-10-12-jobcrawler/jobcrawler.html#introduction",
    "href": "posts/2023-10-12-jobcrawler/jobcrawler.html#introduction",
    "title": "Introducing Jobcrawler",
    "section": "",
    "text": "I’ve been working on Jobcrawler for the past six months or so, and I’m proud of where it’s at and thought it might be a good time to share it with all of you (cough, potential beta users). This post will be a brief intro to what the app does, why I built it, a couple things I’ve learned along the way, and some roadmap items for the future. As a disclaimer: I’m still not an engineer, so please don’t read through my largely GPT-generated React code."
  },
  {
    "objectID": "posts/2023-10-12-jobcrawler/jobcrawler.html#whats-jobcrawler",
    "href": "posts/2023-10-12-jobcrawler/jobcrawler.html#whats-jobcrawler",
    "title": "Introducing Jobcrawler",
    "section": "What’s Jobcrawler?",
    "text": "What’s Jobcrawler?\nJobcrawler is an app that I started building when I was passively job hunting from about April until June. I had a problem that I didn’t know of a good solution to: I had a shortlist of companies I thought I might be interested in working for, and I found myself needing to repeatedly check their job boards very often to see if they posted a new job that matched my interests (mostly data science and ML engineering).\nThis was a painful, time-consuming, and, largely, easily automatable process.\nMy initial thought was to just write a cron job that would scrape each board each day, look for job postings that contained “data science” in the titles, and send me an email with the postings. But then I considered that I could do the same email send using a background job in a web app, and I could also build additional features more easily, such as a UI that lets you test scraping a company’s board before adding them to the list of companies, a way to more easily add, delete, and change searches at specific companies, and so on.\nThe app has evolved pretty significantly since then. Now, the app has a handful of core features:\n\nYou can view the companies in its database and add new ones that you’re interested in.\nYou can create searches, which will trigger emails to be sent to you when a company you’re searching at posts a job that matches your search query.\nYou can peruse the job boards of all companies in the database or of a single company and filter for postings matching your interests.\nYou can keep track of some statistics about your usage of the app, including how many postings you’ve seen, how much time you’ve saved, etc.\nAnd more coming soon, including more advanced analytics, including tracking an individual job search by keeping track of postings you’ve applied to and what the results were, recommended jobs for you, improved filtering / searching capabilities of companies and postings, and more!"
  },
  {
    "objectID": "posts/2023-10-12-jobcrawler/jobcrawler.html#why-should-i-use-it",
    "href": "posts/2023-10-12-jobcrawler/jobcrawler.html#why-should-i-use-it",
    "title": "Introducing Jobcrawler",
    "section": "Why Should I Use It?",
    "text": "Why Should I Use It?\nBasically for the reason it says on the tin: It’s a no-bullshit job searching app. I hate LinkedIn, and I largely built this so I could stay off of LinkedIn and sites like it. There aren’t any ads, there’s no recruiter spam, there aren’t any stale postings that aren’t actually open anymore. Jobcrawler just works directly with company job boards and gets rid of the intermediary.\nIt’s also free, and I’m not going to spam you about paying for an account (at least until my hosting costs are higher than the $20 / month that they are now).\nAnd most importantly: I think it’ll make your job search easier and more delightful! It did for me."
  },
  {
    "objectID": "posts/2023-10-12-jobcrawler/jobcrawler.html#wrapping-up",
    "href": "posts/2023-10-12-jobcrawler/jobcrawler.html#wrapping-up",
    "title": "Introducing Jobcrawler",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nIf there are any VCs reading this who think “Wow, this is an amazing start up idea!” please reach out to me.\nBut in all seriousness: I hope someone comes across this post and finds something here useful, and I’d love to hear any feedback from anyone who’s tried the app!"
  }
]